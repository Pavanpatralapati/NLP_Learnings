{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30571543-3982-4863-8e33-3f1081f88dbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc613275-c63c-4978-a88c-b5856b2e792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f8b441-1325-43a6-9d18-c7837c64f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f66bf8b-43ed-4ad5-b672-eb94a173e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.data.path.append(\"C:/Users/Patralapati/AppData/Roaming/nltk_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe242ee-7d8c-494f-b444-b096f89e3b34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c01cd4-6dee-4556-ae26-d09f8c0b5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome,to Pavan kalyan's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82166825-e833-407f-8c11-c4a13bfaa7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# Download both punkt and punkt_tab\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba6f57ad-3bab-4231-9636-74c6a606dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome,to Pavan kalyan's NLP Tutorials.\n",
      "Please do watch the entire course! to become expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7749f4e5-72b4-434c-9310-51b748cb0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Tokenization\n",
    "## Sentence-->paragraphs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6454c4-ca64-4964-8420-14c294af9899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Welcome,to Pavan kalyan's NLP Tutorials.\",\n",
       " 'Please do watch the entire course!',\n",
       " 'to become expert in NLP.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7960b385-309d-4fb0-8aaa-aa2c8ba73201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0841bba2-0438-424a-a991-dbe1f033c6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Pavan',\n",
       " 'kalyan',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c7ee83-15e9-4302-9718-9db722f69b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Pavan', 'kalyan', \"'s\", 'NLP', 'Tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
      "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in documents:\n",
    "    words = word_tokenize(sent)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff31ff-9c62-4bb2-bac2-ef62945fa4d7",
   "metadata": {},
   "source": [
    "### wordpunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "460a2bac-cfeb-4435-acbd-eab6cccb44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08f15432-6697-4e11-99db-6609efb77a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Pavan',\n",
       " 'kalyan',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a0387-c1e3-47b0-97a8-805c7d993f73",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4803819-e735-4ac5-a71c-750e7765e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e76e41-5e63-4906-87a2-073973120c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Pavan',\n",
       " 'kalyan',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d6322-ce15-4633-8e95-5f89975af425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90952578-1848-45d6-ad65-27f27a3b521d",
   "metadata": {},
   "source": [
    "## Stemming and its types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52cf54-d59d-4906-8bf5-43911861500a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e588bb5c-0d23-4442-862d-b5f7419926b4",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fb2c30e-fcc6-4838-a0a2-c97589d0dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification Problem\n",
    "## Comments of product is a positive review or negative review\n",
    "## Reviews----> eating, eat,eaten [going,gone,goes]--->go\n",
    "\n",
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aee7a1-6892-4511-bc62-9231312cb83a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fede9c44-23e9-4430-8a18-f81a6cff530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f89d96f0-1b60-4855-8def-30ba81e3caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e61bef60-5ed9-4da1-bfa7-3cfe2fffc083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to---->to\n",
      "become---->becom\n",
      "expert---->expert\n",
      "in---->in\n",
      "NLP---->nlp\n",
      ".---->.\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e01acd4-5cce-4346-86fb-2972adc83787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e343ab1-b7f0-4944-8c46-08fa2a4cf962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"sitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d9944-92f3-4488-9fcc-1c3deefb83c1",
   "metadata": {},
   "source": [
    "#### RegexpStemmer class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "451998f4-ca9b-4ce0-92f6-3c05f6dfc03e",
   "metadata": {},
   "source": [
    "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac4d9cd-067f-4e9e-91f0-7006daebd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fbc22b3-56d7-4dea-8039-1c8166b34404",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5fc7478-831e-40cb-b6b6-ae3ec70a5047",
   "metadata": {},
   "source": [
    "🔎 How it works\n",
    "\n",
    "RegexpStemmer removes word endings (suffixes) based on a regular expression.\n",
    "\n",
    "In your case:\n",
    "\n",
    "'ing$|s$|e$|able$' → remove suffixes ing, s, e, or able (when they appear at the end of the word).\n",
    "\n",
    "min=4 → ensures the word has at least 4 characters left after stemming (to avoid over-shortening words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29becab8-1a6a-4034-810e-74608ebfa53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6330a467-35c8-4d88-839f-c3a180aa0ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ac79b-950c-4194-a79b-29ab350a95c9",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fc6d961-0359-4471-af0c-b72bbc1dfb6c",
   "metadata": {},
   "source": [
    "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c88a9db3-3f53-417a-9233-b6ed5d20cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62cabd3a-8caa-4e3c-b371-74f98e445835",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballsstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3469127-43f9-4624-b976-8b083aa797a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+snowballsstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a39fbd53-8049-4754-add1-9edcc91066cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df944019-dd34-4688-b54b-bac66a28e744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballsstemmer.stem(\"fairly\"),snowballsstemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c402001-4095-4ed4-a778-fc2e7d3888b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('goe', 'goe')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballsstemmer.stem('goes'),stemming.stem('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c9132-9980-44e2-94bc-f0aafa814f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdb8720e-5cb8-43b3-ad3f-749d9c37600a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1bea05b-9325-4779-9a8c-eb7618515712",
   "metadata": {},
   "source": [
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
    "\n",
    "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c99951df-3494-4ad3-bb5d-245cb0b8e580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6796db0-1670-4a4e-8596-504c7435af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q&A,chatbots,text summarization\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d0e50ff-1065-47f5-b94a-abd8f350171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a26ac0e-7160-4c92-abed-dbd2abceffcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS- Noun-n\n",
    "verb-v\n",
    "adjective-a\n",
    "adverb-r\n",
    "'''\n",
    "lemmatizer.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68571a-df6b-4f4a-9b55-d988994d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e13396e-49e0-4506-9792-c900d643c723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->history\n",
      "finally---->finally\n",
      "finalized---->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6d9b0e5-acc3-41b6-bb0a-fe126caedeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49c711f8-21bd-4457-a259-d32c1005432f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\",pos='v'),lemmatizer.lemmatize(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f865518c-cccc-412f-b94c-efd00db4fb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\",pos='a'),lemmatizer.lemmatize(\"sportingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4642e61-acac-4e02-9dcc-261155a369dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ee0295b-9fd8-4d9f-9202-6c55245120c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6edc8129-7574-4996-a3de-5bbe866d7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dc0d70c-7df2-4f1e-80f3-1329801390a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "baa9dac6-c92b-495f-a3ac-c862c542b238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_count = stopwords.words('english')\n",
    "len(stopwords_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deb06830-02be-4957-bcc2-451970334857",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1fceab7-f271-4037-9d74-b26fe8ac786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8acedfb-5a5a-4392-be18-97d710c6d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a86ff5ab-4916-4b39-95bf-55643550cb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cb2baae-f092-49ff-bc6b-61dcaac672ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db328460-377f-4e7b-be2e-c5eaffb8bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ecf77e1c-5ee1-4d8f-b0a7-888e86539ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'on strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bf99a22-b746-4ce8-90aa-17113cb851cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db75905e-015e-41dd-8a0d-272223e89d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    #sentences[i]=sentences[i].lower()\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5624c678-8c14-4c57-9767-30dbe9f2d574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , come loot us , take .',\n",
       " 'yet do nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'believ india get first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " 'second vision india ’ develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top 5 nation world term gdp .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'see four mileston career']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44fd5b-a757-4eb9-94d1-98cd1b0d791e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "859eb4f8-5941-4452-bf5d-b04fa5a7920e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parts Of Speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90762e77-9062-478c-bbab-91c38264870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65d947ae-0650-4d54-9381-edc0010ee1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0436fb8a-134b-4c60-87cf-df0f4edce5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
       " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
       " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'My second vision for India’s development.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
       " 'We have a 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isn’t this incorrect?',\n",
       " 'I have a third vision.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr. Vikram Sarabhai of the Dept.',\n",
       " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I see four milestones in my career']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee6a2c08-ee32-4d68-a71b-17a55f8ba649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f57a5797-e9b7-4fb9-860e-2a6f5b60121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
      "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
      "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'JJ'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('Why', 'WRB'), ('?', '.')]\n",
      "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others.That', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('War', 'NNP'), ('Independence', 'NNP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
      "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('India', 'NNP'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
      "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n",
      "[('We', 'PRP'), ('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
      "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
      "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
      "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
      "[('I', 'PRP'), ('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
      "[('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
      "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
      "[('Both', 'DT'), ('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Vikram', 'NNP'), ('Sarabhai', 'NNP'), ('Dept', 'NNP'), ('.', '.')]\n",
      "[('space', 'NN'), (',', ','), ('Professor', 'NNP'), ('Satish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('succeeded', 'VBD'), ('Dr.', 'NNP'), ('Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('lucky', 'VBP'), ('worked', 'VBD'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('see', 'VBP'), ('four', 'CD'), ('milestones', 'NNS'), ('career', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "## We will find the Pos Tag\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    #sentences[i]=' '.join(words)# converting all the list of words into sentences\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381237f-874f-4626-86f9-7344f1409a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4d6aea1-e9f0-42c7-ab5a-8173e313566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taj', 'Mahal', 'is', 'a', 'beautiful', 'Monument']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Taj Mahal is a beautiful Monument\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04f265e0-64b3-4da7-9b32-51fc79af276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Monument', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(\"Taj Mahal is a beautiful Monument\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db05019-96aa-458d-8032-720fbc76cacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29f705a2-1a1e-4c4b-8ab7-47048e0cec7d",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24e21e33-f3e0-41eb-9e9f-3473e052dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7eba1423-3b8a-4500-8fcc-e9c3ec38f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6b4e03a-418b-49e0-bdeb-74e39803fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3981e5be-0037-4a66-a74a-a1b76782b435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "19ac237e-fcd3-4038-9e03-f8877ce61eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcc519-ba69-426d-86bd-574b32e6d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef394f-4f9d-4f0b-8ff3-1bfdab220bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii and worked at Google in 2005.\"\n",
    "\n",
    "# Tokenize → POS tagging → NER\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "ner_tree = ne_chunk(tagged)\n",
    "\n",
    "print(ner_tree)  # print in tree format\n",
    "ner_tree.draw()  # visualize entities\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b897a233-0650-4ea3-b726-e4228dfec5ac",
   "metadata": {},
   "source": [
    "⚡ What can we do with NER in NLTK?\n",
    "\n",
    "Information Extraction\n",
    "\n",
    "Extract people, places, organizations from text documents.\n",
    "\n",
    "Example: From news → detect all mentioned politicians, companies, countries.\n",
    "\n",
    "Question Answering Systems\n",
    "\n",
    "User asks: \"Who is the CEO of Google?\"\n",
    "\n",
    "NER helps you recognize Google → ORG and CEO → role, then find the right answer.\n",
    "\n",
    "Search Engines / Knowledge Graphs\n",
    "\n",
    "Identify entities and link them to a database like Wikidata.\n",
    "\n",
    "E.g., \"Apple\" → is it a company or a fruit?\n",
    "\n",
    "Text Summarization\n",
    "\n",
    "Highlight only important entities (who, where, when).\n",
    "\n",
    "Content Tagging\n",
    "\n",
    "Tag documents with detected entities for faster retrieval."
   ]
  },
  {
   "cell_type": "raw",
   "id": "974c2a41-b3fc-4466-b785-bb0267af041a",
   "metadata": {},
   "source": [
    "⚠️ Limitations of NLTK’s NER:\n",
    "\n",
    "It uses a rule-based + maxent classifier (not deep learning).\n",
    "\n",
    "Works best on formal text (like news), not messy social media/chat data.\n",
    "\n",
    "For state-of-the-art NER, libraries like spaCy, Stanza, or transformers (BERT, RoBERTa) are much more powerful."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c311933-6372-4811-8000-bdde5658f3b0",
   "metadata": {},
   "source": [
    "| Feature              | NLTK 🟡                          | spaCy 🟢                                 |\n",
    "| -------------------- | -------------------------------- | ---------------------------------------- |\n",
    "| **Goal**             | Research, teaching               | Industrial use, production-ready         |\n",
    "| **Speed**            | Slower                           | Very fast                                |\n",
    "| **NER accuracy**     | Low / outdated                   | High (neural models)                     |\n",
    "| **POS Tagging**      | Basic perceptron tagger          | Neural + more accurate                   |\n",
    "| **Ease of learning** | Good for beginners (educational) | Clean API, but less theory-heavy         |\n",
    "| **Corpora support**  | Huge (WordNet, etc.)             | Minimal (expects external data)          |\n",
    "| **Customization**    | Easy for rule-based pipelines    | Great for training custom NER, pipelines |\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "deb31f79-8b1d-41a4-b1f8-0764837011c1",
   "metadata": {},
   "source": [
    "✅ Verdict\n",
    "\n",
    "Use NLTK if you are learning NLP concepts or need access to linguistic resources (like WordNet, corpora).\n",
    "\n",
    "Use spaCy if you want fast, production-grade NLP with strong NER and dependency parsing.\n",
    "\n",
    "👉 Many practitioners actually combine both:\n",
    "\n",
    "NLTK for corpora, stopwords, linguistic resources.\n",
    "\n",
    "spaCy for tokenization, POS, NER, dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abba7ab-aea6-4be9-a6e3-feab3c20d2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb607f4-f747-4c18-9925-eb7f2d13733a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8fa29b-95cd-4263-9369-7224eb052db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41e4b23-2ec5-45c1-a4c3-90df14e6af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --force-reinstall h5py spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fcdb3ba-a83e-443e-b85f-a8a549aa8959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy: 3.8.7\n",
      "NumPy: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import spacy, numpy\n",
    "print(\"spaCy:\", spacy.__version__)\n",
    "print(\"NumPy:\", numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0c5c6e6-aa0b-4508-afcb-3eee5cacd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364d4d31-c464-4b95-b637-2bd02fd1149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 19.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 30.9 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e1c7f94-211f-4f23-8bfa-4cd007070dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a34899-7607-4442-a23e-fdaabd47a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English model (download with: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "699b4906-e383-4fc1-855e-6715d4261362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Barack Obama was born in Hawaii. He loves studying natural language processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "129e6789-9c7d-48ab-8e8c-137f3cbb015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a392513-1728-4dcb-b24c-4106535eb7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Barack Obama was born in Hawaii. He loves studying natural language processing."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f688760-b0c8-46fd-91ab-79aa6eeb403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Tokens:\n",
      "Barack\n",
      "Obama\n",
      "was\n",
      "born\n",
      "in\n",
      "Hawaii\n",
      ".\n",
      "He\n",
      "loves\n",
      "studying\n",
      "natural\n",
      "language\n",
      "processing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenization\n",
    "print(\"🔹 Tokens:\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aebe006-df61-4ac0-991d-99888b761727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Lemmatization:\n",
      "Barack          → Barack\n",
      "Obama           → Obama\n",
      "was             → be\n",
      "born            → bear\n",
      "in              → in\n",
      "Hawaii          → Hawaii\n",
      ".               → .\n",
      "He              → he\n",
      "loves           → love\n",
      "studying        → study\n",
      "natural         → natural\n",
      "language        → language\n",
      "processing      → processing\n",
      ".               → .\n"
     ]
    }
   ],
   "source": [
    "# 2. Lemmatization (spaCy doesn’t do stemming, only lemmatization)\n",
    "print(\"\\n🔹 Lemmatization:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} → {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "456ad73b-9716-43e3-957f-b526bd57e4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Stop words removed:\n",
      "['Barack', 'Obama', 'born', 'Hawaii', '.', 'loves', 'studying', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# 3. Stop Words Removal\n",
    "print(\"\\n🔹 Stop words removed:\")\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d5e499e-ed07-4a03-b9e4-135756640c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 POS tagging:\n",
      "Barack          → PROPN      (NNP)\n",
      "Obama           → PROPN      (NNP)\n",
      "was             → AUX        (VBD)\n",
      "born            → VERB       (VBN)\n",
      "in              → ADP        (IN)\n",
      "Hawaii          → PROPN      (NNP)\n",
      ".               → PUNCT      (.)\n",
      "He              → PRON       (PRP)\n",
      "loves           → VERB       (VBZ)\n",
      "studying        → VERB       (VBG)\n",
      "natural         → ADJ        (JJ)\n",
      "language        → NOUN       (NN)\n",
      "processing      → NOUN       (NN)\n",
      ".               → PUNCT      (.)\n"
     ]
    }
   ],
   "source": [
    "# 4. Part-of-Speech (POS) Tagging\n",
    "print(\"\\n🔹 POS tagging:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} → {token.pos_:<10} ({token.tag_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb9b6dbd-7ee1-476e-a271-820b178d5b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Named Entities:\n",
      "Barack Obama    → PERSON\n",
      "Hawaii          → GPE\n"
     ]
    }
   ],
   "source": [
    "# 5. Named Entity Recognition (NER)\n",
    "print(\"\\n🔹 Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<15} → {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7db79-9a55-4331-8c73-83c4aa7fcaa1",
   "metadata": {},
   "source": [
    "### Tokenization techniques using spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e345a9-ff3c-45ef-8c47-c4c6ef8fc440",
   "metadata": {},
   "source": [
    "#### Default type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3141df2-fb53-401a-b0a8-414e4f52afc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ca', \"n't\", 'go', 'to', 'New', '-', 'York', '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I can't go to New-York.\")\n",
    "[t.text for t in doc]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1c3d88d-495f-44ca-8a1a-29631f5dece6",
   "metadata": {},
   "source": [
    "spaCy splits contractions (can't → ca + n't), hyphens, punctuation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373f90c-3df7-4f9a-9bfe-47ae07a7ad60",
   "metadata": {},
   "source": [
    "#### Sentence tokenization (sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea5ba570-6714-41a4-8592-f49dde5780e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't go to New-York.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f572887-824e-4c97-ad67-6132fa447a36",
   "metadata": {},
   "source": [
    "#### Subword tokenization (morphological)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "129e9ede-2b70-41d6-9abb-18a53bbeed59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I I X\n",
      "ca c ca xx\n",
      "n't n n't x'x\n",
      "go g go xx\n",
      "to t to xx\n",
      "New N New Xxx\n",
      "- - - -\n",
      "York Y ork Xxxx\n",
      ". . . .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.prefix_, token.suffix_, token.shape_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546730c8-bf72-45b2-bd5c-48c87bb9a255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8982753-63fc-4990-a655-716579a38362",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Encoding techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87267c82-6b66-4bd9-b80c-685ef4bff846",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40f39e9-9c20-4946-8c2b-08ee1c9c1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6209cb-d317-41e7-8c5c-ea3e9ea5eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a9239b-2a4c-4b45-9c93-a6d109484829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv(r\"SMSSpamCollection.txt\",sep='\\t',names=[\"label\",\"message\"])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76e6b2f-45fe-4ccd-9f30-ac726ec7d8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Cleaning And Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13ac1331-3124-48a5-a46c-1b4853c5aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f588ca5b-ddd0-4b28-bf07-6b8cb327dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(0,len(messages)):\n",
    "    review=re.sub('[^a-zA-z]',' ',messages['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4f5dad5-a21b-4080-8854-2946046a4bc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936d6a2-92d8-448d-aab0-39b050e90f7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create Bag Of Words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efdde8d6-8cbe-46e7-b3a3-0d93bb876b62",
   "metadata": {},
   "source": [
    "🔹 1. Bag of Words (BoW) Concept\n",
    "\n",
    "The Bag of Words model is one of the simplest ways to represent text for machine learning.\n",
    "\n",
    "Imagine you have a vocabulary (set of unique words) from your dataset.\n",
    "\n",
    "Each document (sentence, SMS, email, etc.) is represented as a vector showing how many times each word from the vocabulary appears in it.\n",
    "\n",
    "The order of words is ignored (hence, “bag”).\n",
    "\n",
    "Example\n",
    "\n",
    "Suppose we have two sentences:\n",
    "\n",
    "\"I love NLP\"\n",
    "\n",
    "\"I love Python NLP\"\n",
    "\n",
    "Vocabulary = {I, love, NLP, Python}\n",
    "\n",
    "Sentence 1 → [1, 1, 1, 0]\n",
    "\n",
    "Sentence 2 → [1, 1, 1, 1]\n",
    "\n",
    "Each position corresponds to a word’s count in the sentence. ✅\n",
    "\n",
    "If you set binary=True, it becomes a presence/absence matrix:\n",
    "\n",
    "Sentence 1 → [1, 1, 1, 0]\n",
    "\n",
    "Sentence 2 → [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d0b5e73-af09-4727-b7d1-a2d6acec9bc4",
   "metadata": {},
   "source": [
    "🔹 2. N-grams in Text\n",
    "\n",
    "N-grams capture word sequences, not just single words.\n",
    "\n",
    "Unigrams (n=1) → single words (default BoW).\n",
    "\n",
    "Bigrams (n=2) → pairs of consecutive words.\n",
    "\n",
    "Trigrams (n=3) → triples of consecutive words.\n",
    "\n",
    "Example with \"I love NLP\"\n",
    "\n",
    "Unigrams: [I, love, NLP]\n",
    "\n",
    "Bigrams: [I love, love NLP]\n",
    "\n",
    "Trigrams: [I love NLP]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b01f29e-723d-4920-8eb0-dcb4996abc0e",
   "metadata": {},
   "source": [
    "🔹 3. How to Use N-grams in CountVectorizer\n",
    "\n",
    "You can control this with the ngram_range parameter:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example: use unigrams + bigrams\n",
    "cv = CountVectorizer(max_features=100, ngram_range=(1,2))\n",
    "\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"I love Python NLP\"\n",
    "]\n",
    "\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "print(cv.get_feature_names_out())  # show vocabulary\n",
    "print(X)\n",
    "\n",
    "output:\n",
    "['i' 'i love' 'love' 'love nlp' 'nlp' 'python' 'python nlp']\n",
    "[[1 1 1 1 1 0 0]\n",
    " [1 1 1 1 1 1 1]]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52bc1083-7e87-4c5e-adfc-55432e28e0cd",
   "metadata": {},
   "source": [
    "🔹 4. When to Use N-grams?\n",
    "\n",
    "Unigrams → good for general word frequency.\n",
    "\n",
    "Bigrams/Trigrams → capture context/phrases (e.g., \"New York\", \"credit card\", \"not good\").\n",
    "\n",
    "Downside → increases feature space size (curse of dimensionality)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d142b1a8-41c5-43a1-a1c6-611e95f1a538",
   "metadata": {},
   "source": [
    "🔹 What does binary=True do?\n",
    "\n",
    "Normally, CountVectorizer stores word counts:\n",
    "\n",
    "\"I love NLP love\" → I:1, love:2, NLP:1\n",
    "\n",
    "With binary=True, it only stores presence/absence:\n",
    "\n",
    "\"I love NLP love\" → I:1, love:1, NLP:1\n",
    "\n",
    "So, instead of how many times a word appears, it’s just 0 or 1."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c5b3a08-6967-4af9-8722-4acdf47c0817",
   "metadata": {},
   "source": [
    "🔹 When is binary=True useful?\n",
    "\n",
    "Short Text Classification (e.g., Spam detection, Sentiment analysis)\n",
    "\n",
    "In SMS, tweets, or product reviews, word frequency doesn’t matter much.\n",
    "\n",
    "Example: \"Buy buy buy now!!!\"\n",
    "\n",
    "Normal BoW → \"buy\": 3\n",
    "\n",
    "Binary BoW → \"buy\": 1\n",
    "\n",
    "Since repetition often doesn’t add much meaning, binary can work better.\n",
    "\n",
    "When word frequency is misleading\n",
    "\n",
    "Example:\n",
    "\n",
    "Sentence A: \"I love love love NLP\"\n",
    "\n",
    "Sentence B: \"I love NLP\"\n",
    "\n",
    "Both sentences mean almost the same.\n",
    "\n",
    "Binary BoW treats them equally, while count BoW exaggerates frequency.\n",
    "\n",
    "When you care about feature presence, not intensity\n",
    "\n",
    "In tasks like document classification, sometimes just knowing if a word appears is enough.\n",
    "\n",
    "For example, in spam filtering, if the word \"lottery\" or \"prize\" appears, that’s more important than how many times it appears.\n",
    "\n",
    "To reduce sparsity in very high-dimensional data\n",
    "\n",
    "In large corpora with lots of repeated words, frequencies can create noise.\n",
    "\n",
    "Binary representation simplifies the feature space.\n",
    "\n",
    "🔹 When NOT to use binary=True?\n",
    "\n",
    "If word frequency matters:\n",
    "\n",
    "Sentiment analysis on long reviews (e.g., \"very very good\" vs \"good\").\n",
    "\n",
    "Topic modeling (word counts indicate importance).\n",
    "\n",
    "Information retrieval (term frequency is crucial).\n",
    "\n",
    "👉 In such cases, keep the normal count-based BoW or even better use TF-IDF, which balances frequency with importance.\n",
    "\n",
    "✅ Summary:\n",
    "\n",
    "Use binary=True → when presence matters more than frequency (short texts, spam filtering, keyword detection).\n",
    "\n",
    "Use normal BoW/TF-IDF → when frequency carries meaning (long texts, topic modeling, sentiment intensity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c18889ae-9439-4ec2-b6b7-7de5019f9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Bag OF Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## for Binary BOW enable binary=True\n",
    "cv=CountVectorizer(max_features=100,binary=True)\n",
    "# Builds a vocabulary of at most 100 words (most frequent).\n",
    "# Creates binary vectors for your corpus.\n",
    "# If you add ngram_range=(1,2), it will also include bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35630d0-c313-49f9-800b-dd0ee95ec2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a52907f2-7a28-4372-a02b-5fb47ba0d5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ..., 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ..., 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ..., 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3510b5f-87d5-4fe8-8b15-2da4377e6a31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0214c1b1-6cc9-4ce8-b741-1333d7302b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go': 22,\n",
       " 'great': 25,\n",
       " 'got': 24,\n",
       " 'wat': 90,\n",
       " 'ok': 56,\n",
       " 'free': 18,\n",
       " 'win': 94,\n",
       " 'text': 77,\n",
       " 'txt': 85,\n",
       " 'say': 67,\n",
       " 'alreadi': 0,\n",
       " 'think': 80,\n",
       " 'hey': 28,\n",
       " 'week': 92,\n",
       " 'back': 3,\n",
       " 'like': 38,\n",
       " 'still': 73,\n",
       " 'send': 69,\n",
       " 'even': 15,\n",
       " 'friend': 19,\n",
       " 'prize': 62,\n",
       " 'claim': 7,\n",
       " 'call': 4,\n",
       " 'mobil': 47,\n",
       " 'co': 8,\n",
       " 'home': 30,\n",
       " 'want': 89,\n",
       " 'today': 82,\n",
       " 'cash': 6,\n",
       " 'day': 12,\n",
       " 'repli': 64,\n",
       " 'www': 96,\n",
       " 'right': 65,\n",
       " 'thank': 78,\n",
       " 'take': 75,\n",
       " 'time': 81,\n",
       " 'use': 87,\n",
       " 'messag': 44,\n",
       " 'oh': 55,\n",
       " 'ye': 97,\n",
       " 'make': 42,\n",
       " 'way': 91,\n",
       " 'feel': 16,\n",
       " 'dont': 14,\n",
       " 'miss': 46,\n",
       " 'ur': 86,\n",
       " 'tri': 84,\n",
       " 'da': 11,\n",
       " 'lor': 39,\n",
       " 'meet': 43,\n",
       " 'realli': 63,\n",
       " 'get': 20,\n",
       " 'know': 33,\n",
       " 'love': 40,\n",
       " 'let': 37,\n",
       " 'work': 95,\n",
       " 'wait': 88,\n",
       " 'yeah': 98,\n",
       " 'tell': 76,\n",
       " 'pleas': 61,\n",
       " 'msg': 49,\n",
       " 'see': 68,\n",
       " 'pl': 60,\n",
       " 'need': 51,\n",
       " 'tomorrow': 83,\n",
       " 'hope': 31,\n",
       " 'well': 93,\n",
       " 'lt': 41,\n",
       " 'gt': 26,\n",
       " 'ask': 1,\n",
       " 'morn': 48,\n",
       " 'happi': 27,\n",
       " 'sorri': 72,\n",
       " 'give': 21,\n",
       " 'new': 52,\n",
       " 'find': 17,\n",
       " 'year': 99,\n",
       " 'later': 35,\n",
       " 'pick': 59,\n",
       " 'good': 23,\n",
       " 'come': 9,\n",
       " 'said': 66,\n",
       " 'hi': 29,\n",
       " 'babe': 2,\n",
       " 'im': 32,\n",
       " 'much': 50,\n",
       " 'stop': 74,\n",
       " 'one': 57,\n",
       " 'night': 53,\n",
       " 'servic': 70,\n",
       " 'dear': 13,\n",
       " 'thing': 79,\n",
       " 'contact': 10,\n",
       " 'last': 34,\n",
       " 'min': 45,\n",
       " 'number': 54,\n",
       " 'leav': 36,\n",
       " 'sleep': 71,\n",
       " 'care': 5,\n",
       " 'phone': 58}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe611e8-bae6-4142-b7c4-a5f90e5bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Bag OF Words model with ngram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## for Binary BOW enable binary=True\n",
    "cv=CountVectorizer(max_features=100,binary=True,ngram_range=(2,3))\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "321a0a6d-eef2-4bf9-9767-202b61f298a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'free entri': 32,\n",
       " 'claim call': 17,\n",
       " 'call claim': 3,\n",
       " 'free call': 31,\n",
       " 'chanc win': 16,\n",
       " 'txt word': 91,\n",
       " 'let know': 55,\n",
       " 'go home': 36,\n",
       " 'pleas call': 70,\n",
       " 'lt gt': 61,\n",
       " 'want go': 97,\n",
       " 'like lt': 56,\n",
       " 'like lt gt': 57,\n",
       " 'sorri call': 83,\n",
       " 'call later': 11,\n",
       " 'sorri call later': 84,\n",
       " 'ur award': 92,\n",
       " 'call custom': 4,\n",
       " 'custom servic': 24,\n",
       " 'cash prize': 15,\n",
       " 'call custom servic': 5,\n",
       " 'po box': 71,\n",
       " 'tri contact': 89,\n",
       " 'draw show': 28,\n",
       " 'show prize': 81,\n",
       " 'prize guarante': 75,\n",
       " 'guarante call': 43,\n",
       " 'valid hr': 95,\n",
       " 'draw show prize': 29,\n",
       " 'show prize guarante': 82,\n",
       " 'prize guarante call': 76,\n",
       " 'select receiv': 78,\n",
       " 'privat account': 72,\n",
       " 'account statement': 0,\n",
       " 'call identifi': 6,\n",
       " 'identifi code': 49,\n",
       " 'code expir': 21,\n",
       " 'privat account statement': 73,\n",
       " 'call identifi code': 7,\n",
       " 'identifi code expir': 50,\n",
       " 'urgent mobil': 94,\n",
       " 'call landlin': 10,\n",
       " 'wat time': 98,\n",
       " 'ur mob': 93,\n",
       " 'gud ni': 45,\n",
       " 'new year': 65,\n",
       " 'send stop': 80,\n",
       " 'get back': 34,\n",
       " 'co uk': 20,\n",
       " 'nice day': 66,\n",
       " 'lt decim': 59,\n",
       " 'decim gt': 26,\n",
       " 'lt decim gt': 60,\n",
       " 'good morn': 38,\n",
       " 'good night': 39,\n",
       " 'repli call': 77,\n",
       " 'last night': 54,\n",
       " 'pick phone': 68,\n",
       " 'pl send': 69,\n",
       " 'send messag': 79,\n",
       " 'great day': 40,\n",
       " 'suit land': 85,\n",
       " 'land row': 53,\n",
       " 'suit land row': 86,\n",
       " 'good afternoon': 37,\n",
       " 'take care': 87,\n",
       " 'call mobileupd': 12,\n",
       " 'call optout': 13,\n",
       " 'gt min': 42,\n",
       " 'lt gt min': 62,\n",
       " 'txt stop': 90,\n",
       " 'date servic': 25,\n",
       " 'call land': 8,\n",
       " 'land line': 51,\n",
       " 'line claim': 58,\n",
       " 'claim valid': 18,\n",
       " 'guarante call land': 44,\n",
       " 'call land line': 9,\n",
       " 'land line claim': 52,\n",
       " 'claim valid hr': 19,\n",
       " 'gt lt': 41,\n",
       " 'hope good': 48,\n",
       " 'free text': 33,\n",
       " 'prize claim': 74,\n",
       " 'nd attempt': 64,\n",
       " 'attempt contact': 1,\n",
       " 'ok lor': 67,\n",
       " 'want come': 96,\n",
       " 'everi week': 30,\n",
       " 'come home': 23,\n",
       " 'happi new': 46,\n",
       " 'happi new year': 47,\n",
       " 'nation rate': 63,\n",
       " 'week txt': 99,\n",
       " 'tell ur': 88,\n",
       " 'gift voucher': 35,\n",
       " 'await collect': 2,\n",
       " 'dont know': 27,\n",
       " 'come back': 22,\n",
       " 'call per': 14}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a737b34-6828-4d8b-be35-39e18a2d723c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da062e44-4a66-47ad-bae2-84bd3bb133b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45a78eb3-2edc-4bf0-84e3-70f2c2f0f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "messages=pd.read_csv('SMSSpamCollection.txt',\n",
    "                    sep='\\t',names=[\"label\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc326e14-6137-4fe4-8910-b4d31068e607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e945c6d-0e2e-43f4-9c1b-97fd1ff6a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Cleaning And Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb667c49-ece6-4dc6-bb33-18ad53131c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordlemmatize=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d4e1c7e-708a-4e2e-b281-5f0b65b1f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(0,len(messages)):\n",
    "    review=re.sub('[^a-zA-z]',' ',messages['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[wordlemmatize.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e033a435-c80a-45e0-84ac-a87cf4297875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazy available bugis n great world la e buffet cine got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry wkly comp win fa cup final tkts st may text fa receive entry question std txt rate c apply',\n",
       " 'u dun say early hor u c already say',\n",
       " 'nah think go usf life around though']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57cf2b-a626-4118-bb0e-28ffb727d8dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create TF-IDF And NGrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60eee8-5091-40d4-b025-6d4339b9ffe6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Theory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81c77429-826e-436c-a11f-6551b37c94a5",
   "metadata": {},
   "source": [
    "🔹 TF-IDF Theory\n",
    "\n",
    "TF-IDF stands for Term Frequency – Inverse Document Frequency.\n",
    "It adjusts BoW so that:\n",
    "\n",
    "Words common across all documents (like the, is, and) get low weight.\n",
    "\n",
    "Words unique to a document (like lottery, prize) get high weight."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c054b82-32b0-46f7-90c7-301895c9e6a8",
   "metadata": {},
   "source": [
    "1. Term Frequency (TF)\n",
    "How often a term appears in a document.\n",
    "TF(t,d)=count of term t in document d/total terms in document d\n",
    "2. Inverse Document Frequency (IDF)\n",
    "IDF(t)=log(N/1+DF(t))\n",
    "N = total number of documents\n",
    "DF(t) = number of documents containing term 𝑡\n",
    "Rare terms (appear in fewer docs) → higher IDF.\n",
    "Frequent terms (appear in many docs) → lower IDF.\n",
    "3. TF-IDF score\n",
    "TFIDF(t,d)=TF(t,d)×IDF(t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10b2b8f5-31b3-44f2-bbf2-05f14cf3d0a6",
   "metadata": {},
   "source": [
    "“TF-IDF assigns higher weight to words that occur frequently in a document but are rare across the entire corpus, and lower weight to very common words.”"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83549010-e97d-4c02-96c2-cd69717a04fa",
   "metadata": {},
   "source": [
    "Doc1: \"I love machine learning\"\n",
    "Doc2: \"I love deep learning\"\n",
    "Doc3: \"machine learning is great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9609ff91-eca9-451c-8bbf-ecca280d3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example: \"deep\" in Doc2\n",
    "\n",
    "TF(\"deep\", Doc2) = 1/4 = 0.25\n",
    "\n",
    "DF(\"deep\") = 1 (only appears in Doc2)\n",
    "\n",
    "IDF(\"deep\") = log(3 / (1+1)) = log(1.5) ≈ 0.405\n",
    "\n",
    "TF-IDF = 0.25 × 0.405 ≈ 0.101\n",
    "\n",
    "So \"deep\" is weighted higher than \"machine\" because it’s more unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3461eba0-bbd2-4621-9901-ff8a370b9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ee05130-8f48-4a68-9647-39d6671cd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(max_features=100)\n",
    "X=tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4af8e1e-d86f-489a-9dce-0712106e1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ab8edd-8fea-4370-a2ea-95c73ac0ff7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca4e1ae0-4642-4195-9c68-0b66f4fbf038",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(max_features=100,ngram_range=(2,2))\n",
    "X=tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da3bc52f-3663-4322-8da7-1779b76c141c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'free entry': 33,\n",
       " 'claim call': 17,\n",
       " 'call claim': 3,\n",
       " 'claim code': 18,\n",
       " 'free call': 32,\n",
       " 'chance win': 16,\n",
       " 'txt word': 88,\n",
       " 'let know': 54,\n",
       " 'please call': 66,\n",
       " 'lt gt': 58,\n",
       " 'want go': 97,\n",
       " 'like lt': 55,\n",
       " 'sm ac': 78,\n",
       " 'sorry call': 79,\n",
       " 'call later': 8,\n",
       " 'ur awarded': 89,\n",
       " 'hi hi': 46,\n",
       " 'call customer': 4,\n",
       " 'customer service': 25,\n",
       " 'cash prize': 15,\n",
       " 'po box': 68,\n",
       " 'trying contact': 85,\n",
       " 'draw show': 30,\n",
       " 'show prize': 77,\n",
       " 'prize guaranteed': 72,\n",
       " 'guaranteed call': 42,\n",
       " 'valid hr': 95,\n",
       " 'selected receive': 74,\n",
       " 'private account': 70,\n",
       " 'account statement': 0,\n",
       " 'statement show': 81,\n",
       " 'call identifier': 5,\n",
       " 'identifier code': 49,\n",
       " 'code expires': 22,\n",
       " 'urgent mobile': 94,\n",
       " 'caller prize': 12,\n",
       " 'call landline': 7,\n",
       " 'wat time': 98,\n",
       " 'ur mob': 92,\n",
       " 'gud ni': 44,\n",
       " 'new year': 62,\n",
       " 'send stop': 76,\n",
       " 'ur mobile': 93,\n",
       " 'co uk': 21,\n",
       " 'gud mrng': 43,\n",
       " 'nice day': 63,\n",
       " 'lt decimal': 57,\n",
       " 'decimal gt': 27,\n",
       " 'txt nokia': 86,\n",
       " 'good morning': 37,\n",
       " 'ur friend': 91,\n",
       " 'good night': 38,\n",
       " 'camcorder reply': 13,\n",
       " 'reply call': 73,\n",
       " 'last night': 52,\n",
       " 'camera phone': 14,\n",
       " 'pick phone': 65,\n",
       " 'pls send': 67,\n",
       " 'send message': 75,\n",
       " 'great day': 39,\n",
       " 'ur cash': 90,\n",
       " 'suite land': 82,\n",
       " 'land row': 51,\n",
       " 'good afternoon': 36,\n",
       " 'take care': 83,\n",
       " 'double min': 29,\n",
       " 'call mobileupd': 9,\n",
       " 'call optout': 10,\n",
       " 'gt min': 41,\n",
       " 'txt stop': 87,\n",
       " 'dating service': 26,\n",
       " 'pobox wq': 69,\n",
       " 'last week': 53,\n",
       " 'mobile number': 59,\n",
       " 'call land': 6,\n",
       " 'land line': 50,\n",
       " 'line claim': 56,\n",
       " 'claim valid': 20,\n",
       " 'gt lt': 40,\n",
       " 'hope good': 48,\n",
       " 'free text': 34,\n",
       " 'holiday cash': 47,\n",
       " 'prize claim': 71,\n",
       " 'nd attempt': 61,\n",
       " 'attempt contact': 1,\n",
       " 'claim ur': 19,\n",
       " 'ok lor': 64,\n",
       " 'want come': 96,\n",
       " 'every week': 31,\n",
       " 'come home': 24,\n",
       " 'happy new': 45,\n",
       " 'national rate': 60,\n",
       " 'st week': 80,\n",
       " 'week txt': 99,\n",
       " 'tell ur': 84,\n",
       " 'gift voucher': 35,\n",
       " 'await collection': 2,\n",
       " 'dont know': 28,\n",
       " 'come back': 23,\n",
       " 'call per': 11}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7c35b-a665-4d27-8763-7b1d1119dd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc020987-d738-42fa-a1dd-909ecf693bc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f90f2e9e-3d7e-41ab-95f7-e5b51fe31f57",
   "metadata": {},
   "source": [
    "1. What is Word2Vec?\n",
    "\n",
    "Word2Vec is a neural embedding model that converts words into dense vectors (embeddings).\n",
    "\n",
    "Instead of representing words as sparse one-hot vectors (very high dimensional), Word2Vec represents each word in a continuous vector space where similar words are closer together.\n",
    "\n",
    "Example:\n",
    "\n",
    "king ≈ queen\n",
    "\n",
    "cricket ≈ football\n",
    "\n",
    "doctor ≈ nurse"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f427397-b834-4c02-a1fa-1e3b50c4d1fd",
   "metadata": {},
   "source": [
    "2. How Word2Vec Works\n",
    "\n",
    "Word2Vec is trained on a large text corpus to predict context. It uses a shallow neural network (1 hidden layer).\n",
    "\n",
    "There are two main architectures:\n",
    "\n",
    "2.1 CBOW (Continuous Bag of Words)\n",
    "\n",
    "Input: Context words around a target word\n",
    "\n",
    "Output: The target word\n",
    "\n",
    "Example: Sentence: the cat sits on the mat\n",
    "\n",
    "Context: [the, cat, on, the] → Predict: sits\n",
    "\n",
    "Good when data is smaller (faster, efficient)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "331544e9-ba5c-4fc2-b4f6-cd35c0736727",
   "metadata": {},
   "source": [
    "2.2 Skip-Gram\n",
    "\n",
    "Input: A target word\n",
    "\n",
    "Output: Context words\n",
    "\n",
    "Example: Sentence: the cat sits on the mat\n",
    "\n",
    "Input: sits → Predict: [the, cat, on, the, mat]\n",
    "\n",
    "Better when data is large, captures rare words well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce8763d0-3940-4aa8-9544-97813a5ea3a2",
   "metadata": {},
   "source": [
    "3. Avg Word2Vec\n",
    "\n",
    "Word2Vec gives embeddings at the word level.\n",
    "\n",
    "To represent an entire sentence/document, we often average the embeddings of all words in it.\n",
    "\n",
    "Example: Sentence \"I love cricket\"\n",
    "\n",
    "vec(I) + vec(love) + vec(cricket)\n",
    "\n",
    "Divide by 3 → sentence embedding\n",
    "\n",
    "This is simple, but surprisingly effective for tasks like text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645897c-acdb-4d8e-8cd0-a049ce47d2c9",
   "metadata": {},
   "source": [
    "4. Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9621a5b0-97bf-4391-a7d8-d220e2010886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patralapati\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\Patralapati\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\Patralapati\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5e162d-5d61-4d0f-8b6a-29fb0f2858c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"the cat sits on the mat\",\n",
    "    \"dogs and cats are friends\",\n",
    "    \"I love playing cricket\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"football and cricket are popular sports\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b5ef2b-af0c-4942-b602-a2d44a3b82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: tokenize\n",
    "tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3484fa-b8c5-4a6a-aae2-288ceedb8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CBOW ----\n",
    "# sg=0 → CBOW\n",
    "cbow_model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=3, min_count=1, sg=0, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d932cb9-d163-4f01-ac6c-4800137bb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Skip-Gram ----\n",
    "# sg=1 → Skip-Gram\n",
    "skipgram_model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=3, min_count=1, sg=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d05185d-4d52-4521-ab82-7ade52eac991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'cat' (CBOW):\n",
      "[-0.01723936  0.00733263  0.01037713  0.01148337  0.01493676 -0.01233252\n",
      "  0.00220988  0.01209561 -0.00568326 -0.01234465 -0.00082223 -0.01673907\n",
      " -0.01119649  0.01420941  0.00670795  0.01444903  0.01360241  0.01506522\n",
      " -0.00758151 -0.00112561  0.00469409 -0.00903994  0.01677604 -0.01971959\n",
      "  0.0135323   0.00582696 -0.00986245  0.00879834 -0.00348168  0.01342432\n",
      "  0.01993182 -0.00872768 -0.00120151 -0.0113922   0.00769839  0.00557269\n",
      "  0.01378196  0.01220344  0.01907759  0.01854652  0.01579399 -0.01397833\n",
      " -0.0183121  -0.0007089  -0.00619818  0.0157904   0.01187767 -0.00309238\n",
      "  0.00302026  0.00357967]\n",
      "\n",
      "Most similar to 'cricket' (Skip-Gram):\n",
      "[('i', 0.19010192155838013), ('mat', 0.0449172779917717), ('dogs', -0.010175946168601513), ('the', -0.014623512513935566), ('sports', -0.023241858929395676), ('on', -0.04407211393117905), ('football', -0.044128309935331345), ('friends', -0.08944642543792725), ('popular', -0.09488877654075623), ('cats', -0.12279319018125534)]\n"
     ]
    }
   ],
   "source": [
    "# Access embeddings\n",
    "print(\"Word vector for 'cat' (CBOW):\")\n",
    "print(cbow_model.wv['cat'])\n",
    "\n",
    "print(\"\\nMost similar to 'cricket' (Skip-Gram):\")\n",
    "print(skipgram_model.wv.most_similar('cricket'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefac45-13bb-43a6-b7b9-ffdcbb9b462c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f68587e4-c22c-4210-922c-31832fc2a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\patralapati\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71d22972-beea-4145-a540-dc98b7e2eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patralapati\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\Patralapati\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\Patralapati\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b67fc95-75b8-4232-b19e-2f0ede4a4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1995c8-b2c2-4d3c-ace6-621e2e130c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## References: https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97c57e0c-d1ad-49d6-94c1-4ba34f582946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb123330-7f54-45da-89b2-cec68ca253d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.126, 0.0298, 0.00861, 0.14, -0.0256, -0.0361, 0.112, -0.198, 0.0513, 0.363, -0.242, -0.303, -0.178, -0.0249, -0.168, -0.17, 0.0347, 0.00522, 0.0464, 0.129, 0.137, 0.113, 0.0596, 0.137, 0.101, -0.177, -0.252, 0.0598, 0.342, -0.0311, 0.104, 0.0618, 0.125, 0.4, -0.322, 0.084, 0.0391, 0.00586, 0.0703, 0.173, 0.139, -0.231, 0.283, 0.143, 0.342, -0.0239, -0.11, 0.0332, -0.0547, 0.0153, -0.162, 0.158, -0.26, 0.0201, -0.163, 0.00136, -0.145, -0.0569, 0.043, -0.0247, 0.186, 0.447, 0.00958, 0.132, 0.0986, -0.186, -0.1, -0.134, -0.125, 0.283, 0.123, 0.0532, -0.178, 0.0859, -0.0219, 0.0205, -0.14, 0.0251, 0.139, -0.105, 0.139, 0.0889, -0.0752, -0.0214, 0.173, 0.0464, -0.266, 0.00891, 0.149, 0.0378, 0.238, -0.125, -0.218, -0.182, 0.0298, 0.0571, -0.0289, 0.0125, 0.0967, -0.231, 0.0581, 0.0669, 0.0708, -0.309, -0.215, 0.146, -0.428, -0.0094, 0.154, -0.0767, 0.289, 0.277, -0.000486, -0.137, 0.324, -0.246, -0.00304, -0.212, 0.125, 0.27, 0.204, 0.0825, -0.201, -0.16, -0.0378, -0.12, 0.115, -0.041, -0.0396, -0.0898, 0.00635, 0.203, 0.187, 0.273, 0.063, 0.142, -0.0981, 0.139, 0.183, 0.174, 0.174, -0.237, 0.179, 0.0635, 0.236, -0.209, 0.0874, -0.166, -0.0791, 0.243, -0.0889, 0.127, -0.217, -0.174, -0.359, -0.0825, -0.0649, 0.0508, 0.136, -0.0747, -0.164, 0.0115, 0.445, -0.216, -0.111, -0.192, 0.171, -0.125, 0.00266, 0.192, -0.175, 0.14, 0.293, 0.113, 0.0596, -0.064, 0.0996, -0.0272, 0.0197, 0.0427, -0.246, 0.064, -0.226, -0.169, 0.0029, 0.082, 0.342, 0.0432, 0.133, 0.143, 0.0762, 0.0598, -0.119, 0.00275, -0.063, -0.0272, -0.00482, -0.082, -0.0249, -0.4, -0.107, 0.0425, 0.0776, -0.117, 0.0737, -0.0923, 0.108, 0.158, 0.0425, 0.127, 0.0361, 0.268, -0.101, -0.303, -0.0576, 0.0505, 0.000526, -0.207, -0.139, -0.00897, -0.0278, -0.142, 0.207, -0.158, 0.128, 0.149, -0.0225, -0.0845, 0.123, 0.216, -0.214, -0.312, -0.373, 0.00409, 0.107, 0.107, 0.0732, 0.00897, -0.0388, -0.13, 0.149, -0.215, -0.00184, 0.0991, 0.157, -0.114, -0.205, 0.0991, 0.369, -0.197, 0.0354, 0.109, 0.132, 0.167, 0.235, 0.105, -0.496, -0.164, -0.156, -0.0522, 0.103, 0.243, -0.188, 0.0508, -0.0938, -0.0669, 0.0227, 0.0762, 0.289, 0.311, -0.0537, 0.229, 0.0251, 0.0679, -0.121, -0.216, -0.273, -0.0308, -0.338, 0.153, 0.233, -0.208, 0.373, 0.082, 0.252, -0.0762, -0.0466, -0.0223, 0.0299, -0.0593, -0.00467, -0.244, -0.21, -0.287, -0.0454, -0.178, -0.279, -0.0859, 0.0913, 0.252], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e3cc7e9-0c87-4493-bced-30d038234999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c2fe43a-a14e-4bd1-b06f-6c1c566088bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.367, -0.122, 0.285, 0.0815, 0.032, -0.032, 0.135, -0.273, 0.00946, -0.107, 0.248, -0.605, 0.0503, 0.299, 0.0957, 0.14, -0.0542, 0.291, 0.285, 0.151, -0.289, -0.0347, 0.0182, -0.393, 0.246, 0.252, -0.0986, 0.322, 0.449, -0.137, -0.234, 0.0413, -0.216, 0.17, 0.0256, 0.015, -0.0376, 0.00696, 0.4, 0.21, 0.118, -0.042, 0.234, 0.203, -0.187, -0.246, 0.312, -0.26, -0.107, 0.104, -0.18, 0.0571, -0.00742, -0.0559, 0.0762, -0.414, -0.365, -0.336, -0.154, -0.239, -0.373, 0.00227, -0.352, 0.0864, 0.127, 0.222, -0.0986, 0.109, 0.365, -0.0566, 0.0566, -0.109, -0.167, -0.0454, -0.2, -0.123, 0.132, -0.132, 0.103, -0.342, -0.157, 0.204, 0.0439, 0.244, -0.032, 0.32, -0.0442, 0.108, -0.0498, -0.00952, 0.246, -0.0559, 0.0408, -0.0178, -0.0295, 0.165, 0.504, -0.281, 0.0981, 0.0181, -0.184, 0.254, 0.226, 0.0164, 0.182, 0.139, 0.334, 0.14, 0.0146, -0.0289, -0.084, 0.15, 0.168, 0.229, 0.359, 0.123, -0.328, -0.156, 0.277, 0.0177, -0.146, -0.00452, -0.0447, 0.176, -0.375, 0.117, -0.14, 0.256, -0.196, -0.0258, -0.0542, -0.0251, -0.193, -0.0317, -0.0874, -0.133, -0.0212, 0.434, -0.052, 0.0347, 0.0801, 0.0342, 0.199, -0.0239, -0.237, 0.193, 0.0732, -0.287, 0.125, 0.0845, 0.131, -0.22, -0.161, -0.264, -0.547, -0.297, 0.0344, -0.287, -0.193, -0.161, -0.385, -0.215, -0.00623, -0.128, -0.1, -0.621, 0.379, -0.459, 0.145, -0.0913, -0.309, 0.224, 0.0786, -0.217, 0.0879, -0.167, 0.0115, -0.254, -0.0625, 0.00604, 0.156, 0.438, -0.224, -0.232, 0.275, 0.239, 0.0449, -0.0752, 0.574, -0.0261, -0.122, 0.244, -0.338, 0.0859, -0.0771, 0.0486, 0.144, 0.426, -0.043, -0.108, 0.12, -0.191, -0.213, -0.287, -0.115, -0.204, -0.0206, -0.254, 0.0825, -0.0398, -0.157, 0.135, 0.208, -0.179, -0.02, -0.0835, -0.121, 0.043, -0.194, -0.133, -0.0217, -0.235, -0.363, 0.151, 0.0933, 0.163, 0.102, -0.428, 0.283, 0.000275, -0.32, 0.0168, 0.406, -0.0525, 0.0791, -0.142, 0.527, -0.127, 0.475, -0.0664, 0.342, -0.179, 0.369, -0.205, 0.00583, -0.185, -0.0889, -0.182, -0.0481, 0.439, 0.213, -0.0308, 0.0933, 0.24, 0.239, 0.252, -0.0199, 0.125, -0.0474, -0.0214, 0.0312, 0.0305, 0.279, 0.0908, -0.202, -0.022, -0.264, 0.0879, -0.107, -0.249, -0.0122, 0.174, -0.0991, 0.0728, 0.26, -0.461, 0.359, -0.226, 0.0188, -0.22, -0.209, -0.151, 0.0864, 0.0112, 0.0693, -0.0299, 0.144, 0.189, -0.133, 0.473, -0.141, -0.0253, 0.191, -0.264, -0.14, 0.109, 0.0198, 0.249, -0.143, 0.0415], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['cricket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79ec3595-955e-444a-9902-096db2d526cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cricketing', 0.8372225761413574),\n",
       " ('cricketers', 0.8165745735168457),\n",
       " ('Test_cricket', 0.8094819188117981),\n",
       " ('Twenty##_cricket', 0.8068488240242004),\n",
       " ('Twenty##', 0.7624265551567078),\n",
       " ('Cricket', 0.75413978099823),\n",
       " ('cricketer', 0.7372578382492065),\n",
       " ('twenty##', 0.7316356897354126),\n",
       " ('T##_cricket', 0.7304614186286926),\n",
       " ('West_Indies_cricket', 0.6987985968589783)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ad5ab03-9e7b-4274-9b68-f983c20132b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glad', 0.7408890724182129),\n",
       " ('pleased', 0.6632170677185059),\n",
       " ('ecstatic', 0.6626912355422974),\n",
       " ('overjoyed', 0.6599286794662476),\n",
       " ('thrilled', 0.6514049172401428),\n",
       " ('satisfied', 0.6437949538230896),\n",
       " ('proud', 0.636042058467865),\n",
       " ('delighted', 0.627237856388092),\n",
       " ('disappointed', 0.6269949674606323),\n",
       " ('excited', 0.6247665286064148)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11d7559f-4e9b-4df3-a5be-d2d2f9cd336c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53541523"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity(\"hockey\",\"sports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23f4997d-acbd-4e4e-8b5e-135061a86ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=wv['king']-wv['man']+wv['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1784c0f2-d65a-449d-8e00-20b6e3359ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.043, -0.178, -0.129, 0.115, 0.00269, -0.102, 0.196, -0.18, 0.0195, 0.41, -0.368, -0.396, -0.157, 0.00146, -0.093, -0.116, -0.0552, -0.108, 0.0791, 0.199, 0.239, 0.0634, -0.0217, 0, 0.0472, -0.218, -0.345, 0.0637, 0.316, -0.198, 0.0859, -0.0812, -0.0371, 0.316, -0.342, -0.0469, 0.0977, 0.084, -0.0972, 0.0518, -0.05, -0.221, 0.229, 0.126, 0.249, 0.021, -0.11, 0.0581, -0.0336, 0.13, 0.0242, 0.0348, -0.26, 0.242, -0.322, 0.0145, -0.159, -0.0837, 0.165, 0.00159, 0.31, 0.316, 0.00739, 0.241, 0.0491, -0.0986, 0.0291, 0.149, -0.0483, 0.236, 0.221, 0.125, -0.139, 0.154, 0.0719, 0.13, -0.106, 0.0601, 0.315, 0.11, 0.085, 0.0771, -0.0217, 0.0612, -0.19, 0.208, -0.163, 0.114, 0.201, 0.0607, 0.128, -0.311, -0.28, -0.156, 0.0415, 0.0988, 0.17, -0.0349, 0.208, -0.099, 0.00439, -0.0728, -0.0425, -0.409, -0.276, 0.164, -0.558, -0.202, 0.212, -0.0981, 0.231, 0.276, 0.168, -0.045, 0.172, -0.377, -0.00352, -0.302, 0.174, 0.33, 0.201, 0.118, -0.138, -0.107, 0.0862, 0.106, 0.145, 0.00305, 0.0181, 0.0374, 0.00732, 0.133, 0.0962, 0.336, 0.181, 0.241, -0.085, -0.11, 0.212, 0.00586, 0.162, -0.416, 0.139, 0.102, 0.145, -0.109, 0.0488, 0.0615, -0.17, 0.0328, 0.0557, 0.147, -0.0225, -0.274, -0.282, -0.139, -0.182, 0.0934, 0.121, -0.00537, -0.188, 0.000305, 0.553, -0.0972, -0.182, -0.152, 0.0776, -0.238, -0.0264, 0.226, -0.303, 0.135, 0.323, 0.126, 0.0352, -0.204, 0.296, 0.103, -0.00476, 0.169, -0.351, 0.0247, -0.391, -0.271, 0.0185, 0.104, 0.284, 0.135, -0.0596, 0.188, 0.0888, 0.0325, -0.0898, 0.0545, 0.0565, 0.157, -0.0097, -0.0708, 0.0571, -0.309, -0.192, 0.0483, 0.0522, -0.16, -0.0449, -0.0737, 0.0552, 0.212, 0.205, -0.0273, 0.0786, 0.319, -0.157, -0.393, 0.04, 0.0994, -0.0197, -0.0825, 0.0254, 0.0311, -0.0364, 0.0149, 0.221, -0.0598, 0.0615, -0.0714, -0.04, -0.104, 0.0923, 0.272, -0.23, -0.263, -0.562, 0.0139, 0.11, 0.0723, 0.0459, -0.0332, -0.0804, -0.0061, 0.21, -0.387, 0.145, 0.0806, 0.296, -0.0117, -0.235, 0.132, 0.254, -0.247, 0.104, 0.114, 0.172, -0.00562, 0.205, 0.0635, -0.451, -0.227, -0.103, -0.131, 0.0376, 0.271, -0.239, 0.0381, -0.0391, -0.0942, 0.0083, 0.0703, 0.275, 0.332, -0.0107, 0.372, -0.125, 0.194, -0.136, -0.31, -0.236, -0.0127, -0.277, 0.158, 0.308, -0.233, 0.325, 0.0137, 0.199, -0.0262, -0.0808, -0.0751, -0.0411, 0.196, -0.0564, -0.279, -0.275, -0.404, -0.0176, -0.00586, -0.0771, 0.134, 0.237, 0.202], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "360e6b74-573b-479a-9f2a-f23dbdf01e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300517559051514),\n",
       " ('monarch', 0.645466148853302),\n",
       " ('princess', 0.6156251430511475),\n",
       " ('crown_prince', 0.5818676352500916),\n",
       " ('prince', 0.5777117609977722),\n",
       " ('kings', 0.5613663792610168),\n",
       " ('sultan', 0.5376775860786438),\n",
       " ('Queen_Consort', 0.5344247817993164),\n",
       " ('queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar([vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb929383-99ab-4830-abd8-fc0a617f42fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Spam Ham Projects Using Word2vec,AvgWord2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d77dec3c-ede3-47c0-afbe-31ad4f2c51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "messages=pd.read_csv('SMSSpamCollection.txt',\n",
    "                    sep='\\t',names=[\"label\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e5c6646-0dd3-48c8-b040-22ad18756238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a412e921-0def-4570-9c23-7b7ef33997f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f557fdb3-058b-4241-8f2d-3305f8503930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patralapati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df670c87-0de3-45e0-8509-2911f5afbc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3947c36-6fe4-44cc-9566-d1533d110ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, '', '645'], [0, '', ':) '], [0, '', ':-) :-)']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i,j,k] for i,j,k in zip(list(map(len,corpus)),corpus, messages['message']) if i<1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54fe0af8-193a-42e9-9569-afb65fb4d7d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry question std txt rate t c s apply over s',\n",
       " 'u dun say so early hor u c already then say',\n",
       " 'nah i don t think he go to usf he life around here though',\n",
       " 'freemsg hey there darling it s been week s now and no word back i d like some fun you up for it still tb ok xxx std chgs to send to rcv',\n",
       " 'even my brother is not like to speak with me they treat me like aid patent',\n",
       " 'a per your request melle melle oru minnaminunginte nurungu vettam ha been set a your callertune for all caller press to copy your friend callertune',\n",
       " 'winner a a valued network customer you have been selected to receivea prize reward to claim call claim code kl valid hour only',\n",
       " 'had your mobile month or more u r entitled to update to the latest colour mobile with camera for free call the mobile update co free on',\n",
       " 'i m gonna be home soon and i don t want to talk about this stuff anymore tonight k i ve cried enough today',\n",
       " 'six chance to win cash from to pound txt csh and send to cost p day day tsandcs apply reply hl info',\n",
       " 'urgent you have won a week free membership in our prize jackpot txt the word claim to no t c www dbuk net lccltd pobox ldnw a rw',\n",
       " 'i ve been searching for the right word to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all time',\n",
       " 'i have a date on sunday with will',\n",
       " 'xxxmobilemovieclub to use your credit click the wap link in the next txt message or click here http wap xxxmobilemovieclub com n qjkgighjjgcbl',\n",
       " 'oh k i m watching here',\n",
       " 'eh u remember how spell his name yes i did he v naughty make until i v wet',\n",
       " 'fine if that s the way u feel that s the way it gota b',\n",
       " 'england v macedonia dont miss the goal team news txt ur national team to eg england to try wale scotland txt poboxox w wq',\n",
       " 'is that seriously how you spell his name',\n",
       " 'i m going to try for month ha ha only joking',\n",
       " 'so pay first lar then when is da stock comin',\n",
       " 'aft i finish my lunch then i go str down lor ard smth lor u finish ur lunch already',\n",
       " 'ffffffffff alright no way i can meet up with you sooner',\n",
       " 'just forced myself to eat a slice i m really not hungry tho this suck mark is getting worried he know i m sick when i turn down pizza lol',\n",
       " 'lol your always so convincing',\n",
       " 'did you catch the bus are you frying an egg did you make a tea are you eating your mom s left over dinner do you feel my love',\n",
       " 'i m back amp we re packing the car now i ll let you know if there s room',\n",
       " 'ahhh work i vaguely remember that what doe it feel like lol',\n",
       " 'wait that s still not all that clear were you not sure about me being sarcastic or that that s why x doesn t want to live with u',\n",
       " 'yeah he got in at and wa v apologetic n had fallen out and she wa actin like spoilt child and he got caught up in that till but we won t go there not doing too badly cheer you',\n",
       " 'k tell me anything about you',\n",
       " 'for fear of fainting with the of all that housework you just did quick have a cuppa',\n",
       " 'thanks for your subscription to ringtone uk your mobile will be charged month please confirm by replying yes or no if you reply no you will not be charged',\n",
       " 'yup ok i go home look at the timing then i msg again xuhui going to learn on nd may too but her lesson is at am',\n",
       " 'oops i ll let you know when my roommate s done',\n",
       " 'i see the letter b on my car',\n",
       " 'anything lor u decide',\n",
       " 'hello how s you and how did saturday go i wa just texting to see if you d decided to do anything tomo not that i m trying to invite myself or anything',\n",
       " 'pls go ahead with watt i just wanted to be sure do have a great weekend abiola',\n",
       " 'did i forget to tell you i want you i need you i crave you but most of all i love you my sweet arabian steed mmmmmm yummy',\n",
       " 'rodger burn msg we tried to call you re your reply to our sm for a free nokia mobile free camcorder please call now for delivery tomorrow',\n",
       " 'who are you seeing',\n",
       " 'great i hope you like your man well endowed i am lt gt inch',\n",
       " 'no call message missed call',\n",
       " 'didn t you get hep b immunisation in nigeria',\n",
       " 'fair enough anything going on',\n",
       " 'yeah hopefully if tyler can t do it i could maybe ask around a bit',\n",
       " 'u don t know how stubborn i am i didn t even want to go to the hospital i kept telling mark i m not a weak sucker hospital are for weak sucker',\n",
       " 'what you thinked about me first time you saw me in class',\n",
       " 'a gram usually run like lt gt a half eighth is smarter though and get you almost a whole second gram for lt gt',\n",
       " 'k fyi x ha a ride early tomorrow morning but he s crashing at our place tonight',\n",
       " 'wow i never realized that you were so embarassed by your accomodations i thought you liked it since i wa doing the best i could and you always seemed so happy about the cave i m sorry i didn t and don t have more to give i m sorry i offered i m sorry your room wa so embarassing',\n",
       " 'sm ac sptv the new jersey devil and the detroit red wing play ice hockey correct or incorrect end reply end sptv',\n",
       " 'do you know what mallika sherawat did yesterday find out now lt url gt',\n",
       " 'congrats year special cinema pas for is yours call now c suprman v matrix starwars etc all free bx ip we pm dont miss out',\n",
       " 'sorry i ll call later in meeting',\n",
       " 'tell where you reached',\n",
       " 'yes gauti and sehwag out of odi series',\n",
       " 'your gonna have to pick up a burger for yourself on your way home i can t even move pain is killing me',\n",
       " 'ha ha ha good joke girl are situation seeker',\n",
       " 'it a part of checking iq',\n",
       " 'sorry my roommate took forever it ok if i come by now',\n",
       " 'ok lar i double check wif da hair dresser already he said wun cut v short he said will cut until i look nice',\n",
       " 'a a valued customer i am pleased to advise you that following recent review of your mob no you are awarded with a bonus prize call',\n",
       " 'today is song dedicated day which song will u dedicate for me send this to all ur valuable frnds but first rply me',\n",
       " 'urgent ur awarded a complimentary trip to eurodisinc trav aco entry or to claim txt dis to morefrmmob shracomorsglsuplt l aj',\n",
       " 'did you hear about the new divorce barbie it come with all of ken s stuff',\n",
       " 'i plane to give on this month end',\n",
       " 'wah lucky man then can save money hee',\n",
       " 'finished class where are you',\n",
       " 'hi babe im at home now wanna do something xx',\n",
       " 'k k where are you how did you performed',\n",
       " 'u can call me now',\n",
       " 'i am waiting machan call me once you free',\n",
       " 'thats cool i am a gentleman and will treat you with dignity and respect',\n",
       " 'i like you people very much but am very shy pa',\n",
       " 'doe not operate after lt gt or what',\n",
       " 'it not the same here still looking for a job how much do ta s earn there',\n",
       " 'sorry i ll call later',\n",
       " 'k did you call me just now ah',\n",
       " 'ok i am on the way to home hi hi',\n",
       " 'you will be in the place of that man',\n",
       " 'yup next stop',\n",
       " 'i call you later don t have network if urgnt sm me',\n",
       " 'for real when u getting on yo i only need more ticket and one more jacket and i m done i already used all my multis',\n",
       " 'yes i started to send request to make it but pain came back so i m back in bed double coin at the factory too i gotta cash in all my nitros',\n",
       " 'i m really not up to it still tonight babe',\n",
       " 'ela kano il download come wen ur free',\n",
       " 'yeah do don t stand to close tho you ll catch something',\n",
       " 'sorry to be a pain is it ok if we meet another night i spent late afternoon in casualty and that mean i haven t done any of y stuff moro and that includes all my time sheet and that sorry',\n",
       " 'smile in pleasure smile in pain smile when trouble pours like rain smile when sum hurt u smile becoz someone still love to see u smiling',\n",
       " 'please call our customer service representative on between am pm a you have won a guaranteed cash or prize',\n",
       " 'havent planning to buy later i check already lido only got show in e afternoon u finish work already',\n",
       " 'your free ringtone is waiting to be collected simply text the password mix to to verify get usher and britney fml po box mk h ppw',\n",
       " 'watching telugu movie wat abt u',\n",
       " 'i see when we finish we have load of loan to pay',\n",
       " 'hi wk been ok on hols now yes on for a bit of a run forgot that i have hairdresser appointment at four so need to get home n shower beforehand doe that cause prob for u',\n",
       " 'i see a cup of coffee animation',\n",
       " 'please don t text me anymore i have nothing else to say',\n",
       " 'okay name ur price a long a it legal wen can i pick them up y u ave x am xx',\n",
       " 'i m still looking for a car to buy and have not gone the driving test yet',\n",
       " 'a per your request melle melle oru minnaminunginte nurungu vettam ha been set a your callertune for all caller press to copy your friend callertune',\n",
       " 'wow you re right i didn t mean to do that i guess once i gave up on boston men and changed my search location to nyc something changed cuz on my signin page it still say boston',\n",
       " 'umma my life and vava umma love you lot dear',\n",
       " 'thanks a lot for your wish on my birthday thanks you for making my birthday truly memorable',\n",
       " 'aight i ll hit you up when i get some cash',\n",
       " 'how would my ip address test that considering my computer isn t a minecraft server',\n",
       " 'i know grumpy old people my mom wa like you better not be lying then again i am always the one to play joke',\n",
       " 'dont worry i guess he s busy',\n",
       " 'what is the plural of the noun research',\n",
       " 'going for dinner msg you after',\n",
       " 'i m ok wif it co i like try new thing but i scared u dun like mah co u said not too loud',\n",
       " 'gent we are trying to contact you last weekend draw show that you won a prize guaranteed call claim code k valid hr only ppm',\n",
       " 'wa ur openin sentence very formal anyway i m fine too juz tt i m eatin too much n puttin on weight haha so anythin special happened',\n",
       " 'a i entered my cabin my pa said happy b day bos i felt special she askd me lunch after lunch she invited me to her apartment we went there',\n",
       " 'you are a winner u have been specially selected receive or a holiday flight inc speak to a live operator claim p min',\n",
       " 'goodo yes we must speak friday egg potato ratio for tortilla needed',\n",
       " 'hmm my uncle just informed me that he s paying the school directly so pls buy food',\n",
       " 'private your account statement for show unredeemed bonus point to claim call identifier code expires',\n",
       " 'urgent your mobile no wa awarded bonus caller prize on this is our final try to contact u call from landline box wr c ppm',\n",
       " 'here is my new address apple pair all that malarky',\n",
       " 'today voda number ending are selected to receive a award if you have a match please call quoting claim code standard rate app',\n",
       " 'i am going to sao mu today will be done only at',\n",
       " 'predict wat time ll finish buying',\n",
       " 'good stuff will do',\n",
       " 'just so that you know yetunde hasn t sent money yet i just sent her a text not to bother sending so it over you dont have to involve yourself in anything i shouldn t have imposed anything on you in the first place so for that i apologise',\n",
       " 'are you there in room',\n",
       " 'hey girl how r u hope u r well me an del r bak again long time no c give me a call sum time from lucyxx',\n",
       " 'k k how much doe it cost',\n",
       " 'i m home',\n",
       " 'dear will call tmorrow pls accomodate',\n",
       " 'first answer my question',\n",
       " 'sunshine quiz wkly q win a top sony dvd player if u know which country the algarve is in txt ansr to sp tyrone',\n",
       " 'want get laid tonight want real dogging location sent direct ur mob join the uk s largest dogging network bt txting gravel to nt ec a p msg p',\n",
       " 'i only haf msn it s yijue hotmail com',\n",
       " 'he is there you call and meet him',\n",
       " 'no no i will check all room befor activity',\n",
       " 'you ll not rcv any more msg from the chat svc for free hardcore service text go to if u get nothing u must age verify with yr network try again',\n",
       " 'got c i lazy to type i forgot in lect i saw a pouch but like not v nice',\n",
       " 'k text me when you re on the way',\n",
       " 'sir waiting for your mail',\n",
       " 'a swt thought nver get tired of doing little thing lovable person coz somtimes those little thing occupy d biggest part in their heart gud ni',\n",
       " 'i know you are can you pls open the back',\n",
       " 'yes see ya not on the dot',\n",
       " 'whats the staff name who is taking class for u',\n",
       " 'freemsg why haven t you replied to my text i m randy sexy female and live local luv to hear from u netcollex ltd p per msg reply stop to end',\n",
       " 'ummma will call after check in our life will begin from qatar so pls pray very hard',\n",
       " 'k i deleted my contact that why',\n",
       " 'sindu got job in birla soft',\n",
       " 'the wine is flowing and i m i have nevering',\n",
       " 'yup i thk cine is better co no need go down plaza mah',\n",
       " 'ok ur typical reply',\n",
       " 'a per your request melle melle oru minnaminunginte nurungu vettam ha been set a your callertune for all caller press to copy your friend callertune',\n",
       " 'you are everywhere dirt on the floor the window even on my shirt and sometimes when i open my mouth you are all that come flowing out i dream of my world without you then half my chore are out too a time of joy for me lot of tv show i ll see but i guess like all thing you just must exist like rain hail and mist and when my time here is done you and i become one',\n",
       " 'aaooooright are you at work',\n",
       " 'i m leaving my house now',\n",
       " 'hello my love what are you doing did you get to that interview today are you you happy are you being a good boy do you think of me are you missing me',\n",
       " 'customer service annoncement you have a new year delivery waiting for you please call now to arrange delivery',\n",
       " 'you are a winner u have been specially selected receive cash or a holiday flight inc speak to a live operator claim',\n",
       " 'keep yourself safe for me because i need you and i miss you already and i envy everyone that see s you in real life',\n",
       " 'new car and house for my parent i have only new job in hand',\n",
       " 'i m so in love with you i m excited each day i spend with you you make me so happy',\n",
       " 'pls stop bootydelious f is inviting you to be her friend reply yes or no see her www sm ac u bootydelious stop send stop frnd to',\n",
       " 'bangbabes ur order is on the way u should receive a service msg download ur content if u do not goto wap bangb tv on ur mobile internet service menu',\n",
       " 'i place all ur point on e culture module already',\n",
       " 'urgent we are trying to contact you last weekend draw show that you have won a prize guaranteed call claim code s valid hr only',\n",
       " 'hi frnd which is best way to avoid missunderstding wit our beloved one s',\n",
       " 'great escape i fancy the bridge but need her lager see you tomo',\n",
       " 'yes it completely in out of form clark also utter waste',\n",
       " 'sir i need axis bank account no and bank address',\n",
       " 'hmmm thk sure got time to hop ard ya can go free abt muz call u to discus liao',\n",
       " 'what time you coming down later',\n",
       " 'bloody hell cant believe you forgot my surname mr ill give u a clue it spanish and begin with m',\n",
       " 'well i m gonna finish my bath now have a good fine night',\n",
       " 'let me know when you ve got the money so carlos can make the call',\n",
       " 'u still going to the mall',\n",
       " 'turn out my friend are staying for the whole show and won t be back til lt gt so feel free to go ahead and smoke that lt gt worth',\n",
       " 'text her if she doesnt reply let me know so i can have her log in',\n",
       " 'hi you just spoke to maneesha v we d like to know if you were satisfied with the experience reply toll free with yes or no',\n",
       " 'you lifted my hope with the offer of money i am in need especially when the end of the month approach and it hurt my studying anyways have a gr weekend',\n",
       " 'lol no u can trust me',\n",
       " 'ok i am a gentleman and will treat you with dignity and respect',\n",
       " 'he will you guy close',\n",
       " 'going on nothing great bye',\n",
       " 'hello handsome are you finding that job not being lazy working towards getting back that net for mummy where s my boytoy now doe he miss me',\n",
       " 'haha awesome be there in a minute',\n",
       " 'please call our customer service representative on freephone between am pm a you have won a guaranteed cash or prize',\n",
       " 'have you got xmas radio time if not i will get it now',\n",
       " 'i jus reached home i go bathe first but my si using net tell u when she finish k',\n",
       " 'are you unique enough find out from th august www areyouunique co uk',\n",
       " 'i m sorry i ve joined the league of people that dont keep in touch you mean a great deal to me you have been a friend at all time even at great personal cost do have a great week',\n",
       " 'hi finally i completed the course',\n",
       " 'it will stop on itself i however suggest she stay with someone that will be able to give or for every stool',\n",
       " 'how are you doing hope you ve settled in for the new school year just wishin you a gr day',\n",
       " 'gud mrng dear hav a nice day',\n",
       " 'did u got that person story',\n",
       " 'is your hamster dead hey so tmr i meet you at pm orchard mrt',\n",
       " 'hi it kate how is your evening i hope i can see you tomorrow for a bit but i have to bloody babyjontet txt back if u can xxx',\n",
       " 'found it enc lt gt where you at',\n",
       " 'i sent you lt gt buck',\n",
       " 'hello darlin ive finished college now so txt me when u finish if u can love kate xxx',\n",
       " 'your account ha been refilled successfully by inr lt decimal gt your keralacircle prepaid account balance is r lt decimal gt your transaction id is kr lt gt',\n",
       " 'goodmorning sleeping ga',\n",
       " 'u call me alter at ok',\n",
       " 'say until like dat i dun buy ericsson oso cannot oredi lar',\n",
       " 'a i entered my cabin my pa said happy b day bos i felt special she askd me lunch after lunch she invited me to her apartment we went there',\n",
       " 'aight yo dat straight dogg',\n",
       " 'you please give u connection today itself before lt decimal gt or refund the bill',\n",
       " 'both i shoot big load so get ready',\n",
       " 'what s up bruv hope you had a great break do have a rewarding semester',\n",
       " 'home so we can always chat',\n",
       " 'k k good study well',\n",
       " 'yup how noe leh',\n",
       " 'sound great are you home now',\n",
       " 'finally the match heading towards draw a your prediction',\n",
       " 'tired i haven t slept well the past few night',\n",
       " 'easy ah sen got selected mean it good',\n",
       " 'i have to take exam with march',\n",
       " 'yeah you should i think you can use your gt atm now to register not sure but if there s anyway i can help let me know but when you do be sure you are ready',\n",
       " 'ok no prob take ur time',\n",
       " 'there is o called ubandu which will run without installing in hard disk you can use that o to copy the important file in system and give it to repair shop',\n",
       " 'sorry i ll call later',\n",
       " 'u say leh of course nothing happen lar not say v romantic jus a bit only lor i thk e nite scenery not so nice leh',\n",
       " 'new mobile from must go txt nokia to no collect yours today from only www tc biz optout gbp mtmsg',\n",
       " 'would really appreciate if you call me just need someone to talk to',\n",
       " 'will u meet ur dream partner soon is ur career off a flyng start find out free txt horo followed by ur star sign e g horo aries',\n",
       " 'hey company elama po mudyadhu',\n",
       " 'life is more strict than teacher bcoz teacher teach lesson amp then conduct exam but life first conduct exam amp then teach lesson happy morning',\n",
       " 'dear good morning now only i am up',\n",
       " 'get down in gandhipuram and walk to cross cut road right side lt gt street road and turn at first right',\n",
       " 'dear we are going to our rubber place',\n",
       " 'sorry battery died yeah i m here',\n",
       " 'yes here tv is always available in work place',\n",
       " 'text meet someone sexy today u can find a date or even flirt it up to u join just p reply with name age eg sam msg recd thirtyeight penny',\n",
       " 'i have printed it oh so lt gt come upstairs',\n",
       " 'or ill be a little closer like at the bus stop on the same street',\n",
       " 'where are you when wil you reach here',\n",
       " 'new theory argument win d situation but loses the person so dont argue with ur friend just kick them amp say i m always correct',\n",
       " 'u have a secret admirer who is looking make contact with u find out who they r reveal who think ur so special call on',\n",
       " 'tomarrow final hearing on my laptop case so i cant',\n",
       " 'pleassssssseeeeee tel me v avent done sportsx',\n",
       " 'okay no no just shining on that wa meant to be signing but that sound better',\n",
       " 'although i told u dat i m into baig face watch now but i really like e watch u gave co it s fr u thanx everything dat u ve done today i m touched',\n",
       " 'u don t remember that old commercial',\n",
       " 'too late i said i have the website i didn t i have or dont have the slipper',\n",
       " 'i asked you to call him now ok',\n",
       " 'kallis wont bat in nd inning',\n",
       " 'it didnt work again oh ok goodnight then i ll fix and have it ready by the time you wake up you are very dearly missed have a good night sleep',\n",
       " 'congratulation ur awarded of cd voucher or gift guaranteed free entry wkly draw txt music to tncs www ldew com win ppmx age',\n",
       " 'ranjith cal drpd deeraj and deepak min hold',\n",
       " 'wen ur lovable bcums angry wid u dnt take it seriously coz being angry is d most childish n true way of showing deep affection care n luv kettoda manda have nice day da',\n",
       " 'what you doing how are you',\n",
       " 'ups which is day also and the shipping company that take wks the other way is usps which take a week but when it get to lag you may have to bribe nipost to get your stuff',\n",
       " 'i m back lemme know when you re ready',\n",
       " 'don t necessarily expect it to be done before you get back though because i m just now headin out',\n",
       " 'mmm so yummy babe nice jolt to the suzy',\n",
       " 'where are you lover i need you',\n",
       " 'we tried to contact you re your reply to our offer of a video handset anytime network min unlimited text camcorder reply or call now',\n",
       " 'i m parked next to a mini when are you coming in today do you think',\n",
       " 'yup',\n",
       " 'anyway i m going shopping on my own now co my si not done yet dun disturb u liao',\n",
       " 'my no in luton ring me if ur around h',\n",
       " 'hey i am really horny want to chat or see me naked text hot to text charged at pm to unsubscribe text stop',\n",
       " 'why you dint come with u',\n",
       " 'same wana plan a trip sometme then',\n",
       " 'not sure yet still trying to get a hold of him',\n",
       " 'ur ringtone service ha changed free credit go to club mobile com to choose content now stop txt club stop to p wk club po box mk wt',\n",
       " 'the evo i just had to download flash jealous',\n",
       " 'ringtone club get the uk single chart on your mobile each week and choose any top quality ringtone this message is free of charge',\n",
       " 'come to mu we re sorting out our narcotic situation',\n",
       " 'night ha ended for another day morning ha come in a special way may you smile like the sunny ray and leaf your worry at the blue blue bay',\n",
       " 'hmv bonus special pound of genuine hmv voucher to be won just answer easy question play now send hmv to more info www percent real com',\n",
       " 'usf i guess might a well take car',\n",
       " 'no objection my bf not coming',\n",
       " 'thanx',\n",
       " 'tell rob to mack his gf in the theater',\n",
       " 'awesome i ll see you in a bit',\n",
       " 'just sent it so what type of food do you like',\n",
       " 'all done all handed in celebration in full swing yet',\n",
       " 'you got called a tool',\n",
       " 'wen u miss someone the person is definitely special for u but if the person is so special why to miss them just keep in touch gdeve',\n",
       " 'ok i asked for money how far',\n",
       " 'okie',\n",
       " 'yeah i think my usual guy s still passed out from last night if you get ahold of anybody let me know and i ll throw down',\n",
       " 'k i might come by tonight then if my class let out early',\n",
       " 'ok',\n",
       " 'hi baby im cruisin with my girl friend what r u up give me a call in and hour at home if thats alright or fone me on this fone now love jenny xxx',\n",
       " 'my life mean a lot to me not because i love my life but because i love the people in my life the world call them friend i call them my world ge',\n",
       " 'dear shall mail tonite busy in the street shall update you tonite thing are looking ok varunnathu edukkukayee raksha ollu but a good one in real sense',\n",
       " 'hey you told your name to gautham ah',\n",
       " 'haf u found him i feel so stupid da v cam wa working',\n",
       " 'oops got that bit',\n",
       " 'are you this much buzy',\n",
       " 'i accidentally deleted the message resend please',\n",
       " 't mobile customer you may now claim your free camera phone upgrade a pay go sim card for your loyalty call on offer end thfeb t c s apply',\n",
       " 'unless it s a situation where you go gurl would be more appropriate',\n",
       " 'hurt me tease me make me cry but in the end of my life when i die plz keep one rose on my grave and say stupid i miss u have a nice day bslvyl',\n",
       " 'i cant pick the phone right now pls send a message',\n",
       " 'need a coffee run tomo can t believe it s that time of week already',\n",
       " 'awesome i remember the last time we got somebody high for the first time with diesel v',\n",
       " 'shit that is really shocking and scary cant imagine for a second def up for night out do u think there is somewhere i could crash for night save on taxi',\n",
       " 'oh and by the way you do have more food in your fridge want to go out for a meal tonight',\n",
       " 'he is a womdarfull actor',\n",
       " 'sm ac blind date u rodds is m from aberdeen united kingdom check him out http img sm ac w icmb cktz r no blind date send hide',\n",
       " 'yup from what i remb i think should be can book',\n",
       " 'jos ask if u wana meet up',\n",
       " 'lol yes our friendship is hanging on a thread cause u won t buy stuff',\n",
       " 'themob check out our newest selection of content game tone gossip babe and sport keep your mobile fit and funky text wap to',\n",
       " 'where are the garage key they aren t on the bookshelf',\n",
       " 'today is accept day u accept me a brother sister lover dear best clos lvblefrnd jstfrnd cutefrnd lifpartnr belovd swtheart bstfrnd no rply mean enemy',\n",
       " 'think ur smart win this week in our weekly quiz text play to now t c winnersclub po box m uz gbp week',\n",
       " 'he say he ll give me a call when his friend s got the money but that he s definitely buying before the end of the week',\n",
       " 'hi the way i wa with u day is the normal way this is the real me ur unique i hope i know u the rest of mylife hope u find wot wa lost',\n",
       " 'you made my day do have a great day too',\n",
       " 'k k advance happy pongal',\n",
       " 'hmmm guess we can go kb n power yoga haha dunno we can tahan power yoga anot thk got lo oso forgot liao',\n",
       " 'not really dude have no friend i m afraid',\n",
       " 'december only had your mobile mths you are entitled to update to the latest colour camera mobile for free call the mobile update co free on',\n",
       " 'coffee cake i guess',\n",
       " 'merry christmas to you too babe i love ya kiss',\n",
       " 'hey why dont we just go watch x men and have lunch haha',\n",
       " 'cud u tell ppl im gona b a bit l co bus hav gon past co they were full im still waitin pete x',\n",
       " 'that would be great we ll be at the guild could meet on bristol road or somewhere will get in touch over weekend our plan take flight have a good week',\n",
       " 'no problem how are you doing',\n",
       " 'no call message missed call',\n",
       " 'hi da how is the today class',\n",
       " 'i d say that s a good sign but well you know my track record at reading woman',\n",
       " 'cool text me when you re parked',\n",
       " 'i m reading the text i just sent you it meant to be a joke so read it in that light',\n",
       " 'k k apo k good movie',\n",
       " 'maybe i could get book out tomo then return it immediately or something',\n",
       " 'call germany for only penny per minute call from a fixed line via access number no prepayment direct access',\n",
       " 'any chance you might have had with me evaporated a soon a you violated my privacy by stealing my phone number from your employer s paperwork not cool at all please do not contact me again or i will report you to your supervisor',\n",
       " 'valentine day special win over in our quiz and take your partner on the trip of a lifetime send go to now p msg rcvd custcare',\n",
       " 'ta daaaaa i am home babe are you still up',\n",
       " 'cool so how come you havent been wined and dined before',\n",
       " 'just sleeping and surfing',\n",
       " 'sorry i ll call later',\n",
       " 'u calling me right call my hand phone',\n",
       " 'ok that s great thanx a lot',\n",
       " 'i take it the post ha come then you must have s of text now happy reading my one from wiv hello caroline at the end is my favourite bless him',\n",
       " 'where u been hiding stranger',\n",
       " 'am not interested to do like that',\n",
       " 'my sister cleared two round in birla soft yesterday',\n",
       " 'gudnite tc practice going on',\n",
       " 'dis is yijue i jus saw ur mail in case huiming havent sent u my num dis is my num',\n",
       " 'one small prestige problem now',\n",
       " 'fancy a shag i do interested sextextuk com txt xxuk suzy to txts cost per msg tncs on website x',\n",
       " 'just checking in on you really do miss seeing jeremiah do have a great month',\n",
       " 'nah can t help you there i ve never had an iphone',\n",
       " 'if you re not in my car in an hour and a half i m going apeshit',\n",
       " 'today is sorry day if ever i wa angry with you if ever i misbehaved or hurt you plz plz just slap urself bcoz it ur fault i m basically good',\n",
       " 'yo you guy ever figure out how much we need for alcohol jay and i are trying to figure out how much we can safely spend on weed',\n",
       " 'lt gt ish minute wa minute ago wtf',\n",
       " 'thank you for calling forgot to say happy onam to you sirji i am fine here and remembered you when i met an insurance person meet you in qatar insha allah rakhesh ex tata aig who joined tissco tayseer',\n",
       " 'congratulation ur awarded of cd voucher or gift guaranteed free entry wkly draw txt music to tncs www ldew com win ppmx age',\n",
       " 'ur cash balance is currently pound to maximize ur cash in now send cash to only p msg cc hg suite land row w j hl',\n",
       " 'i m an actor when i work i work in the evening and sleep late since i m unemployed at the moment i always sleep late when you re unemployed every day is saturday',\n",
       " 'hello just got here st andrew boy it a long way it cold i will keep you posted',\n",
       " 'ha ha cool cool chikku chikku db',\n",
       " 'oh ok no prob',\n",
       " 'check audrey s status right now',\n",
       " 'busy here trying to finish for new year i am looking forward to finally meeting you',\n",
       " 'good afternoon sunshine how dawn that day are we refreshed and happy to be alive do we breathe in the air and smile i think of you my love a always',\n",
       " 'well i know z will take care of me so no worry',\n",
       " 'update now xmas offer latest motorola sonyericsson nokia free bluetooth double min txt on orange call mobileupd on or call optout f q',\n",
       " 'here is your discount code rp to stop further message reply stop www regalportfolio co uk customer service',\n",
       " 'wat uniform in where get',\n",
       " 'cool text me when you re ready',\n",
       " 'hello my boytoy geeee i miss you already and i just woke up i wish you were here in bed with me cuddling me i love you',\n",
       " 'i will spoil you in bed a well',\n",
       " 'i m going for bath will msg you next lt gt min',\n",
       " 'i cant keep talking to people if am not sure i can pay them if they agree to price so pls tell me what you want to really buy and how much you are willing to pay',\n",
       " 'thanks for your ringtone order reference t you will be charged gbp per week you can unsubscribe at anytime by calling customer service on',\n",
       " 'can you say what happen',\n",
       " 'you could have seen me i did t recognise you face',\n",
       " 'well there s not a lot of thing happening in lindsay on new year sigh some bar in ptbo and the blue heron ha something going',\n",
       " 'keep my payasam there if rinu brings',\n",
       " 'i taught that ranjith sir called me so only i sm like that becaus he verifying about project prabu told today so only pa dont mistake me',\n",
       " 'i guess that s why you re worried you must know that there s a way the body repair itself and i m quite sure you shouldn t worry we ll take it slow first the test they will guide when your ovulation is then just relax nothing you ve said is a reason to worry but i ll keep on followin you up',\n",
       " 'yeah sure give me a couple minute to track down my wallet',\n",
       " 'hey leave it not a big deal take care',\n",
       " 'hey i will be late ah meet you at',\n",
       " 'double min and txts month free bluetooth on orange available on sony nokia motorola phone call mobileupd on or call optout n dx',\n",
       " 'it took mr owl lick',\n",
       " 'customer place i will call you',\n",
       " 'mm that time you dont like fun',\n",
       " 'mths half price orange line rental latest camera phone free had your phone mths call mobilesdirect free on to update now or stoptxt',\n",
       " 'yup having my lunch buffet now u eat already',\n",
       " 'huh so late fr dinner',\n",
       " 'hey so this sat are we going for the intro pilate only or the kickboxing too',\n",
       " 'morning only i can ok',\n",
       " 'yes i think so i am in office but my lap is in room i think thats on for the last few day i didnt shut that down',\n",
       " 'pick you up bout ish what time are and that going',\n",
       " 'from here after the performance award is calculated every two month not for current one month period',\n",
       " 'wa actually sleeping and still might when u call back so a text is gr you rock si will send u a text wen i wake',\n",
       " 'you are always putting your business out there you put picture of your as on facebook you are one of the most open people i ve ever met why would i think a picture of your room would hurt you make you feel violated',\n",
       " 'good evening sir al salam wahleykkum sharing a happy news by the grace of god i got an offer from tayseer tissco and i joined hope you are fine inshah allah meet you sometime rakhesh visitor from india',\n",
       " 'hmmm k but i want to change the field quickly da i wanna get system administrator or network administrator',\n",
       " 'free ringtone text first to for a poly or text get to for a true tone help after st free tone are x pw to e nd txt stop',\n",
       " 'dear how is chechi did you talk to her',\n",
       " 'the hair cream ha not been shipped',\n",
       " 'none of that s happening til you get here though',\n",
       " 'yep the great loxahatchee xmas tree burning of lt gt start in an hour',\n",
       " 'haha get used to driving to usf man i know a lot of stoner',\n",
       " 'all wa well until slightly disastrous class this pm with my fav darling hope day off ok coffee wld be good a can t stay late tomorrow same time place a always',\n",
       " 'hello good week fancy a drink or something later',\n",
       " 'headin towards busetop',\n",
       " 'message some text missing sender name missing number missing sent date missing missing u a lot thats y everything is missing sent via fullonsms com',\n",
       " 'come by our room at some point so we can iron out the plan for this weekend',\n",
       " 'co i want it to be your thing',\n",
       " 'okies i ll go yan jiu too we can skip ard oso go cine den go mrt one blah blah blah',\n",
       " 'bring home some wendy d',\n",
       " 'dating service cal l box sk ch',\n",
       " 'whatsup there dont u want to sleep',\n",
       " 'alright i have a new goal now',\n",
       " 'free entry into our weekly competition just text the word win to now t c www txttowin co uk',\n",
       " 'alright i ll head out in a few minute text me where to meet you',\n",
       " 'send a logo ur lover name joined by a heart txt love name name mobno eg love adam eve to yahoo pobox w wq txtno no ad p',\n",
       " 'yes from last week itself i m taking live call',\n",
       " 'someone ha contacted our dating service and entered your phone because they fancy you to find out who it is call from a landline pobox n tf p',\n",
       " 'siva is in hostel aha',\n",
       " 'urgent your mobile number ha been awarded with a prize guaranteed call from land line claim valid hr only',\n",
       " 'send this to ur friend and receive something about ur voice how is my speaking expression childish naughty sentiment rowdy ful of attitude romantic shy attractive funny lt gt irritating lt gt lovable reply me',\n",
       " 'ok she ll be ok i guess',\n",
       " 'aathi where are you dear',\n",
       " 'any pain on urination any thing else',\n",
       " 'at esplanade do mind giving me a lift co i got no car today',\n",
       " 'i wnt to buy a bmw car urgently it vry urgent but hv a shortage of lt gt lac there is no source to arng dis amt lt gt lac thats my prob',\n",
       " 'at home watching tv lor',\n",
       " 'doe she usually take fifteen fucking minute to respond to a yes or no question',\n",
       " 'congrats nokia video camera phone is your call call cost ppm ave call min vary from mobile close post bcm ldn wc n xx',\n",
       " 'booked ticket for pongal',\n",
       " 'you available now i m like right around hillsborough amp lt gt th',\n",
       " 'the message sent is askin for lt gt dollar shoul i pay lt gt or lt gt',\n",
       " 'ask g or iouri i ve told the story like ten time already',\n",
       " 'how long doe applebees fucking take',\n",
       " 'hi hope u get this txt journey hasnt been gd now about min late i think',\n",
       " 'but i have to i like to have love and arrange',\n",
       " 'yes he is really great bhaji told kallis best cricketer after sachin in world very tough to get out',\n",
       " 'you were supposed to wake me up gt',\n",
       " 'oic i saw him too but i tot he din c me i found a group liao',\n",
       " 'sorry i ll call later',\n",
       " 'hey hey werethe monkeespeople say we monkeyaround howdy gorgeous howu doin foundurself a jobyet sausage love jen xxx',\n",
       " 'sorry my battery died i can come by but i m only getting a gram for now where s your place',\n",
       " 'well done blimey exercise yeah i kinda remember wot that is hmm',\n",
       " 'i wont get concentration dear you know you are my mind and everything',\n",
       " 'lol have you made plan for new year',\n",
       " 'min later k',\n",
       " 'hank lotsly',\n",
       " 'thanks for this hope you had a good day today',\n",
       " 'k k what are detail you want to transfer acc no enough',\n",
       " 'ok i will tell her to stay out yeah it been tough but we are optimistic thing will improve this month',\n",
       " 'loan for any purpose homeowner tenant welcome have you been previously refused we can still help call free or text back help',\n",
       " 'si si i think ill go make those oreo truffle',\n",
       " 'look at amy ure a beautiful intelligent woman and i like u a lot i know u don t like me like that so don t worry',\n",
       " 'i hope you that s the result of being consistently intelligent and kind start asking him about practicum link and keep your ear open and all the best ttyl',\n",
       " 'that call cost which i guess isnt bad miss ya need ya want ya love ya',\n",
       " 'going thru a very different feeling wavering decision and coping up with the same is the same individual time will heal everything i believe',\n",
       " 'where did u go my phone is gonna die you have to stay in here',\n",
       " 'great never been better each day give even more reason to thank god',\n",
       " 'upgrdcentre orange customer you may now claim your free camera phone upgrade for your loyalty call now on offer end th july t c s apply opt out available',\n",
       " 'sorry i ll call later ok bye',\n",
       " 'ok i am on the way to railway',\n",
       " 'great princess i love giving and receiving oral doggy style is my fave position how about you i enjoy making love lt gt time per night',\n",
       " 'they don t put that stuff on the road to keep it from getting slippery over there',\n",
       " 'when are you going to ride your bike',\n",
       " 'yup no need i ll jus wait e rain stop',\n",
       " 'there are many company tell me the language',\n",
       " 'okmail dear dave this is your final notice to collect your tenerife holiday or cash award call from landline tc sae box cw wx ppm',\n",
       " 'how long ha it been since you screamed princess',\n",
       " 'nothing i meant that once the money enters your account here the bank will remove it flat rate someone transfered lt gt to my account and lt gt dollar got removed so the bank differ and charge also differ be sure you trust the ja person you are sending account detail to co',\n",
       " 'want get laid tonight want real dogging location sent direct ur mob join the uk s largest dogging network by txting moan to nyt ec a p msg p',\n",
       " 'nice line said by a broken heart plz don t cum more time infront of me other wise once again i ll trust u good t',\n",
       " 'ok i m gonna head up to usf in like fifteen minute',\n",
       " 'love you aathi love u lot',\n",
       " 'tension ah what machi any problem',\n",
       " 'k can i pick up another th when you re done',\n",
       " 'when re you guy getting back g said you were thinking about not staying for mcr',\n",
       " 'almost there see u in a sec',\n",
       " 'yo carlos a few friend are already asking me about you you working at all this weekend',\n",
       " 'watching tv lor',\n",
       " 'thank you baby i cant wait to taste the real thing',\n",
       " 'you should change your fb to jaykwon thuglyfe falconerf',\n",
       " 'if we win it really no side for long time',\n",
       " 'free message activate your free text message by replying to this message with the word free for term condition visit www com',\n",
       " 'dear reached railway what happen to you',\n",
       " 'depends on quality if you want the type i sent boye faded glory then about if you want ralphs maybe',\n",
       " 'i think i ve fixed it can you send a test message',\n",
       " 'sorry man my account s dry or i would if you want we could trade back half or i could buy some shit with my credit card',\n",
       " 'congrats year special cinema pas for is yours call now c suprman v matrix starwars etc all free bx ip we pm dont miss out',\n",
       " 'sorry in meeting i ll call later',\n",
       " 'what class of lt gt reunion',\n",
       " 'are you free now can i call now',\n",
       " 'got meh when',\n",
       " 'nope think i will go for it on monday sorry i replied so late',\n",
       " 'some of them told accenture is not confirm is it true',\n",
       " 'kate jackson rec center before ish right',\n",
       " 'dear i have reache room',\n",
       " 'fighting with the world is easy u either win or lose bt fightng with some who is close to u is dificult if u lose u lose if u win u still lose',\n",
       " 'when can come out',\n",
       " 'check with nuerologist',\n",
       " 'lolnice i went from a fish to water',\n",
       " 'congratulation in this week s competition draw u have won the prize to claim just call b t c stop sm over only ppm',\n",
       " 'no it s waiting in e car dat s bored wat co wait outside got nothing do at home can do my stuff or watch tv wat',\n",
       " 'maybe westshore or hyde park village the place near my house',\n",
       " 'you should know now so how s anthony are you bringing money i ve school fee to pay and rent and stuff like that thats why i need your help a friend in need',\n",
       " 'what s the significance',\n",
       " 'your opinion about me over jada kusruthi lovable silent spl character not matured stylish simple pls reply',\n",
       " 'at the latest g s still there if you can scrounge up some ammo and want to give the new ak a try',\n",
       " 'prabha i m soryda realy frm heart i m sory',\n",
       " 'lol ok your forgiven',\n",
       " 'no jst change tat only',\n",
       " 'you are guaranteed the latest nokia phone a gb ipod mp player or a prize txt word collect to no ibhltd ldnw h p mtmsgrcvd',\n",
       " 's no competition for him',\n",
       " 'boltblue tone for p reply poly or mono eg poly cha cha slide yeah slow jamz toxic come with me or stop more tone txt more',\n",
       " 'your credit have been topped up for http www bubbletext com your renewal pin is tgxxrz',\n",
       " 'that way transport is less problematic than on sat night by the way if u want to ask n to join my bday feel free but need to know definite no a booking on fri',\n",
       " 'usually the person is unconscious that s in child but in adult they may just behave abnormally i ll call you now',\n",
       " 'but that s on ebay it might be less elsewhere',\n",
       " 'shall i come to get pickle',\n",
       " 'were gonna go get some taco',\n",
       " 'that s very rude you on campus',\n",
       " 'urgent your mobile no wa awarded a bonus caller prize on this is our nd attempt to contact you call box qu',\n",
       " 'hi i won t b ard christmas but do enjoy n merry x ma',\n",
       " 'today s offer claim ur worth of discount voucher text yes to now savamob member offer mobile t c sub unsub reply x',\n",
       " 'yes how is a pretty lady like you single',\n",
       " 'you will recieve your tone within the next hr for term and condition please see channel u teletext pg',\n",
       " 'jay say that you re a double faggot',\n",
       " 'private your account statement for show un redeemed s i m point call identifier code expires',\n",
       " 'what today sunday sunday is holiday so no work',\n",
       " 'gudnite tc practice going on',\n",
       " 'i ll be late',\n",
       " 'i ve not called you in a while this is hoping it wa l r malaria and that you know that we miss you guy i miss ban big so pls give her my love especially have a great day',\n",
       " 'good afternoon my love how go that day i hope maybe you got some lead on a job i think of you boytoy and send you a passionate kiss from across the sea',\n",
       " 'probably gonna be here for a while see you later tonight lt',\n",
       " 'or maybe my fat finger just press all these button and it doesn t know what to do',\n",
       " 'ummmmmaah many many happy return of d day my dear sweet heart happy birthday dear',\n",
       " 'i am in tirupur da once you started from office call me',\n",
       " 'from www applausestore com monthlysubscription p msg max month t csc web age stop txt stop',\n",
       " 'a famous quote when you develop the ability to listen to anything unconditionally without losing your temper or self confidence it mean you are married',\n",
       " 'but am going to college pa what to do are else ill come there it self pa',\n",
       " 'oclock at mine just to bash out a flat plan',\n",
       " 'this girl doe not stay in bed this girl doesn t need recovery time id rather pas out while having fun then be cooped up in bed',\n",
       " 'then any special there',\n",
       " 'i know but you need to get hotel now i just got my invitation but i had to apologise cali is to sweet for me to come to some english bloke s weddin',\n",
       " 'sorry that took so long omw now',\n",
       " 'wait lt gt min',\n",
       " 'ok give me minute i think i see her btw you re my alibi you were cutting my hair the whole time',\n",
       " 'imagine you finally get to sink into that bath after i have put you through your pace maybe even having you eat me for a while before i left but also imagine the feel of that cage on your cock surrounded by the bath water reminding you always who owns you enjoy my cuck',\n",
       " 'hurry up i ve been weed deficient for like three day',\n",
       " 'sure if i get an acknowledgement from you that it s astoundingly tactless and generally faggy to demand a blood oath fo',\n",
       " 'ok every night take a warm bath drink a cup of milk and you ll see a work of magic you still need to loose weight just so that you know',\n",
       " 'i ll have a look at the frying pan in case it s cheap or a book perhaps no that s silly a frying pan isn t likely to be a book',\n",
       " 'o well uv cause mutation sunscreen is like essential thesedays',\n",
       " 'having lunch you are not in online why',\n",
       " 'i know that my friend already told that',\n",
       " 'hi princess thank you for the pic you are very pretty how are you',\n",
       " 'aiyo u always c our ex one i dunno abt mei she haven reply first time u reply so fast y so lucky not workin huh got bao by ur sugardad ah gee',\n",
       " 'hi msg me i m in office',\n",
       " 'thanx e brownie it s v nice',\n",
       " 'geeeee i love you so much i can barely stand it',\n",
       " 'gent we are trying to contact you last weekend draw show that you won a prize guaranteed call claim code k valid hr only ppm',\n",
       " 'fuck babe i miss you already you know can t you let me send you some money towards your net i need you i want you i crave you',\n",
       " 'ill call u mrw at ninish with my address that icky american freek wont stop callin me bad jen k eh',\n",
       " 'oooh bed ridden ey what are you thinking of',\n",
       " 'so anyways you can just go to your gym or whatever my love smile i hope your ok and having a good day babe i miss you so much already',\n",
       " 'love it daddy will make you scream with pleasure i am going to slap your as with my dick',\n",
       " 'wot u wanna do then missy',\n",
       " 'yar lor wait my mum finish sch then have lunch lor i whole morning stay at home clean my room now my room quite clean hee',\n",
       " 'do you know where my lab goggles went',\n",
       " 'can you open the door',\n",
       " 'waiting for your call',\n",
       " 'nope i waiting in sch daddy',\n",
       " 'you have won cash or a prize to claim call',\n",
       " 'i m tired of arguing with you about this week after week do what you want and from now on i ll do the same',\n",
       " 'wait me in sch i finish ard',\n",
       " 'our mobile number ha won to claim call u back or ring the claim hot line on',\n",
       " 'arngd marriage is while u r walkin unfortuntly a snake bite u bt love marriage is dancing in frnt of d snake amp sayin bite me bite me',\n",
       " 'huh so early then having dinner outside izzit',\n",
       " 'ok anyway no need to change with what you said',\n",
       " 'we tried to contact you re your reply to our offer of min textand a new video phone call now or reply for free delivery tomorrow',\n",
       " 'my ex wife wa not able to have kid do you want kid one day',\n",
       " 'so how s scotland hope you are not over showing your jjc tendency take care live the dream',\n",
       " 'tell them u have a headache and just want to use hour of sick time',\n",
       " 'i dun thk i ll quit yet hmmm can go jazz yogasana oso can we can go meet em after our lesson den',\n",
       " 'pete can you please ring meive hardly gotany credit',\n",
       " 'ya srsly better than yi tho',\n",
       " 'i m in a meeting call me later at',\n",
       " 'for ur chance to win a wkly shopping spree txt shop to t s c s www txt shop com custcare x p wk',\n",
       " 'you have been specially selected to receive a pound award call before the line close cost ppm t c apply ag promo',\n",
       " 'private your account statement for show un redeemed s i m point call identifier code expires',\n",
       " 'you still at grand prix',\n",
       " 'i met you a a stranger and choose you a my friend a long a the world stand our friendship never end let be friend forever gud nitz',\n",
       " 'i am great how are you',\n",
       " 'gud mrng dear have a nice day',\n",
       " 'you have an important customer service announcement call freephone now',\n",
       " 'will do wa exhausted on train this morning too much wine and pie you sleep well too',\n",
       " 'i m going out to buy mum s present ar',\n",
       " 'mind blastin no more tsunami will occur from now on rajnikant stopped swimming in indian ocean d',\n",
       " 'if u sending her home first it s ok lor i m not ready yet',\n",
       " 'speaking of doe he have any cash yet',\n",
       " 'be happy there i will come after noon',\n",
       " 'meet after lunch la',\n",
       " 'take care n get well soon',\n",
       " 'xclusive clubsaisai morow soiree speciale zouk with nichols from paris free rose all lady info',\n",
       " 'what i meant to say is cant wait to see u again getting bored of this bridgwater banter',\n",
       " 'neva mind it s ok',\n",
       " 'it s fine imma get a drink or somethin want me to come find you',\n",
       " 'day to kick off for euro u will be kept up to date with the latest news and result daily to be removed send get txt stop to',\n",
       " 'it a valentine game send dis msg to all ur friend if answer r d same then someone really love u ques which colour suit me the best rply me',\n",
       " 'i have many dependent',\n",
       " 'thanx today cer it wa nice catch up but we ave find more time more often oh well take care c u soon c',\n",
       " 'i called and said all to him then he have to choose this future',\n",
       " 'happy valentine day i know it early but i have hundred of handsomes and beauty to wish so i thought to finish off aunty and uncle st',\n",
       " 'he like not v shock leh co telling shuhui is like telling leona also like dat almost all know liao he got ask me abt ur reaction lor',\n",
       " 'for my family happiness',\n",
       " 'i come n pick up come out immediately aft ur lesson',\n",
       " 'let there be snow let there be snow this kind of weather brings ppl together so friendship can grow',\n",
       " 'dear we got lt gt dollar hi hi',\n",
       " 'good word but word may leave u in dismay many time',\n",
       " 'make sure alex know his birthday is over in fifteen minute a far a you re concerned',\n",
       " 'sorry no have got few thing to do may be in pub later',\n",
       " 'nah it s straight if you can just bring bud or drink or something that s actually a little more useful than straight cash',\n",
       " 'haha good to hear i m officially paid and on the market for an th',\n",
       " 'how many lick doe it take to get to the center of a tootsie pop',\n",
       " 'yup i thk they r e teacher said that will make my face look longer darren ask me not cut too short',\n",
       " 'new textbuddy chat horny guy in ur area just p free receive search postcode or at gaytextbuddy com txt one name to',\n",
       " 'today vodafone number ending with are selected to a receive a award if your number match call to receive your award',\n",
       " 'please dont say like that hi hi hi',\n",
       " 'thank u',\n",
       " 'oh that wa a forwarded message i thought you send that to me',\n",
       " 'got it seventeen pound for seven hundred ml hope ok',\n",
       " 'dear voucher holder claim this week offer at your pc go to http www e tlp co uk expressoffer t c apply stop text txt stop to',\n",
       " 'me n him so funny',\n",
       " 'sweetheart hope you are not having that kind of day have one with load of reason to smile biola',\n",
       " 'when login dat time dad fetching home now',\n",
       " 'what will we do in the shower baby',\n",
       " 'i had askd u a question some hour before it answer',\n",
       " 'well imma definitely need to restock before thanksgiving i ll let you know when i m out',\n",
       " 'said kiss kiss i can t do the sound effect he is a gorgeous man isn t he kind of person who need a smile to brighten his day',\n",
       " 'probably gonna swing by in a wee bit',\n",
       " 'ya very nice be ready on thursday',\n",
       " 'allo we have braved the bus and taken on the train and triumphed i mean we re in b ham have a jolly good rest of week',\n",
       " 'watching cartoon listening music amp at eve had to go temple amp church what about u',\n",
       " 'do you mind if i ask what happened you dont have to say if it is uncomfortable',\n",
       " 'private your account statement for show un redeemed s i m point call identifier code expires',\n",
       " 'no prob i will send to your email',\n",
       " 'you have won cash or a prize to claim call t c rstm sw s ppm',\n",
       " 'thats cool sometimes slow and gentle sonetimes rough and hard',\n",
       " 'i m gonna say no sorry i would but a normal am starting to panic about time sorry again are you seeing on tuesday',\n",
       " 'wait do you know if wesley in town i bet she doe hella drug',\n",
       " 'fine i miss you very much',\n",
       " 'did u got that person story',\n",
       " 'tell them the drug dealer s getting impatient',\n",
       " 'sun cant come to earth but send luv a ray cloud cant come to river but send luv a rain i cant come to meet u but can send my care a msg to u gud evng',\n",
       " 'you will be in the place of that man',\n",
       " 'it doesnt make sense to take it there unless it free if you need to know more wikipedia com',\n",
       " 'and are premium phone service call',\n",
       " 'under the sea there lay a rock in the rock there is an envelope in the envelope there is a paper on the paper there are word',\n",
       " 'then mum s repent how',\n",
       " 'sorry me going home first daddy come fetch later',\n",
       " 'leave it de start prepare for next',\n",
       " 'yes baby we can study all the position of the kama sutra',\n",
       " 'en chikku nange bakra msg kalstiya then had tea coffee',\n",
       " 'carlos ll be here in a minute if you still need to buy',\n",
       " 'this pay is lt decimal gt lakh',\n",
       " 'have a good evening ttyl',\n",
       " 'did u receive my msg',\n",
       " 'ho ho big belly laugh see ya tomo',\n",
       " 'sm ac sun post hello you seem cool wanted to say hi hi stop send stop to',\n",
       " 'get ur st ringtone free now reply to this msg with tone gr top tone to your phone every week just per wk opt out send stop',\n",
       " 'ditto and you won t have to worry about me saying anything to you anymore like i said last night you do whatever you want and i ll do the same peace',\n",
       " 'i ve got lt gt any way i could pick up',\n",
       " 'i dont knw pa i just drink milk',\n",
       " 'maybe say hi to and find out if got his card great escape or wetherspoons',\n",
       " 'piggy r u awake i bet u re still sleeping i m going lunch now',\n",
       " 'cause i m not freaky lol',\n",
       " 'missed your call cause i wa yelling at scrappy miss u can t wait for u to come home i m so lonely today',\n",
       " 'what is this hex place you talk of explain',\n",
       " 'log off wat it s sdryb i',\n",
       " 'is xy going e lunch',\n",
       " 'hi i m sue i am year old and work a a lapdancer i love sex text me live i m i my bedroom now text sue to by textoperator g da ppmsg',\n",
       " 'i wanted to ask to wait me to finish lect co my lect finish in an hour anyway',\n",
       " 'have you finished work yet',\n",
       " 'every king wa once a cry baby and every great building wa once a map not imprtant where u r today but where u wil reach tomorw gud ni',\n",
       " 'dear me at cherthala in case u r coming cochin pls call bfore u start i shall also reach accordingly or tell me which day u r coming tmorow i am engaged an it holiday',\n",
       " 'thanks love but am i doing torch or bold',\n",
       " 'forwarded from please call immediately a there is an urgent message waiting for you',\n",
       " 'wa the farm open',\n",
       " 'sorry to trouble u again can buy d for my dad again all big small sat n sun thanx',\n",
       " 'my sister in law hope you are having a great month just saying hey abiola',\n",
       " 'will purchase d stuff today and mail to you do you have a po box number',\n",
       " 'ah poop look like ill prob have to send in my laptop to get fixed cuz it ha a gpu problem',\n",
       " 'good good job i like entrepreneur',\n",
       " 'aight you close by or still down around alex s place',\n",
       " 'meet you in corporation st outside gap you can see how my mind is working',\n",
       " 'mum ask to buy food home',\n",
       " 'k u also dont msg or reply to his msg',\n",
       " 'how much r willing to pay',\n",
       " 'sorry i ll call later',\n",
       " 'what is important is that you prevent dehydration by giving her enough fluid',\n",
       " 'thats a bit weird even where is the do supposed to be happening but good idea sure they will be in pub',\n",
       " 'true dear i sat to pray evening and felt so so i sm d you in some time',\n",
       " 'i don t think i can get away for a trek that long with family in town sorry',\n",
       " 'so when do you wanna gym harri',\n",
       " 'quite late lar ard anyway i wun b drivin',\n",
       " 'to review and keep the fantastic nokia n gage game deck with club nokia go www cnupdates com newsletter unsubscribe from alert reply with the word out',\n",
       " 'mths half price orange line rental latest camera phone free had your phone mths call mobilesdirect free on to update now or stoptxt t c',\n",
       " 'height of confidence all the aeronautics professor wer calld amp they wer askd sit in an aeroplane aftr they sat they wer told dat the plane w made by their student dey all hurried out of d plane bt only didnt move he said if it is made by my student this wont even start datz confidence',\n",
       " 'it just seems like weird timing that the night that all you and g want is for me to come smoke is the same day a when a shitstorm is attributed to me always coming over and making everyone smoke',\n",
       " 'between am pm cost p',\n",
       " 'save yourself the stress if the person ha a dorm account just send your account detail and the money will be sent to you',\n",
       " 'he also know about lunch menu only da i know',\n",
       " 'when i have stuff to sell i ll tell you',\n",
       " 'urgent this is the nd attempt to contact u u have won call b t csbcm wc n xx callcost ppm mobilesvary max',\n",
       " 'book which lesson then you msg me i will call up after work or sth i m going to get spec my membership is px',\n",
       " 'you have won a guaranteed cash or a prize to claim yr prize call our customer service representative on between am pm',\n",
       " 'macha dont feel upset i can assume your mindset believe me one evening with me and i have some wonderful plan for both of u let life begin again call me anytime',\n",
       " 'oh is it send me the address',\n",
       " 's fine anytime all the best with it',\n",
       " 'that is wondar full flim',\n",
       " 'ya even those cooky have jelly on them',\n",
       " 'the world is running and i am still maybe all are feeling the same so be it or i have to admit i am mad then where is the correction or let me call this is life and keep running with the world may be u r also running let run',\n",
       " 'got it it look scrumptious daddy want to eat you all night long',\n",
       " 'of co can lar i m not so ba dao ok pm lor y u never ask where we go ah i said u would ask on fri but he said u will ask today',\n",
       " 'alright omw gotta change my order to a half th',\n",
       " 'exactly anyways how far is jide her to study or just visiting',\n",
       " 'dunno y u ask me',\n",
       " 'email alertfrom jeri stewartsize kbsubject low cost prescripiton drvgsto listen to email call',\n",
       " 'no he didn t spring is coming early yay',\n",
       " 'lol you won t feel bad when i use her money to take you out to a steak dinner d',\n",
       " 'even u dont get in trouble while convincing just tel him once or twice and just tel neglect his msg dont c and read it just dont reply',\n",
       " 'leaving to qatar tonite in search of an opportunity all went fast pls add me in ur prayer dear rakhesh',\n",
       " 'then why no one talking to me',\n",
       " 'thanks for looking out for me i really appreciate',\n",
       " 'hi customer loyalty offer the new nokia mobile from only at txtauction txt word start to no get yours now t ctxt tc p mtmsg',\n",
       " 'wish i were with you now',\n",
       " 'haha mayb u re rite u know me well da feeling of being liked by someone is gd lor u faster go find one then all gal in our group attached liao',\n",
       " 'yes i will be there glad you made it',\n",
       " 'do well all will for little time thing of good time ahead',\n",
       " 'just got up have to be out of the room very soon i hadn t put the clock back til at i shouted at everyone to get up and then realised it wa wahay another hour in bed',\n",
       " 'ok there may be a free gym about',\n",
       " 'men like shorter lady gaze up into his eye',\n",
       " 'dunno he jus say go lido same time',\n",
       " 'i promise to take good care of you princess i have to run now please send pic when you get a chance ttyl',\n",
       " 'u are subscribed to the best mobile content service in the uk for per day until you send stop to helpline',\n",
       " 'is there a reason we ve not spoken this year anyways have a great week and all the best in your exam',\n",
       " 'by monday next week give me the full gist',\n",
       " 'do you realize that in about year we ll have thousand of old lady running around with tattoo',\n",
       " 'you have an important customer service announcement from premier',\n",
       " 'dont gimme that lip caveboy',\n",
       " 'when did you get to the library',\n",
       " 'realy sorry i don t recognise this number and am now confused who r u please',\n",
       " 'so why didnt you holla',\n",
       " 'cant think of anyone with spare room off top of my head',\n",
       " 'faith make thing possible hope make thing work love make thing beautiful may you have all three this christmas merry christmas',\n",
       " 'u should have made an appointment',\n",
       " 'call me when you carlos is are here my phone s vibrate is acting up and i might not hear text',\n",
       " 'romantic paris night flight from book now next year call t c apply',\n",
       " 'we are at grandma oh dear u still ill i felt shit this morning but i think i am just hungover another night then we leave on sat',\n",
       " 'urgent ur guaranteed award is still unclaimed call now closingdate claimcode m m pmmorefrommobile bremoved mobypobox l yf',\n",
       " 'nothing but we jus tot u would ask co u ba gua but we went mt faber yest yest jus went out already mah so today not going out jus call lor',\n",
       " 'wishing you and your family merry x ma and happy new year in advance',\n",
       " 'ur awarded a city break and could win a summer shopping spree every wk txt store to skilgme tscs winawk age perwksub',\n",
       " 'i m nt goin got somethin on unless they meetin dinner lor haha i wonder who will go ti time',\n",
       " 'sorry i ll call later',\n",
       " 'i cant pick the phone right now pls send a message',\n",
       " 'lol i know they re so dramatic school already closed for tomorrow apparently we can t drive in the inch of snow were supposed to get',\n",
       " 'not getting anywhere with this damn job hunting over here',\n",
       " 'lol u drunkard just doing my hair at d moment yeah still up tonight wats the plan',\n",
       " 'idc get over here you are not weaseling your way out of this shit twice in a row',\n",
       " 'i wil be there with in lt gt minute got any space',\n",
       " 'just sleeping and surfing',\n",
       " 'thanks for picking up the trash',\n",
       " 'why don t you go tell your friend you re not sure you want to live with him because he smoke too much then spend hour begging him to come smoke',\n",
       " 'hi it kate it wa lovely to see you tonight and ill phone you tomorrow i got to sing and a guy gave me his card xxx',\n",
       " 'happy new year my dear brother i really do miss you just got your number and decided to send you this text wishing you only happiness abiola',\n",
       " 'that mean get the door',\n",
       " 'your opinion about me over jada kusruthi lovable silent spl character not matured stylish simple pls reply',\n",
       " 'hmmm i thought we said hour slave not you are late how should i punish you',\n",
       " 'beerage',\n",
       " 'you have an important customer service announcement from premier call freephone now',\n",
       " 'dont think so it turn off like randomlly within min of opening',\n",
       " 'she wa supposed to be but couldn t make it she s still in town though',\n",
       " 'it doe it on it own most of the time it fix my spelling but sometimes it get a completely diff word go figure',\n",
       " 'ever thought about living a good life with a perfect partner just txt back name and age to join the mobile community p sm',\n",
       " 'free top polyphonic tone call national rate get a toppoly tune sent every week just text subpoly to per pole unsub',\n",
       " 'gud mrng dear hav a nice day',\n",
       " 'this is hoping you enjoyed your game yesterday sorry i ve not been in touch but pls know that you are fondly bein thot off have a great week abiola',\n",
       " 'all e best ur driving tmr',\n",
       " 'y where u at dogbreath it just sounding like jan c that s al',\n",
       " 'omg i want to scream i weighed myself and i lost more weight woohoo',\n",
       " 'there generally isn t one it s an uncountable noun u in the dictionary piece of research',\n",
       " 'it s really getting me down just hanging around',\n",
       " 'orange customer you may now claim your free camera phone upgrade for your loyalty call now on offer end thmarch t c s apply opt out availa',\n",
       " 'petey boy whereare you me and all your friendsare in thekingshead come down if you canlove nic',\n",
       " 'ok i msg u b i leave my house',\n",
       " 'gimme a few wa lt gt minute ago',\n",
       " 'last chance claim ur worth of discount voucher today text shop to now savamob offer mobile t c savamob pobox m uz sub',\n",
       " 'appt is at lt time gt am not my fault u don t listen i told u twice',\n",
       " 'free for st week no nokia tone ur mobile every week just txt nokia to get txting and tell ur mate www getzed co uk pobox w wq norm p tone',\n",
       " 'you have won a guaranteed award or even cashto claim ur award call free on stop getstop on php rg jx',\n",
       " 'k i ll be there before',\n",
       " 'i dled d it very imp',\n",
       " 'sure but make sure he know we ain t smokin yet',\n",
       " 'boooo you always work just quit',\n",
       " 'i am taking half day leave bec i am not well',\n",
       " 'ugh i don t wanna get out of bed it s so warm',\n",
       " 's s nervous lt gt',\n",
       " 'so there s a ring that come with the guy costume it s there so they can gift their future yowifes hint hint',\n",
       " 'congratulation ur awarded either of cd gift voucher free entry our weekly draw txt music to tncs www ldew com win ppmx age',\n",
       " 'i borrow ur bag ok',\n",
       " 'u were outbid by simonwatson on the shinco dvd plyr bid again visit sm ac smsrewards end bid notification reply end out',\n",
       " 'where s my boytoy i miss you what happened',\n",
       " 'he ha lot of used one babe but the model doesn t help youi have to bring it over and he ll match it up',\n",
       " 'also are you bringing galileo or dobby',\n",
       " 'then why you not responding',\n",
       " 'boo babe u enjoyin yourjob u seemed b gettin on well hunny hope ure ok take care i llspeak u soonlots of loveme xxxx',\n",
       " 'good afternoon starshine how s my boytoy doe he crave me yet ache to fuck me sip cappuccino i miss you babe teasing kiss',\n",
       " 'on the road so cant txt',\n",
       " 'smsservices for yourinclusive text credit pls goto www comuk net login qxj unsubscribe with stop no extra charge help comuk cm ae',\n",
       " 'p alfie moon s child in need song on ur mob tell ur m s txt tone charity to for nokias or poly charity for polys zed profit charity',\n",
       " 'have a good evening ttyl',\n",
       " 'hmm bit and piece lol sigh',\n",
       " 'hahaha use your brain dear',\n",
       " 'hey you got any mail',\n",
       " 'sorry light turned green i meant another friend wanted lt gt worth but he may not be around',\n",
       " 'thanks for yesterday sir you have been wonderful hope you enjoyed the burial mojibiola',\n",
       " 'u have a secret admirer reveal who think u r so special call to opt out reply reveal stop per msg recd cust care',\n",
       " 'hi mate it rv did u hav a nice hol just a message say hello coz haven t sent u in age started driving so stay off road rvx',\n",
       " 'dear voucher holder to claim this week offer at you pc please go to http www e tlp co uk expressoffer t c apply to stop text txt stop to',\n",
       " 'thank you so much when we skyped wit kz and sura we didnt get the pleasure of your company hope you are good we ve given you ultimatum oh we are countin down to aburo enjoy this is the message i sent day ago',\n",
       " 'surely result will offer',\n",
       " 'good morning my dear have a great amp successful day',\n",
       " 'do you want anytime any network min text and a new video phone for only five pound per week call or reply for delivery tomorrow',\n",
       " 'sir i have been late in paying rent for the past few month and had to pay a lt gt charge i felt it would be inconsiderate of me to nag about something you give at great cost to yourself and that s why i didnt speak up i however am in a recession and wont be able to pay the charge this month hence my askin well ahead of month s end can you please help thanks',\n",
       " 'we tried to contact you re our offer of new video phone anytime any network min half price rental camcorder call or reply for delivery wed',\n",
       " 'last chance claim ur worth of discount voucher text yes to now savamob member offer mobile t c sub remove txt x or stop',\n",
       " 'i luv u soo much u don t understand how special u r me ring u morrow luv u xxx',\n",
       " 'pls send me a comprehensive mail about who i m paying when and how much',\n",
       " 'our prashanthettan s mother passed away last night pray for her and family',\n",
       " 'urgent call from your landline your complimentary ibiza holiday or cash await collection sae t c po box sk wp ppm',\n",
       " 'k k when are you going',\n",
       " 'meanwhile in the shit suite xavier decided to give u lt gt second of warning that samantha wa coming over and is playing jay s guitar to impress her or some shit also i don t think doug realizes i don t live here anymore',\n",
       " 'my stomach ha been thru so much trauma i swear i just can t eat i better lose weight',\n",
       " 'i am in office whats the matter msg me now i will call you at break',\n",
       " 'yeah there s barely enough room for the two of u x ha too many fucking shoe sorry man see you later',\n",
       " 'today s offer claim ur worth of discount voucher text yes to now savamob member offer mobile t c sub unsub reply x',\n",
       " 'u reach orchard already u wan go buy ticket first',\n",
       " 'i am real baby i want to bring out your inner tigress',\n",
       " 'no da if you run that it activate the full version da',\n",
       " 'ah poor baby hope urfeeling bettersn luv probthat overdose of work hey go careful spk u sn lot of lovejen xxx',\n",
       " 'stop the story i ve told him i ve returned it and he s saying i should not re order it',\n",
       " 'talk sexy make new friend or fall in love in the world most discreet text dating service just text vip to and see who you could meet',\n",
       " 'going to take your babe out',\n",
       " 'hai ana tomarrow am coming on morning lt decimal gt ill be there in sathy then we ll go to rto office reply me after came to home',\n",
       " 'spoon it is then okay',\n",
       " 'did he just say somebody is named tampa',\n",
       " 'in work now going have in few min',\n",
       " 'your brother is a genius',\n",
       " 'sorry i guess whenever i can get a hold of my connection maybe an hour or two i ll text you',\n",
       " 'did u find out what time the bus is at coz i need to sort some stuff out',\n",
       " 'dude ive been seeing a lotta corvette lately',\n",
       " 'congratulation ur awarded either a yr supply of cd from virgin record or a mystery gift guaranteed call t c www smsco net pm approx min',\n",
       " 'same here but i consider wall and bunker and shit important just because i never play on peaceful but i guess your place is high enough that it don t matter',\n",
       " 'private your account statement for xxxxxx show un redeemed s i m point call identifier code expires',\n",
       " 'hello we need some posh bird and chap to user trial prod for champneys can i put you down i need your address and dob asap ta r',\n",
       " 'what do u want for xmas how about free text message a new video phone with half price line rental call free now on to find out more',\n",
       " 'well am officially in a philosophical hole so if u wanna call am at home ready to be saved',\n",
       " 'it going good no problem but still need little experience to understand american customer voice',\n",
       " 'i ll text you when i drop x off',\n",
       " 'ugh it been a long day i m exhausted just want to cuddle up and take a nap',\n",
       " 'talk with yourself atleast once in a day otherwise you will miss your best friend in this world shakespeare shesil lt gt',\n",
       " 'shop till u drop is it you either k k cash or travel voucher call now ntt po box cr bt fixedline cost ppm mobile vary',\n",
       " 'are you in castor you need to see something',\n",
       " 'sunshine quiz wkly q win a top sony dvd player if u know which country liverpool played in mid week txt ansr to sp tyrone',\n",
       " 'u have a secret admirer who is looking make contact with u find out who they r reveal who think ur so special call on',\n",
       " 'u have a secret admirer who is looking make contact with u find out who they r reveal who think ur so special call on stopsms',\n",
       " 'reminder you have not downloaded the content you have already paid for goto http doit mymoby tv to collect your content',\n",
       " 'see i knew giving you a break a few time woul lead to you always wanting to miss curfew i wa gonna gibe you til one but a midnight movie is not gonna get out til after you need to come home you need to getsleep and if anything you need to b studdying ear training',\n",
       " 'i love to give massage i use lot of baby oil what is your fave position',\n",
       " 'dude we should go sup again',\n",
       " 'yoyyooo u know how to change permission for a drive in mac my usb flash drive',\n",
       " 'gibbs unsold mike hussey',\n",
       " 'i like to talk pa but am not able to i dont know y',\n",
       " 'y dun cut too short leh u dun like ah she failed she s quite sad',\n",
       " 'you unbelievable faglord',\n",
       " 'wife how she knew the time of murder exactly',\n",
       " 'why do you ask princess',\n",
       " 'i am great princess what are you thinking about me',\n",
       " 'nutter cutter ctter cttergg cttargg ctargg ctagg ie you',\n",
       " 'it s ok i noe u re busy but i m really too bored so i msg u i oso dunno wat colour she choose me one',\n",
       " 'doesn t g have class early tomorrow and thus shouldn t be trying to smoke at lt gt',\n",
       " 'superb thought be grateful that u dont have everything u want that mean u still have an opportunity to be happier tomorrow than u are today',\n",
       " 'hope you are having a good week just checking in',\n",
       " 'i m used to it i just hope my agent don t drop me since i ve only booked a few thing this year this whole me in boston them in nyc wa an experiment',\n",
       " 'thursday night yeah sure thing we ll work it out then',\n",
       " 'your free ringtone is waiting to be collected simply text the password mix to to verify get usher and britney fml po box mk h ppw',\n",
       " 'probably money worry thing are coming due and i have several outstanding invoice for work i did two and three month ago',\n",
       " 'how is it possible to teach you and where',\n",
       " 'i wonder if your phone battery went dead i had to tell you i love you babe',\n",
       " 'lovely smell on this bus and it ain t tobacco',\n",
       " 'we re all getting worried over here derek and taylor have already assumed the worst',\n",
       " 'hey what s up charles sorry about the late reply',\n",
       " 'all the lastest from stereophonics marley dizzee racal libertine and the stroke win nookii game with flirt click themob wap bookmark or text wap to',\n",
       " 'i ll give her once i have it plus she said grinule greet you whenever we speak',\n",
       " 'white fudge oreo are in store',\n",
       " 'january male sale hot gay chat now cheaper call national rate from p min cheap to p min peak to stop text call p min',\n",
       " 'my love how come it took you so long to leave for zaher s i got your word on ym and wa happy to see them but wa sad you had left i miss you',\n",
       " 'i am sorry it hurt you',\n",
       " 'can t i feel nauseous i m so pissed i didn t eat any sweet all week cause today i wa planning to pig out i wa dieting all week and now i m not hungry',\n",
       " 'ok lor but not too early me still having project meeting now',\n",
       " 'call me da i am waiting for your call',\n",
       " 'i could ask carlos if we could get more if anybody else can chip in',\n",
       " 'wa actually about to send you a reminder today have a wonderful weekend',\n",
       " 'when people see my msg they think iam addicted to msging they are wrong bcoz they don t know that iam addicted to my sweet friend bslvyl',\n",
       " 'hey you gave them your photo when you registered for driving ah tmr wanna meet at yck',\n",
       " 'dont talk to him ever ok it my word',\n",
       " 'when u wana see it then',\n",
       " 'on ma way to school can you pls send me ashley s number',\n",
       " 'it shall be fine i have avalarr now will hollalater',\n",
       " 'she went to attend another two round today but still did t reach home',\n",
       " 'actually i deleted my old website now i m blogging at magicalsongs blogspot com',\n",
       " 'k wait chikku il send aftr lt gt min',\n",
       " 'but i m on a diet and i ate too many slice of pizza yesterday ugh i m always on a diet',\n",
       " 'k i will give my kvb acc detail',\n",
       " 'oh all have to come ah',\n",
       " 'money you r a lucky winner claim your prize text money over million to give away ppt x normal text rate box w t jy',\n",
       " 'i m really sorry i won t b able do this friday hope u can find an alternative hope yr term s going ok',\n",
       " 'congratulation ore mo owo re wa enjoy it and i wish you many happy moment to and fro wherever you go',\n",
       " 'so do you have samus shoulder yet',\n",
       " 'what time you think you ll have it need to know when i should be near campus',\n",
       " 'dear matthew please call from a landline your complimentary lux tenerife holiday or cash await collection ppm sae t c box sk xh',\n",
       " 'then dun wear jean lor',\n",
       " 'since when which side any fever any vomitin',\n",
       " 'k k are you in college',\n",
       " 'urgent call from landline your complimentary tenerife holiday or cash await collection sae t c box hp yf ppm',\n",
       " 'better made up for friday and stuffed myself like a pig yesterday now i feel bleh but at least it not writhing pain kind of bleh',\n",
       " 'no we sell it all so we ll have ton if coin then sell our coin to someone thru paypal voila money back in life pocket',\n",
       " 'theyre doing it to lot of place only hospital and medical place are safe',\n",
       " 'how about getting in touch with folk waiting for company just txt back your name and age to opt in enjoy the community p sm',\n",
       " 'and also i ve sorta blown him off a couple time recently so id rather not text him out of the blue looking for weed',\n",
       " 'i sent my score to sophas and i had to do secondary application for a few school i think if you are thinking of applying do a research on cost also contact joke ogunrinde her school is one me the less expensive one',\n",
       " 'i cant wait to see you how were the photo were useful',\n",
       " 'ur cash balance is currently pound to maximize ur cash in now send go to only p msg cc po box tcr w',\n",
       " 'hey i booked the kb on sat already what other lesson are we going for ah keep your sat night free we need to meet and confirm our lodging',\n",
       " 'chk in ur belovd m dict',\n",
       " 'is that what time you want me to come',\n",
       " 'awesome lemme know whenever you re around',\n",
       " 'shb b ok lor thanx',\n",
       " 'beautiful truth against gravity read carefully our heart feel light when someone is in it but it feel very heavy when someone leaf it good night',\n",
       " 'also remember to get dobby s bowl from your car',\n",
       " 'filthy story and girl waiting for your',\n",
       " 'sorry i now then c ur msg yar lor so poor thing but only one night tmr u ll have a brand new room sleep in',\n",
       " 'love isn t a decision it s a feeling if we could decide who to love then life would be much simpler but then less magical',\n",
       " 'welp apparently he retired',\n",
       " 'my sort code is and acc no is the bank is natwest can you reply to confirm i ve sent this to the right person',\n",
       " 'where',\n",
       " 'u sure u can t take any sick time',\n",
       " 'urgent we are trying to contact u today draw show that you have won a prize guaranteed call from land line claim m valid hr only',\n",
       " 'watching cartoon listening music amp at eve had to go temple amp church what about u',\n",
       " 'yo chad which gymnastics class do you wanna take the site say christian class is full',\n",
       " 'are you this much buzy',\n",
       " 'or better still can you catch her and let ask her if she can sell lt gt for me',\n",
       " 'i am not sure about night menu i know only about noon menu',\n",
       " 'what do u want when i come back a beautiful necklace a a token of my heart for you thats what i will give but only to my wife of my liking be that and see no one can give you that dont call me i will wait till i come',\n",
       " 'are you willing to go for aptitude class',\n",
       " 'it wont b until a trying sort house out is that ok',\n",
       " 'yar lor he wan go c horse racing today mah so eat earlier lor i ate chicken rice u',\n",
       " 'haha awesome omw back now then',\n",
       " 'yup i thk so until e shop close lor',\n",
       " 'what is your account number',\n",
       " 'eh u send wrongly lar',\n",
       " 'hey no i ad a crap nite wa borin without ya boggy with me u boring biatch thanx but u wait til nxt time il ave ya',\n",
       " 'ok i shall talk to him',\n",
       " 'dont hesitate you know this is the second time she ha had weakness like that so keep i notebook of what she eat and did the day before or if anything changed the day before so that we can be sure it nothing',\n",
       " 'hey you can pay with salary de only lt gt',\n",
       " 'another month i need chocolate weed and alcohol',\n",
       " 'if he started searching he will get job in few day he have great potential and talent',\n",
       " 'reckon need to be in town by eightish to walk from carpark',\n",
       " 'congrats mobile g videophones r yours call now videochat wid your mate play java game dload polyph music noline rentl',\n",
       " 'look at the fuckin time what the fuck you think is up',\n",
       " 'yo guess what i just dropped',\n",
       " 'carlos say he ll be at mu in lt gt minute',\n",
       " 'i m in office now i will call you lt gt min',\n",
       " 'geeee i miss you already you know your all i can think about fuck i can t wait till next year when we will be together loving kiss',\n",
       " 'yun ah the ubi one say if wan call by tomorrow call look for irene ere only got bus ubi cres ubi tech park ph for st wkg day n',\n",
       " 'ugh gotta drive back to sd from la my butt is sore',\n",
       " 'th of july',\n",
       " 'hi im having the most relaxing time ever we have to get up at am every day wa the party good the other night i get home tomorrow at ish',\n",
       " 'up to wan come then come lor but i din c any stripe skirt',\n",
       " 'the xmas story is peace the xmas msg is love the xmas miracle is jesus hav a blessed month ahead amp wish u merry xmas',\n",
       " 'i can t i don t have her number',\n",
       " 'change again it s e one next to escalator',\n",
       " 'yetunde i m in class can you not run water on it to make it ok pls now',\n",
       " 'not a lot ha happened here feel very quiet beth is at her aunt and charlie is working lot just me and helen in at the mo how have you been',\n",
       " 'then wait me at bus stop aft ur lect lar if i dun c then i go get my car then come back n pick',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78b1835d-2872-4780-9e57-fe1cd4c0d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52d7a9ba-ae46-4687-bc97-40cc541af604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = your cleaned text messages.\n",
    "\n",
    "# sent_tokenize(sent) = splits a message into sentences.\n",
    "\n",
    "# simple_preprocess(sent) = lowercases, removes punctuations, tokenizes words.\n",
    "words=[]\n",
    "for sent in corpus:\n",
    "    sent_token=sent_tokenize(sent)\n",
    "    for sent in sent_token:\n",
    "        words.append(simple_preprocess(sent))\n",
    "# After this step, words is a list of lists, where each sublist is a tokenized sentence.\n",
    "# corpus = [\"I love dogs. They are good!\", \"Cats are smart.\"]\n",
    "# words =\n",
    "# [\n",
    "#    ['i', 'love', 'dogs'],\n",
    "#    ['they', 'are', 'good'],\n",
    "#    ['cats', 'are', 'smart']\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df71268b-e7f0-46ca-843c-4ea0cfd0f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "439ef927-59a4-4881-8b4c-754b022b3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets train Word2vec from scratch\n",
    "# This trains a Word2Vec embedding model on your sentences.\n",
    "# Each word gets mapped to a vector (default: 100-dimensional).\n",
    "model=gensim.models.Word2Vec(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e4789d5-e54c-4a92-94d1-2706d51622de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'you',\n",
       " 'the',\n",
       " 'it',\n",
       " 'and',\n",
       " 'in',\n",
       " 'is',\n",
       " 'me',\n",
       " 'my',\n",
       " 'for',\n",
       " 'your',\n",
       " 'call',\n",
       " 'of',\n",
       " 'that',\n",
       " 'have',\n",
       " 'on',\n",
       " 'now',\n",
       " 'are',\n",
       " 'can',\n",
       " 'so',\n",
       " 'but',\n",
       " 'not',\n",
       " 'or',\n",
       " 'we',\n",
       " 'do',\n",
       " 'get',\n",
       " 'at',\n",
       " 'ur',\n",
       " 'will',\n",
       " 'if',\n",
       " 'be',\n",
       " 'with',\n",
       " 'no',\n",
       " 'just',\n",
       " 'this',\n",
       " 'gt',\n",
       " 'lt',\n",
       " 'go',\n",
       " 'how',\n",
       " 'up',\n",
       " 'when',\n",
       " 'ok',\n",
       " 'day',\n",
       " 'what',\n",
       " 'free',\n",
       " 'from',\n",
       " 'all',\n",
       " 'out',\n",
       " 'know',\n",
       " 'll',\n",
       " 'come',\n",
       " 'like',\n",
       " 'good',\n",
       " 'time',\n",
       " 'am',\n",
       " 'then',\n",
       " 'got',\n",
       " 'wa',\n",
       " 'there',\n",
       " 'he',\n",
       " 'love',\n",
       " 'text',\n",
       " 'only',\n",
       " 'want',\n",
       " 'send',\n",
       " 'one',\n",
       " 'need',\n",
       " 'txt',\n",
       " 'today',\n",
       " 'by',\n",
       " 'going',\n",
       " 'don',\n",
       " 'stop',\n",
       " 'home',\n",
       " 'she',\n",
       " 'about',\n",
       " 'lor',\n",
       " 'sorry',\n",
       " 'see',\n",
       " 'still',\n",
       " 'mobile',\n",
       " 'take',\n",
       " 'back',\n",
       " 'da',\n",
       " 'reply',\n",
       " 'dont',\n",
       " 'our',\n",
       " 'think',\n",
       " 'tell',\n",
       " 'week',\n",
       " 'hi',\n",
       " 'phone',\n",
       " 'they',\n",
       " 'new',\n",
       " 'please',\n",
       " 'later',\n",
       " 'pls',\n",
       " 'any',\n",
       " 'her',\n",
       " 'ha',\n",
       " 'co',\n",
       " 'did',\n",
       " 'been',\n",
       " 'msg',\n",
       " 'min',\n",
       " 'some',\n",
       " 'an',\n",
       " 'night',\n",
       " 'make',\n",
       " 'dear',\n",
       " 'who',\n",
       " 'here',\n",
       " 'message',\n",
       " 'say',\n",
       " 'well',\n",
       " 'where',\n",
       " 're',\n",
       " 'thing',\n",
       " 'much',\n",
       " 'oh',\n",
       " 'great',\n",
       " 'claim',\n",
       " 'hope',\n",
       " 'hey',\n",
       " 'him',\n",
       " 'number',\n",
       " 'give',\n",
       " 'more',\n",
       " 'too',\n",
       " 'happy',\n",
       " 'work',\n",
       " 'wat',\n",
       " 'friend',\n",
       " 'had',\n",
       " 'yes',\n",
       " 'way',\n",
       " 'www',\n",
       " 've',\n",
       " 'let',\n",
       " 'should',\n",
       " 'prize',\n",
       " 'won',\n",
       " 'right',\n",
       " 'tomorrow',\n",
       " 'already',\n",
       " 'tone',\n",
       " 'after',\n",
       " 'ask',\n",
       " 'win',\n",
       " 'said',\n",
       " 'life',\n",
       " 'amp',\n",
       " 'cash',\n",
       " 'doing',\n",
       " 'im',\n",
       " 'yeah',\n",
       " 'really',\n",
       " 'meet',\n",
       " 'babe',\n",
       " 'why',\n",
       " 'find',\n",
       " 'them',\n",
       " 'miss',\n",
       " 'morning',\n",
       " 'very',\n",
       " 'last',\n",
       " 'year',\n",
       " 'service',\n",
       " 'thanks',\n",
       " 'uk',\n",
       " 'care',\n",
       " 'com',\n",
       " 'would',\n",
       " 'anything',\n",
       " 'lol',\n",
       " 'nokia',\n",
       " 'also',\n",
       " 'every',\n",
       " 'feel',\n",
       " 'keep',\n",
       " 'sure',\n",
       " 'pick',\n",
       " 'urgent',\n",
       " 'over',\n",
       " 'sent',\n",
       " 'contact',\n",
       " 'something',\n",
       " 'buy',\n",
       " 'gud',\n",
       " 'again',\n",
       " 'cant',\n",
       " 'wait',\n",
       " 'before',\n",
       " 'place',\n",
       " 'box',\n",
       " 'first',\n",
       " 'his',\n",
       " 'even',\n",
       " 'someone',\n",
       " 'guy',\n",
       " 'help',\n",
       " 'went',\n",
       " 'wish',\n",
       " 'next',\n",
       " 'tonight',\n",
       " 'were',\n",
       " 'nice',\n",
       " 'soon',\n",
       " 'show',\n",
       " 'which',\n",
       " 'off',\n",
       " 'around',\n",
       " 'word',\n",
       " 'could',\n",
       " 'customer',\n",
       " 'money',\n",
       " 'sleep',\n",
       " 'chat',\n",
       " 'per',\n",
       " 'many',\n",
       " 'late',\n",
       " 'always',\n",
       " 'ya',\n",
       " 'gonna',\n",
       " 'sm',\n",
       " 'down',\n",
       " 'leave',\n",
       " 'wan',\n",
       " 'name',\n",
       " 'lot',\n",
       " 'end',\n",
       " 'other',\n",
       " 'dun',\n",
       " 'pm',\n",
       " 'told',\n",
       " 'st',\n",
       " 'person',\n",
       " 'waiting',\n",
       " 'hello',\n",
       " 'special',\n",
       " 'try',\n",
       " 'month',\n",
       " 'girl',\n",
       " 'hour',\n",
       " 'may',\n",
       " 'fine',\n",
       " 'best',\n",
       " 'haha',\n",
       " 'minute',\n",
       " 'heart',\n",
       " 'people',\n",
       " 'coming',\n",
       " 'done',\n",
       " 'guaranteed',\n",
       " 'yet',\n",
       " 'thk',\n",
       " 'same',\n",
       " 'th',\n",
       " 'getting',\n",
       " 'smile',\n",
       " 'ppm',\n",
       " 'use',\n",
       " 'thought',\n",
       " 'god',\n",
       " 'didn',\n",
       " 'offer',\n",
       " 'stuff',\n",
       " 'holiday',\n",
       " 'talk',\n",
       " 'start',\n",
       " 'class',\n",
       " 'man',\n",
       " 'cost',\n",
       " 'live',\n",
       " 'mean',\n",
       " 'bit',\n",
       " 'line',\n",
       " 'car',\n",
       " 'lunch',\n",
       " 'few',\n",
       " 'draw',\n",
       " 'finish',\n",
       " 'job',\n",
       " 'being',\n",
       " 'problem',\n",
       " 'never',\n",
       " 'yup',\n",
       " 'ill',\n",
       " 'better',\n",
       " 'plan',\n",
       " 'trying',\n",
       " 'house',\n",
       " 'thats',\n",
       " 'cool',\n",
       " 'hr',\n",
       " 'meeting',\n",
       " 'account',\n",
       " 'rate',\n",
       " 'mind',\n",
       " 'pobox',\n",
       " 'ready',\n",
       " 'having',\n",
       " 'dat',\n",
       " 'long',\n",
       " 'weekend',\n",
       " 'game',\n",
       " 'chance',\n",
       " 'world',\n",
       " 'real',\n",
       " 'half',\n",
       " 'enjoy',\n",
       " 'latest',\n",
       " 'wk',\n",
       " 'po',\n",
       " 'room',\n",
       " 'yo',\n",
       " 'sir',\n",
       " 'check',\n",
       " 'because',\n",
       " 'than',\n",
       " 'bt',\n",
       " 'guess',\n",
       " 'play',\n",
       " 'awarded',\n",
       " 'wanna',\n",
       " 'nothing',\n",
       " 'lar',\n",
       " 'receive',\n",
       " 'boy',\n",
       " 'voucher',\n",
       " 'eat',\n",
       " 'sweet',\n",
       " 'luv',\n",
       " 'look',\n",
       " 'camera',\n",
       " 'pic',\n",
       " 'another',\n",
       " 'liao',\n",
       " 'big',\n",
       " 'shit',\n",
       " 'dinner',\n",
       " 'into',\n",
       " 'ah',\n",
       " 'landline',\n",
       " 'birthday',\n",
       " 'xxx',\n",
       " 'jus',\n",
       " 'might',\n",
       " 'ever',\n",
       " 'quite',\n",
       " 'video',\n",
       " 'kiss',\n",
       " 'age',\n",
       " 'watching',\n",
       " 'wont',\n",
       " 'question',\n",
       " 'land',\n",
       " 'watch',\n",
       " 'dream',\n",
       " 'orange',\n",
       " 'early',\n",
       " 'thanx',\n",
       " 'worry',\n",
       " 'baby',\n",
       " 'called',\n",
       " 'speak',\n",
       " 'two',\n",
       " 'tv',\n",
       " 'aight',\n",
       " 'once',\n",
       " 'hear',\n",
       " 'fun',\n",
       " 'probably',\n",
       " 'nd',\n",
       " 'point',\n",
       " 'bed',\n",
       " 'pa',\n",
       " 'pay',\n",
       " 'doe',\n",
       " 'actually',\n",
       " 'network',\n",
       " 'princess',\n",
       " 'nite',\n",
       " 'apply',\n",
       " 'maybe',\n",
       " 'shall',\n",
       " 'bus',\n",
       " 'part',\n",
       " 'sat',\n",
       " 'left',\n",
       " 'forgot',\n",
       " 'den',\n",
       " 'bad',\n",
       " 'ringtone',\n",
       " 'remember',\n",
       " 'office',\n",
       " 'hurt',\n",
       " 'easy',\n",
       " 'reach',\n",
       " 'code',\n",
       " 'shopping',\n",
       " 'between',\n",
       " 'dunno',\n",
       " 'made',\n",
       " 'xx',\n",
       " 'dis',\n",
       " 'little',\n",
       " 'evening',\n",
       " 'didnt',\n",
       " 'award',\n",
       " 'put',\n",
       " 'true',\n",
       " 'leh',\n",
       " 'fuck',\n",
       " 'everything',\n",
       " 'wife',\n",
       " 'anyway',\n",
       " 'face',\n",
       " 'dad',\n",
       " 'looking',\n",
       " 'town',\n",
       " 'thank',\n",
       " 'afternoon',\n",
       " 'gift',\n",
       " 'school',\n",
       " 'enough',\n",
       " 'sound',\n",
       " 'those',\n",
       " 'mail',\n",
       " 'working',\n",
       " 'mate',\n",
       " 'selected',\n",
       " 'yr',\n",
       " 'movie',\n",
       " 'most',\n",
       " 'collect',\n",
       " 'pound',\n",
       " 'detail',\n",
       " 'without',\n",
       " 'asked',\n",
       " 'entry',\n",
       " 'while',\n",
       " 'missing',\n",
       " 'tmr',\n",
       " 'hav',\n",
       " 'join',\n",
       " 'price',\n",
       " 'sexy',\n",
       " 'okay',\n",
       " 'though',\n",
       " 'pain',\n",
       " 'wif',\n",
       " 'important',\n",
       " 'must',\n",
       " 'xmas',\n",
       " 'wanted',\n",
       " 'until',\n",
       " 'since',\n",
       " 'valid',\n",
       " 'came',\n",
       " 'update',\n",
       " 'mob',\n",
       " 'answer',\n",
       " 'wot',\n",
       " 'lesson',\n",
       " 'missed',\n",
       " 'wake',\n",
       " 'book',\n",
       " 'abt',\n",
       " 'run',\n",
       " 'collection',\n",
       " 'bring',\n",
       " 'able',\n",
       " 'til',\n",
       " 'wen',\n",
       " 'haven',\n",
       " 'decimal',\n",
       " 'de',\n",
       " 'test',\n",
       " 'charge',\n",
       " 'juz',\n",
       " 'plus',\n",
       " 'change',\n",
       " 'stay',\n",
       " 'date',\n",
       " 'plz',\n",
       " 'colour',\n",
       " 'away',\n",
       " 'double',\n",
       " 'weekly',\n",
       " 'havent',\n",
       " 'yesterday',\n",
       " 'saw',\n",
       " 'else',\n",
       " 'music',\n",
       " 'till',\n",
       " 'shop',\n",
       " 'dude',\n",
       " 'bored',\n",
       " 'attempt',\n",
       " 'alright',\n",
       " 'hair',\n",
       " 'lei',\n",
       " 'food',\n",
       " 'optout',\n",
       " 'trip',\n",
       " 'credit',\n",
       " 'friendship',\n",
       " 'making',\n",
       " 'drink',\n",
       " 'net',\n",
       " 'these',\n",
       " 'online',\n",
       " 'id',\n",
       " 'haf',\n",
       " 'yours',\n",
       " 'top',\n",
       " 'oso',\n",
       " 'coz',\n",
       " 'goin',\n",
       " 'tried',\n",
       " 'gr',\n",
       " 'family',\n",
       " 'address',\n",
       " 'delivery',\n",
       " 'sch',\n",
       " 'hot',\n",
       " 'player',\n",
       " 'club',\n",
       " 'either',\n",
       " 'smoke',\n",
       " 'ard',\n",
       " 'driving',\n",
       " 'feeling',\n",
       " 'national',\n",
       " 'yourself',\n",
       " 'nt',\n",
       " 'sad',\n",
       " 'order',\n",
       " 'lose',\n",
       " 'together',\n",
       " 'calling',\n",
       " 'full',\n",
       " 'wid',\n",
       " 'second',\n",
       " 'story',\n",
       " 'mom',\n",
       " 'busy',\n",
       " 'ring',\n",
       " 'beautiful',\n",
       " 'head',\n",
       " 'bonus',\n",
       " 'walk',\n",
       " 'http',\n",
       " 'brother',\n",
       " 'tot',\n",
       " 'both',\n",
       " 'si',\n",
       " 'sae',\n",
       " 'post',\n",
       " 'believe',\n",
       " 'smiling',\n",
       " 'huh',\n",
       " 'close',\n",
       " 'poly',\n",
       " 'old',\n",
       " 'eve',\n",
       " 'row',\n",
       " 'chikku',\n",
       " 'happen',\n",
       " 'noe',\n",
       " 'drive',\n",
       " 'await',\n",
       " 'set',\n",
       " 'info',\n",
       " 'hand',\n",
       " 'saying',\n",
       " 'mum',\n",
       " 'sleeping',\n",
       " 'leaving',\n",
       " 'awesome',\n",
       " 'mths',\n",
       " 'took',\n",
       " 'congrats',\n",
       " 'pub',\n",
       " 'hl',\n",
       " 'email',\n",
       " 'drop',\n",
       " 'parent',\n",
       " 'wil',\n",
       " 'rite',\n",
       " 'tomo',\n",
       " 'match',\n",
       " 'thinking',\n",
       " 'suite',\n",
       " 'started',\n",
       " 'news',\n",
       " 'simple',\n",
       " 'aft',\n",
       " 'finished',\n",
       " 'private',\n",
       " 'cause',\n",
       " 'doesn',\n",
       " 'okie',\n",
       " 'tho',\n",
       " 'unsubscribe',\n",
       " 'auction',\n",
       " 'available',\n",
       " 'tc',\n",
       " 'forget',\n",
       " 'content',\n",
       " 'everyone',\n",
       " 'company',\n",
       " 'anyone',\n",
       " 'sister',\n",
       " 'touch',\n",
       " 'break',\n",
       " 'caller',\n",
       " 'valentine',\n",
       " 'reason',\n",
       " 'mine',\n",
       " 'final',\n",
       " 'card',\n",
       " 'angry',\n",
       " 'neva',\n",
       " 'taking',\n",
       " 'gd',\n",
       " 'statement',\n",
       " 'open',\n",
       " 'dating',\n",
       " 'loving',\n",
       " 'whats',\n",
       " 'alone',\n",
       " 'found',\n",
       " 'treat',\n",
       " 'whatever',\n",
       " 'lucky',\n",
       " 'fancy',\n",
       " 'carlos',\n",
       " 'gal',\n",
       " 'choose',\n",
       " 'worth',\n",
       " 'opt',\n",
       " 'each',\n",
       " 'ticket',\n",
       " 'search',\n",
       " 'sun',\n",
       " 'knw',\n",
       " 'type',\n",
       " 'bank',\n",
       " 'expires',\n",
       " 'wonderful',\n",
       " 'frnd',\n",
       " 'hows',\n",
       " 'mobileupd',\n",
       " 'hit',\n",
       " 'winner',\n",
       " 'hard',\n",
       " 'boytoy',\n",
       " 'gone',\n",
       " 'saturday',\n",
       " 'gbp',\n",
       " 'welcome',\n",
       " 'fast',\n",
       " 'happened',\n",
       " 'quiz',\n",
       " 'anytime',\n",
       " 'kind',\n",
       " 'congratulation',\n",
       " 'bout',\n",
       " 'secret',\n",
       " 'far',\n",
       " 'identifier',\n",
       " 'decided',\n",
       " 'sub',\n",
       " 'ni',\n",
       " 'exam',\n",
       " 'uncle',\n",
       " 'ltd',\n",
       " 'party',\n",
       " 'smth',\n",
       " 'friday',\n",
       " 'college',\n",
       " 'visit',\n",
       " 'nyt',\n",
       " 'prob',\n",
       " 'song',\n",
       " 'darlin',\n",
       " 'mu',\n",
       " 'hold',\n",
       " 'read',\n",
       " 'light',\n",
       " 'operator',\n",
       " 'oredi',\n",
       " 'goodmorning',\n",
       " 'finally',\n",
       " 'mrng',\n",
       " 'tel',\n",
       " 'sea',\n",
       " 'wit',\n",
       " 'project',\n",
       " 'pretty',\n",
       " 'outside',\n",
       " 'nope',\n",
       " 'term',\n",
       " 'used',\n",
       " 'drug',\n",
       " 'fucking',\n",
       " 'wonder',\n",
       " 'camcorder',\n",
       " 'lovely',\n",
       " 'wrong',\n",
       " 'least',\n",
       " 'chennai',\n",
       " 'fri',\n",
       " 'crazy',\n",
       " 'ten',\n",
       " 'log',\n",
       " 'cum',\n",
       " 'listen',\n",
       " 'frnds',\n",
       " 'freemsg',\n",
       " 'seeing',\n",
       " 'blue',\n",
       " 'telling',\n",
       " 'fone',\n",
       " 'case',\n",
       " 'meant',\n",
       " 'jay',\n",
       " 'whole',\n",
       " 'fr',\n",
       " 'unlimited',\n",
       " 'cd',\n",
       " 'their',\n",
       " 'wasn',\n",
       " 'isn',\n",
       " 'support',\n",
       " 'course',\n",
       " 'frm',\n",
       " 'sunday',\n",
       " 'hmm',\n",
       " 'wq',\n",
       " 'savamob',\n",
       " 'snow',\n",
       " 'hungry',\n",
       " 'earlier',\n",
       " 'wkly',\n",
       " 'stupid',\n",
       " 'die',\n",
       " 'happiness',\n",
       " 'un',\n",
       " 'dnt',\n",
       " 'move',\n",
       " 'etc',\n",
       " 'hee',\n",
       " 'within',\n",
       " 'single',\n",
       " 'yar',\n",
       " 'hmmm',\n",
       " 'cut',\n",
       " 'mr',\n",
       " 'eh',\n",
       " 'moment',\n",
       " 'march',\n",
       " 'enter',\n",
       " 'joy',\n",
       " 'luck',\n",
       " 'film',\n",
       " 'na',\n",
       " 'balance',\n",
       " 'gn',\n",
       " 'john',\n",
       " 'gas',\n",
       " 'child',\n",
       " 'knew',\n",
       " 'understand',\n",
       " 'pas',\n",
       " 'father',\n",
       " 'valued',\n",
       " 'store',\n",
       " 'tired',\n",
       " 'bslvyl',\n",
       " 'mayb',\n",
       " 'sell',\n",
       " 'almost',\n",
       " 'sex',\n",
       " 'paper',\n",
       " 'press',\n",
       " 'side',\n",
       " 'area',\n",
       " 'sk',\n",
       " 'currently',\n",
       " 'couple',\n",
       " 'txts',\n",
       " 'computer',\n",
       " 'rock',\n",
       " 'invited',\n",
       " 'felt',\n",
       " 'as',\n",
       " 'ago',\n",
       " 'download',\n",
       " 'india',\n",
       " 'lost',\n",
       " 'mah',\n",
       " 'christmas',\n",
       " 'talking',\n",
       " 'reading',\n",
       " 'load',\n",
       " 'motorola',\n",
       " 'park',\n",
       " 'shower',\n",
       " 'bill',\n",
       " 'hospital',\n",
       " 'askd',\n",
       " 'picking',\n",
       " 'charged',\n",
       " 'photo',\n",
       " 'direct',\n",
       " 'heard',\n",
       " 'return',\n",
       " 'rental',\n",
       " 'eye',\n",
       " 'via',\n",
       " 'darren',\n",
       " 'confirm',\n",
       " 'semester',\n",
       " 'correct',\n",
       " 'reveal',\n",
       " 'red',\n",
       " 'doin',\n",
       " 'ac',\n",
       " 'laptop',\n",
       " 'xy',\n",
       " 'supposed',\n",
       " 'wow',\n",
       " 'sort',\n",
       " 'ugh',\n",
       " 'extra',\n",
       " 'information',\n",
       " 'bcoz',\n",
       " 'kid',\n",
       " 'gym',\n",
       " 'swing',\n",
       " 'redeemed',\n",
       " 'surprise',\n",
       " 'seen',\n",
       " 'difficult',\n",
       " 'through',\n",
       " 'ge',\n",
       " 'promise',\n",
       " 'met',\n",
       " 'max',\n",
       " 'sending',\n",
       " 'lady',\n",
       " 'complimentary',\n",
       " 'comp',\n",
       " 'figure',\n",
       " 'ipod',\n",
       " 'gotta',\n",
       " 'grin',\n",
       " 'ish',\n",
       " 'abiola',\n",
       " 'slowly',\n",
       " 'ex',\n",
       " 'whenever',\n",
       " 'discount',\n",
       " 'lovable',\n",
       " 'yep',\n",
       " 'muz',\n",
       " 'request',\n",
       " 'std',\n",
       " 'bath',\n",
       " 'police',\n",
       " 'hg',\n",
       " 'crave',\n",
       " 'usf',\n",
       " 'safe',\n",
       " 'reward',\n",
       " 'nobody',\n",
       " 'eg',\n",
       " 'orchard',\n",
       " 'road',\n",
       " 'kate',\n",
       " 'wine',\n",
       " 'comin',\n",
       " 'slow',\n",
       " 'weed',\n",
       " 'link',\n",
       " 'asap',\n",
       " 'truth',\n",
       " 'wap',\n",
       " 'fantasy',\n",
       " 'study',\n",
       " 'fact',\n",
       " 'loved',\n",
       " 'loan',\n",
       " 'cheer',\n",
       " 'small',\n",
       " 'somebody',\n",
       " 'page',\n",
       " 'rest',\n",
       " 'laugh',\n",
       " 'rply',\n",
       " 'hmv',\n",
       " 'joke',\n",
       " 'leaf',\n",
       " 'entered',\n",
       " 'txting',\n",
       " 'blood',\n",
       " 'wana',\n",
       " 'idea',\n",
       " 'noon',\n",
       " 'clean',\n",
       " 'dogging',\n",
       " 'door',\n",
       " 'checking',\n",
       " 'asking',\n",
       " 'train',\n",
       " 'own',\n",
       " 'remove',\n",
       " 'lover',\n",
       " 'monday',\n",
       " 'save',\n",
       " 'rent',\n",
       " 'pete',\n",
       " 'member',\n",
       " 'energy',\n",
       " 'nah',\n",
       " 'deal',\n",
       " 'near',\n",
       " 'del',\n",
       " 'forever',\n",
       " 'mistake',\n",
       " 'cup',\n",
       " 'copy',\n",
       " 'normal',\n",
       " 'somewhere',\n",
       " 'men',\n",
       " 'england',\n",
       " 'la',\n",
       " 'opinion',\n",
       " 'situation',\n",
       " 'em',\n",
       " 'cheap',\n",
       " 'warm',\n",
       " 'hoping',\n",
       " 'empty',\n",
       " 'across',\n",
       " 'hw',\n",
       " 'woke',\n",
       " 'usual',\n",
       " 'rakhesh',\n",
       " 'callertune',\n",
       " 'spend',\n",
       " 'med',\n",
       " 'gave',\n",
       " 'cover',\n",
       " 'write',\n",
       " 'short',\n",
       " 'bb',\n",
       " 'tonite',\n",
       " 'ringtones',\n",
       " 'bathe',\n",
       " 'water',\n",
       " 'different',\n",
       " 'representative',\n",
       " 'sony',\n",
       " 'ho',\n",
       " 'ntt',\n",
       " 'voice',\n",
       " 'king',\n",
       " 'merry',\n",
       " 'gap',\n",
       " 'poor',\n",
       " 'fantastic',\n",
       " 'booked',\n",
       " 'oops',\n",
       " 'wc',\n",
       " 'gettin',\n",
       " 'bag',\n",
       " 'admirer',\n",
       " 'getzed',\n",
       " 'ldn',\n",
       " 'kick',\n",
       " 'less',\n",
       " 'immediately',\n",
       " 'glad',\n",
       " 'summer',\n",
       " 'wishing',\n",
       " 'street',\n",
       " 'teach',\n",
       " 'cr',\n",
       " 'otherwise',\n",
       " 'worried',\n",
       " 'doctor',\n",
       " 'sale',\n",
       " 'il',\n",
       " 'convey',\n",
       " 'custcare',\n",
       " 'indian',\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To Get All the Vocabulary\n",
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba538fe6-6a3f-4687-85a6-ab1a506cf6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5569"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of sentences it was trained on\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b0450b8-587a-4f9d-83c0-26142b47cc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of training iterations\n",
    "model.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0172f0a0-c2d6-40b7-a293-2c8637851c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('my', 0.9992116093635559),\n",
       " ('well', 0.9988468289375305),\n",
       " ('night', 0.9987664818763733),\n",
       " ('not', 0.9987120032310486),\n",
       " ('day', 0.9986882209777832),\n",
       " ('all', 0.9986481666564941),\n",
       " ('hope', 0.9986382126808167),\n",
       " ('happy', 0.9986214637756348),\n",
       " ('great', 0.9986198544502258),\n",
       " ('nice', 0.998595118522644)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar words to \"good\"\n",
    "model.wv.similar_by_word('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "62bb542a-2f3d-4e0d-a0ab-1a7fb04327f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['good'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d0c64c5-db76-493e-96c7-20b9ee382c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'only',\n",
       " 'in',\n",
       " 'bugis',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b4b1c09-f1da-49e4-a494-8c92c974923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a list of words (doc).\n",
    "\n",
    "# It fetches the vector for each word from Word2Vec.\n",
    "\n",
    "# Takes the average → gives one fixed-length vector per sentence.\n",
    "\n",
    "# 👉 Why average? Because Word2Vec gives vectors for individual words, but for ML you often need sentence/document vectors. Averaging is a simple approach.\n",
    "def avg_word2vec(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    #sent = [word for word in doc if word in model.wv.index_to_key]\n",
    "    #print(sent)\n",
    "    \n",
    "    return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)\n",
    "                #or [np.zeros(len(model.wv.index_to_key))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0bc2b0fc-f1ca-4151-a334-d523879239de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(doc):\n",
    "    valid_words = [model.wv[word] for word in doc if word in model.wv.index_to_key]\n",
    "    if len(valid_words) == 0:\n",
    "        # return a zero vector if no valid words\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(valid_words, axis=0)\n",
    "\n",
    "# ✅ In short\n",
    "\n",
    "# You’re trying to create a 2D NumPy array of sentence embeddings.\n",
    "\n",
    "# Error = some rows are missing / have wrong shape (empty embeddings).\n",
    "\n",
    "# Fix = return a zero vector (or skip sentence) when no valid words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b40b054-2510-4b01-a180-f094c3cc53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09c64c2a-645b-468d-9906-96d30e765ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0686d017-c4b6-4ccc-854c-7bc854cedb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5569/5569 [00:00<00:00, 7473.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loops through all tokenized sentences in words.\n",
    "\n",
    "# Converts each into a vector using avg_word2vec.\n",
    "\n",
    "# Stores results in X.\n",
    "\n",
    "# So X becomes a list of sentence embeddings.\n",
    "#apply for the entire sentences\n",
    "import numpy as np\n",
    "X=[]\n",
    "for i in tqdm(range(len(words))):\n",
    "    X.append(avg_word2vec(words[i]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d51a6bb1-d94b-4159-93cd-9fe84544cad7",
   "metadata": {},
   "source": [
    "sample output:\n",
    "Sentence \"they are good\" → [0.12, -0.03, 0.55, ..., 0.08] (100-dim vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c950fe8-3cf8-4df1-bdbd-f16decbbd1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5569"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ceda9dbd-417a-45f1-a4e5-24dec03fb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "##independent Features\n",
    "X_new=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "08ec5b2b-fe95-432b-8c61-6a84a93c9e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6c4fe507-2b96-4949-8e0e-6c8bf2fa0d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.181, 0.204, 0.0944, 0.0807, 0.0841, -0.423, 0.131, 0.418, -0.216, -0.0977, -0.176, -0.333, -0.0744, 0.103, 0.153, -0.142, 0.112, -0.279, -0.0583, -0.473, 0.167, 0.0902, 0.0667, -0.17, -0.0146, -0.0123, -0.175, -0.185, -0.225, 0.0249, 0.282, 0.0219, 0.0995, -0.163, -0.125, 0.315, 0.0537, -0.0993, -0.0853, -0.427, 0.0867, -0.212, -0.177, 0.0253, 0.133, -2.21e-05, -0.112, -0.0547, 0.195, 0.119, 0.147, -0.158, -0.0399, 0.0543, -0.0719, 0.0325, 0.151, 0.0209, -0.327, 0.155, -0.0139, 0.139, -0.00809, -0.112, -0.235, 0.226, 0.0787, 0.191, -0.298, 0.33, -0.222, 0.14, 0.332, -0.101, 0.304, 0.0414, 0.0694, -0.0901, -0.138, 0.0643, -0.198, -0.0825, -0.228, 0.399, -0.132, -0.0528, 0.0599, 0.189, 0.319, 0.0607, 0.306, 0.129, 0.0678, 0.0425, 0.373, 0.148, 0.157, -0.182, 0.14, 0.00511], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6725840-f30e-4f9a-b830-9c16f1217806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5569, 100)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c06f03b6-f79e-4f4e-811e-d98248f62427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f425b339-d5b9-42bd-a9ff-eb7846715e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "638451bc-f773-45b3-a78a-c6847a48064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependent Features\n",
    "## Output Features\n",
    "y = messages[list(map(lambda x: len(x)>0 ,corpus))]\n",
    "y=pd.get_dummies(y['label'])\n",
    "y=y.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66210377-f7a7-4eab-8abb-f0ca0d1fea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5569,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "05370935-6fe8-49d2-8294-d81cc1ff3161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "234e3d05-255e-43aa-9346-f0cd9d3c5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the final independent features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# X is a list of vectors → convert directly to DataFrame\n",
    "df = pd.DataFrame(np.vstack(X))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8566265c-f8f2-4db9-be11-1982cd04cb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.197791</td>\n",
       "      <td>0.234851</td>\n",
       "      <td>0.111199</td>\n",
       "      <td>0.094850</td>\n",
       "      <td>0.089863</td>\n",
       "      <td>-0.488009</td>\n",
       "      <td>0.163624</td>\n",
       "      <td>0.475161</td>\n",
       "      <td>-0.247561</td>\n",
       "      <td>-0.119808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344247</td>\n",
       "      <td>0.156473</td>\n",
       "      <td>0.084090</td>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.439757</td>\n",
       "      <td>0.174327</td>\n",
       "      <td>0.178551</td>\n",
       "      <td>-0.198179</td>\n",
       "      <td>0.152833</td>\n",
       "      <td>0.013486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.181282</td>\n",
       "      <td>0.203537</td>\n",
       "      <td>0.094383</td>\n",
       "      <td>0.080723</td>\n",
       "      <td>0.084116</td>\n",
       "      <td>-0.423218</td>\n",
       "      <td>0.130834</td>\n",
       "      <td>0.418275</td>\n",
       "      <td>-0.215775</td>\n",
       "      <td>-0.097678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.129170</td>\n",
       "      <td>0.067846</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>0.372701</td>\n",
       "      <td>0.148263</td>\n",
       "      <td>0.156775</td>\n",
       "      <td>-0.181845</td>\n",
       "      <td>0.139796</td>\n",
       "      <td>0.005114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.208521</td>\n",
       "      <td>0.248745</td>\n",
       "      <td>0.117649</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.071415</td>\n",
       "      <td>-0.521740</td>\n",
       "      <td>0.164226</td>\n",
       "      <td>0.466627</td>\n",
       "      <td>-0.260094</td>\n",
       "      <td>-0.146971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337535</td>\n",
       "      <td>0.158235</td>\n",
       "      <td>0.085359</td>\n",
       "      <td>0.044096</td>\n",
       "      <td>0.449679</td>\n",
       "      <td>0.159058</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>-0.229918</td>\n",
       "      <td>0.180621</td>\n",
       "      <td>0.028197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.270588</td>\n",
       "      <td>0.317035</td>\n",
       "      <td>0.145805</td>\n",
       "      <td>0.127467</td>\n",
       "      <td>0.122373</td>\n",
       "      <td>-0.658161</td>\n",
       "      <td>0.216160</td>\n",
       "      <td>0.649599</td>\n",
       "      <td>-0.338687</td>\n",
       "      <td>-0.153752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470785</td>\n",
       "      <td>0.208257</td>\n",
       "      <td>0.109789</td>\n",
       "      <td>0.081812</td>\n",
       "      <td>0.586340</td>\n",
       "      <td>0.242524</td>\n",
       "      <td>0.258446</td>\n",
       "      <td>-0.270055</td>\n",
       "      <td>0.208485</td>\n",
       "      <td>0.008672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.234551</td>\n",
       "      <td>0.260698</td>\n",
       "      <td>0.131103</td>\n",
       "      <td>0.103837</td>\n",
       "      <td>0.114205</td>\n",
       "      <td>-0.561903</td>\n",
       "      <td>0.180247</td>\n",
       "      <td>0.555272</td>\n",
       "      <td>-0.292435</td>\n",
       "      <td>-0.137722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405193</td>\n",
       "      <td>0.176492</td>\n",
       "      <td>0.095929</td>\n",
       "      <td>0.070978</td>\n",
       "      <td>0.501211</td>\n",
       "      <td>0.208528</td>\n",
       "      <td>0.213357</td>\n",
       "      <td>-0.239355</td>\n",
       "      <td>0.171119</td>\n",
       "      <td>0.006762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.197791  0.234851  0.111199  0.094850  0.089863 -0.488009  0.163624   \n",
       "1 -0.181282  0.203537  0.094383  0.080723  0.084116 -0.423218  0.130834   \n",
       "2 -0.208521  0.248745  0.117649  0.114783  0.071415 -0.521740  0.164226   \n",
       "3 -0.270588  0.317035  0.145805  0.127467  0.122373 -0.658161  0.216160   \n",
       "4 -0.234551  0.260698  0.131103  0.103837  0.114205 -0.561903  0.180247   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0  0.475161 -0.247561 -0.119808  ...  0.344247  0.156473  0.084090  0.055626   \n",
       "1  0.418275 -0.215775 -0.097678  ...  0.306300  0.129170  0.067846  0.042506   \n",
       "2  0.466627 -0.260094 -0.146971  ...  0.337535  0.158235  0.085359  0.044096   \n",
       "3  0.649599 -0.338687 -0.153752  ...  0.470785  0.208257  0.109789  0.081812   \n",
       "4  0.555272 -0.292435 -0.137722  ...  0.405193  0.176492  0.095929  0.070978   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.439757  0.174327  0.178551 -0.198179  0.152833  0.013486  \n",
       "1  0.372701  0.148263  0.156775 -0.181845  0.139796  0.005114  \n",
       "2  0.449679  0.159058  0.128186 -0.229918  0.180621  0.028197  \n",
       "3  0.586340  0.242524  0.258446 -0.270055  0.208485  0.008672  \n",
       "4  0.501211  0.208528  0.213357 -0.239355  0.171119  0.006762  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8f6eef5-0dfc-4f99-aa04-b17ffc3c6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Output']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "20b441fb-aca9-4538-92c5-459b96b5c7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.197791</td>\n",
       "      <td>0.234851</td>\n",
       "      <td>0.111199</td>\n",
       "      <td>0.094850</td>\n",
       "      <td>0.089863</td>\n",
       "      <td>-0.488009</td>\n",
       "      <td>0.163624</td>\n",
       "      <td>0.475161</td>\n",
       "      <td>-0.247561</td>\n",
       "      <td>-0.119808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156473</td>\n",
       "      <td>0.084090</td>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.439757</td>\n",
       "      <td>0.174327</td>\n",
       "      <td>0.178551</td>\n",
       "      <td>-0.198179</td>\n",
       "      <td>0.152833</td>\n",
       "      <td>0.013486</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.181282</td>\n",
       "      <td>0.203537</td>\n",
       "      <td>0.094383</td>\n",
       "      <td>0.080723</td>\n",
       "      <td>0.084116</td>\n",
       "      <td>-0.423218</td>\n",
       "      <td>0.130834</td>\n",
       "      <td>0.418275</td>\n",
       "      <td>-0.215775</td>\n",
       "      <td>-0.097678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129170</td>\n",
       "      <td>0.067846</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>0.372701</td>\n",
       "      <td>0.148263</td>\n",
       "      <td>0.156775</td>\n",
       "      <td>-0.181845</td>\n",
       "      <td>0.139796</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.208521</td>\n",
       "      <td>0.248745</td>\n",
       "      <td>0.117649</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.071415</td>\n",
       "      <td>-0.521740</td>\n",
       "      <td>0.164226</td>\n",
       "      <td>0.466627</td>\n",
       "      <td>-0.260094</td>\n",
       "      <td>-0.146971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158235</td>\n",
       "      <td>0.085359</td>\n",
       "      <td>0.044096</td>\n",
       "      <td>0.449679</td>\n",
       "      <td>0.159058</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>-0.229918</td>\n",
       "      <td>0.180621</td>\n",
       "      <td>0.028197</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.270588</td>\n",
       "      <td>0.317035</td>\n",
       "      <td>0.145805</td>\n",
       "      <td>0.127467</td>\n",
       "      <td>0.122373</td>\n",
       "      <td>-0.658161</td>\n",
       "      <td>0.216160</td>\n",
       "      <td>0.649599</td>\n",
       "      <td>-0.338687</td>\n",
       "      <td>-0.153752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208257</td>\n",
       "      <td>0.109789</td>\n",
       "      <td>0.081812</td>\n",
       "      <td>0.586340</td>\n",
       "      <td>0.242524</td>\n",
       "      <td>0.258446</td>\n",
       "      <td>-0.270055</td>\n",
       "      <td>0.208485</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.234551</td>\n",
       "      <td>0.260698</td>\n",
       "      <td>0.131103</td>\n",
       "      <td>0.103837</td>\n",
       "      <td>0.114205</td>\n",
       "      <td>-0.561903</td>\n",
       "      <td>0.180247</td>\n",
       "      <td>0.555272</td>\n",
       "      <td>-0.292435</td>\n",
       "      <td>-0.137722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176492</td>\n",
       "      <td>0.095929</td>\n",
       "      <td>0.070978</td>\n",
       "      <td>0.501211</td>\n",
       "      <td>0.208528</td>\n",
       "      <td>0.213357</td>\n",
       "      <td>-0.239355</td>\n",
       "      <td>0.171119</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.197791  0.234851  0.111199  0.094850  0.089863 -0.488009  0.163624   \n",
       "1 -0.181282  0.203537  0.094383  0.080723  0.084116 -0.423218  0.130834   \n",
       "2 -0.208521  0.248745  0.117649  0.114783  0.071415 -0.521740  0.164226   \n",
       "3 -0.270588  0.317035  0.145805  0.127467  0.122373 -0.658161  0.216160   \n",
       "4 -0.234551  0.260698  0.131103  0.103837  0.114205 -0.561903  0.180247   \n",
       "\n",
       "          7         8         9  ...        91        92        93        94  \\\n",
       "0  0.475161 -0.247561 -0.119808  ...  0.156473  0.084090  0.055626  0.439757   \n",
       "1  0.418275 -0.215775 -0.097678  ...  0.129170  0.067846  0.042506  0.372701   \n",
       "2  0.466627 -0.260094 -0.146971  ...  0.158235  0.085359  0.044096  0.449679   \n",
       "3  0.649599 -0.338687 -0.153752  ...  0.208257  0.109789  0.081812  0.586340   \n",
       "4  0.555272 -0.292435 -0.137722  ...  0.176492  0.095929  0.070978  0.501211   \n",
       "\n",
       "         95        96        97        98        99  Output  \n",
       "0  0.174327  0.178551 -0.198179  0.152833  0.013486    True  \n",
       "1  0.148263  0.156775 -0.181845  0.139796  0.005114    True  \n",
       "2  0.159058  0.128186 -0.229918  0.180621  0.028197   False  \n",
       "3  0.242524  0.258446 -0.270055  0.208485  0.008672    True  \n",
       "4  0.208528  0.213357 -0.239355  0.171119  0.006762    True  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a26f34a-b20b-4f45-9e0f-9d1191e002b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0ce6260d-ecda-49e7-8c31-4cf17925477d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "96        0\n",
       "97        0\n",
       "98        0\n",
       "99        0\n",
       "Output    0\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "719d0f6b-2952-4dbc-b1bf-d2035abca54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Independent Feature\n",
    "X=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cee46fdf-fb6a-49ff-b0ca-920eba8b909d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "96        0\n",
       "97        0\n",
       "98        0\n",
       "99        0\n",
       "Output    0\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0edf8565-beb1-457c-b5ca-6d41d46ecbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6c41f52-22d1-4ac4-9c96-b20c6a453a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64ab3bc6-0274-45e9-92db-eb475648a390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>-0.261877</td>\n",
       "      <td>0.286549</td>\n",
       "      <td>0.143858</td>\n",
       "      <td>0.117111</td>\n",
       "      <td>0.119331</td>\n",
       "      <td>-0.604663</td>\n",
       "      <td>0.192292</td>\n",
       "      <td>0.589083</td>\n",
       "      <td>-0.314654</td>\n",
       "      <td>-0.150389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196457</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.063341</td>\n",
       "      <td>0.549951</td>\n",
       "      <td>0.216261</td>\n",
       "      <td>0.210001</td>\n",
       "      <td>-0.259014</td>\n",
       "      <td>0.196962</td>\n",
       "      <td>0.021156</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>-0.273867</td>\n",
       "      <td>0.298362</td>\n",
       "      <td>0.141409</td>\n",
       "      <td>0.122344</td>\n",
       "      <td>0.125255</td>\n",
       "      <td>-0.627594</td>\n",
       "      <td>0.194242</td>\n",
       "      <td>0.605718</td>\n",
       "      <td>-0.320800</td>\n",
       "      <td>-0.149649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197506</td>\n",
       "      <td>0.095295</td>\n",
       "      <td>0.069012</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.238344</td>\n",
       "      <td>0.226723</td>\n",
       "      <td>-0.274516</td>\n",
       "      <td>0.198609</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>-0.225962</td>\n",
       "      <td>0.240142</td>\n",
       "      <td>0.116530</td>\n",
       "      <td>0.099753</td>\n",
       "      <td>0.121152</td>\n",
       "      <td>-0.529592</td>\n",
       "      <td>0.156118</td>\n",
       "      <td>0.522788</td>\n",
       "      <td>-0.281635</td>\n",
       "      <td>-0.129535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161007</td>\n",
       "      <td>0.069324</td>\n",
       "      <td>0.068676</td>\n",
       "      <td>0.460512</td>\n",
       "      <td>0.200438</td>\n",
       "      <td>0.197454</td>\n",
       "      <td>-0.233920</td>\n",
       "      <td>0.155097</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4066</th>\n",
       "      <td>-0.274014</td>\n",
       "      <td>0.307188</td>\n",
       "      <td>0.156812</td>\n",
       "      <td>0.124937</td>\n",
       "      <td>0.122221</td>\n",
       "      <td>-0.653464</td>\n",
       "      <td>0.202906</td>\n",
       "      <td>0.617572</td>\n",
       "      <td>-0.336684</td>\n",
       "      <td>-0.174236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204897</td>\n",
       "      <td>0.102320</td>\n",
       "      <td>0.063096</td>\n",
       "      <td>0.573670</td>\n",
       "      <td>0.223122</td>\n",
       "      <td>0.200707</td>\n",
       "      <td>-0.287083</td>\n",
       "      <td>0.205938</td>\n",
       "      <td>0.024634</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4937</th>\n",
       "      <td>-0.170343</td>\n",
       "      <td>0.366779</td>\n",
       "      <td>0.147837</td>\n",
       "      <td>0.125275</td>\n",
       "      <td>0.022614</td>\n",
       "      <td>-0.598463</td>\n",
       "      <td>0.283088</td>\n",
       "      <td>0.630162</td>\n",
       "      <td>-0.290636</td>\n",
       "      <td>-0.184983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222009</td>\n",
       "      <td>0.163713</td>\n",
       "      <td>0.112599</td>\n",
       "      <td>0.580203</td>\n",
       "      <td>0.216505</td>\n",
       "      <td>0.269266</td>\n",
       "      <td>-0.164197</td>\n",
       "      <td>0.181518</td>\n",
       "      <td>-0.031500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "240  -0.261877  0.286549  0.143858  0.117111  0.119331 -0.604663  0.192292   \n",
       "2771 -0.273867  0.298362  0.141409  0.122344  0.125255 -0.627594  0.194242   \n",
       "5200 -0.225962  0.240142  0.116530  0.099753  0.121152 -0.529592  0.156118   \n",
       "4066 -0.274014  0.307188  0.156812  0.124937  0.122221 -0.653464  0.202906   \n",
       "4937 -0.170343  0.366779  0.147837  0.125275  0.022614 -0.598463  0.283088   \n",
       "\n",
       "             7         8         9  ...        91        92        93  \\\n",
       "240   0.589083 -0.314654 -0.150389  ...  0.196457  0.100917  0.063341   \n",
       "2771  0.605718 -0.320800 -0.149649  ...  0.197506  0.095295  0.069012   \n",
       "5200  0.522788 -0.281635 -0.129535  ...  0.161007  0.069324  0.068676   \n",
       "4066  0.617572 -0.336684 -0.174236  ...  0.204897  0.102320  0.063096   \n",
       "4937  0.630162 -0.290636 -0.184983  ...  0.222009  0.163713  0.112599   \n",
       "\n",
       "            94        95        96        97        98        99  Output  \n",
       "240   0.549951  0.216261  0.210001 -0.259014  0.196962  0.021156   False  \n",
       "2771  0.551191  0.238344  0.226723 -0.274516  0.198609  0.016318    True  \n",
       "5200  0.460512  0.200438  0.197454 -0.233920  0.155097  0.006159    True  \n",
       "4066  0.573670  0.223122  0.200707 -0.287083  0.205938  0.024634   False  \n",
       "4937  0.580203  0.216505  0.269266 -0.164197  0.181518 -0.031500    True  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99fa321c-fdec-444c-9d32-3d4ee9cdb42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240     False\n",
       "2771     True\n",
       "5200     True\n",
       "4066    False\n",
       "4937     True\n",
       "        ...  \n",
       "4426     True\n",
       "2931     True\n",
       "1759     True\n",
       "4910     True\n",
       "189      True\n",
       "Name: Output, Length: 4455, dtype: bool"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c263c26-d638-47f9-91f6-8a55e56c5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5c0cda46-69eb-432f-98e3-58742565bd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train.values,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed5fa6af-3e2b-40b2-8c29-bb94805583a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "646a8c87-6d9e-4e38-bf2a-fdf853146f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973070017953322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eb81867e-9c2e-4830-99d4-cfc21894a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99       148\n",
      "        True       1.00      1.00      1.00       966\n",
      "\n",
      "    accuracy                           1.00      1114\n",
      "   macro avg       1.00      0.99      0.99      1114\n",
      "weighted avg       1.00      1.00      1.00      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8abec-d42e-4ef5-a9f0-5160667c6590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be6595c0-0028-412c-96f5-72f010e64046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BERT Models from transformers (all-mini-lm-l6-v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c81ad6-7583-441a-a4cd-ac65367a2af2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Theory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6074bb3-4796-44b9-acfc-25a565c395e3",
   "metadata": {},
   "source": [
    "Technical Highlights\n",
    "Feature\tDetails\n",
    "Embedding Size\t384-dimensional vectors\n",
    "Max Input Length\t256 word pieces (longer truncated)\n",
    "Architecture\t6-layer transformer, encoder-only\n",
    "Training Objective\tContrastive learning on 1B sentence pairs\n",
    "Model Size\t~22M parameters\n",
    "Inference Speed\tFast and efficient"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a675661-3a41-428c-a414-4a648f9ab96b",
   "metadata": {},
   "source": [
    "Performance vs. Larger Models:\n",
    "\n",
    "While models like all-mpnet-base-v2 offer higher accuracy, MiniLM-L6-v2 delivers embeddings that are competitive yet ~5× faster, making it ideal for latency-sensitive and resource-constrained environments."
   ]
  },
  {
   "cell_type": "raw",
   "id": "45558f89-02df-4343-8dee-6d95017dcb6e",
   "metadata": {},
   "source": [
    "Yes — all-MiniLM-L6-v2 is based on BERT (Bidirectional Encoder Representations from Transformers), but in a distilled, lightweight form.\n",
    "\n",
    "Here’s how:\n",
    "\n",
    "✅ BERT architecture family → MiniLM was developed by Microsoft as a distilled version of BERT.\n",
    "\n",
    "✅ Fewer layers → Instead of BERT-base’s 12 transformer layers, MiniLM-L6 has 6 layers.\n",
    "\n",
    "✅ Smaller hidden size → Instead of BERT-base’s 768-dimensional hidden states, it uses 384 dimensions.\n",
    "\n",
    "✅ Same encoder-only transformer → Just like BERT, it’s an encoder-only model (no decoder).\n",
    "\n",
    "So you can think of all-MiniLM-L6-v2 as:\n",
    "👉 A smaller, faster BERT-like model fine-tuned to produce high-quality sentence embeddings.\n",
    "\n",
    "⚡ Key difference:\n",
    "\n",
    "BERT (base/large) → Pretrained for masked language modeling (MLM). Needs fine-tuning for downstream tasks.\n",
    "\n",
    "all-MiniLM-L6-v2 → Already fine-tuned with contrastive learning on billions of sentence pairs → ready out-of-the-box for semantic similarity, clustering, retrieval, etc."
   ]
  },
  {
   "cell_type": "raw",
   "id": "67c65a2d-32cc-43da-b03b-79d6be017315",
   "metadata": {},
   "source": [
    "1. What does “distilled version” mean?\n",
    "\n",
    "Distillation = a technique where a large model (teacher) (like BERT-base or RoBERTa-large) trains a smaller model (student) to mimic its behavior.\n",
    "\n",
    "The student (MiniLM) doesn’t learn from raw text only → it learns to approximate the teacher’s attention patterns and outputs.\n",
    "\n",
    "Result: the small model is much faster and lighter 💡 but keeps most of the semantic understanding power of the big one.\n",
    "\n",
    "So MiniLM is basically a compressed BERT, optimized for efficiency."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c83b33f6-8b04-4976-b0cc-ce8b2857ed68",
   "metadata": {},
   "source": [
    "2. Does it give embeddings for the sentence or each word?\n",
    "\n",
    "With all-MiniLM-L6-v2 (via SentenceTransformers):\n",
    "\n",
    "You input a sentence (or paragraph).\n",
    "\n",
    "It outputs a single 384-dimensional embedding vector for that sentence.\n",
    "→ This is the sentence embedding, not per-token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69185f-ca7b-4e42-ab4c-1db4371471c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13912e8b-ee90-4eb1-8268-9380ae67225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e5c4e4-3f5f-4b64-9ccb-1c0acb27ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f15114e-cd34-4c0b-a822-620bb5382378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Patralapati\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d376be0-3f9a-4888-8ff7-c425f29e3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MiniLM model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a28454-c2c0-4ec7-a13e-eaeded741ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"I love playing cricket.\",\n",
    "    \"The bank approved my loan application.\",\n",
    "    \"The boat reached the river bank.\",\n",
    "    \"Deep learning models require a lot of data.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9041ce-0dea-4aa9-95d5-9e16ec3a8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for the entire corpus\n",
    "embeddings = model.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500cf725-b3c7-4147-ab63-33a663238a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: Machine learning is amazing.\n",
      "Embedding shape: (384,)\n",
      "First 5 dimensions: [-0.02847    -0.09029377  0.09246505 -0.0099446  -0.00782896]\n",
      "\n",
      "Sentence: I love playing cricket.\n",
      "Embedding shape: (384,)\n",
      "First 5 dimensions: [ 0.06192518  0.03125419  0.00685254 -0.04609568  0.00389682]\n",
      "\n",
      "Sentence: The bank approved my loan application.\n",
      "Embedding shape: (384,)\n",
      "First 5 dimensions: [ 0.01011685  0.05525683  0.00066423 -0.0566733  -0.03473969]\n",
      "\n",
      "Sentence: The boat reached the river bank.\n",
      "Embedding shape: (384,)\n",
      "First 5 dimensions: [ 0.02062863  0.09431628 -0.05618941  0.03691628  0.00400123]\n",
      "\n",
      "Sentence: Deep learning models require a lot of data.\n",
      "Embedding shape: (384,)\n",
      "First 5 dimensions: [ 0.00020667 -0.10050159  0.09560353  0.01736766 -0.02425423]\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "for i, sentence in enumerate(corpus):\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"Embedding shape: {embeddings[i].shape}\")   # (384,)\n",
    "    print(f\"First 5 dimensions: {embeddings[i][:5]}\") # just preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7b654-ba87-4333-8064-a816a8441b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba25226-2d97-4db0-8b87-550bcb9a5c97",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b9dcf-4c60-422e-9488-fdd41bb119ff",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe7c5ab1-bc2c-4546-bc6d-3ed47d023844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21c82c09-5ff2-467b-b498-12253ba9a076",
   "metadata": {},
   "source": [
    "1. What is Regex?\n",
    "Regex is a way to define patterns to search, match, and manipulate text.\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d75fe8b-a00b-4158-b1cd-75af27b82ae3",
   "metadata": {},
   "source": [
    "2. Basic Regex Elements\n",
    "| Pattern | Meaning                               | Example Match                      |\n",
    "| ------- | ------------------------------------- | ---------------------------------- |\n",
    "| `.`     | Any character (except newline)        | `\"c.t\"` → `\"cat\"`, `\"cut\"`         |\n",
    "| `^`     | Start of string                       | `^Hello` → matches `\"Hello world\"` |\n",
    "| `$`     | End of string                         | `world$` → matches `\"Hello world\"` |\n",
    "| `\\d`    | Digit (0–9)                           | `\"Order 123\"` → `123`              |\n",
    "| `\\D`    | Non-digit                             | `\"abc123\"` → `\"abc\"`               |\n",
    "| `\\w`    | Word character (letters, digits, `_`) | `\"hello_123\"`                      |\n",
    "| `\\W`    | Non-word character                    | `\"hello!\"` → `\"!\"`                 |\n",
    "| `\\s`    | Whitespace (space, tab, newline)      | `\"a b\"`                            |\n",
    "| `\\S`    | Non-whitespace                        | `\"hello\"`                          |\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f52b9b73-f325-4bde-bc32-ca9a76003283",
   "metadata": {},
   "source": [
    "3. Quantifiers\n",
    "| Pattern | Meaning                | Example                                                 |\n",
    "| ------- | ---------------------- | ------------------------------------------------------- |\n",
    "| `*`     | 0 or more times        | `\"go*gle\"` → `\"ggle\"`, `\"google\"`, `\"gooooogle\"`        |\n",
    "| `+`     | 1 or more times        | `\"go+gle\"` → `\"google\"`, `\"gooooogle\"` but not `\"ggle\"` |\n",
    "| `?`     | 0 or 1 time (optional) | `\"colou?r\"` → `\"color\"`, `\"colour\"`                     |\n",
    "| `{n}`   | Exactly n times        | `\\d{4}` → `\"2025\"`                                      |\n",
    "| `{n,}`  | At least n times       | `\\d{2,}` → `\"12\"`, `\"1234\"`                             |\n",
    "| `{n,m}` | Between n and m times  | `a{2,4}` → `\"aa\"`, `\"aaa\"`, `\"aaaa\"`                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46346e6-dcdb-42df-9435-c8644f3e070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Character Sets & Groups\n",
    "| Pattern  | Meaning                      | Example            |\n",
    "| -------- | ---------------------------- | ------------------ |\n",
    "| `[abc]`  | Match `a` or `b` or `c`      | `\"cat\"` → `\"c\"`    |\n",
    "| `[a-z]`  | Range (lowercase letters)    | `\"hello\"`          |\n",
    "| `[A-Z]`  | Uppercase range              | `\"HELLO\"`          |\n",
    "| `[0-9]`  | Digits                       | `\"2025\"`           |\n",
    "| `[^a-z]` | NOT lowercase                | `\"123\"`, `\"HELLO\"` |\n",
    "| `(abc)`  | Group (captures as one unit) | `\"abcabc\"`         |\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbce33bd-b082-41f1-960e-5100e1410efc",
   "metadata": {},
   "source": [
    "re.sub() in Python\n",
    "\n",
    "re.sub(pattern, replacement, string)\n",
    "\n",
    "pattern → the regex pattern you want to match\n",
    "\n",
    "replacement → what you want to replace matches with\n",
    "\n",
    "string → the input text\n",
    "\n",
    "It returns a new string where all matches are replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da76805-2ec2-4357-9046-4434f32b38c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Order  is ready'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove digits\n",
    "re.sub(r'\\d+', '', \"Order 123 is ready\")  \n",
    "# Output: \"Order  is ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de21b9b-3591-4108-b618-e712afc0d5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Order  is  ready'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\d+', '', \"Order 123 is 3 4ready234\")  \n",
    "# Output: \"Order  is ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107fdef8-627c-4bf7-a531-f9f2eb12395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello This is NLP 2025  AI\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "# punctuation usually means all the symbols that are not letters, digits, or whitespace.\n",
    "\n",
    "text = \"Hello!!! This is NLP, 2025 :) #AI\"\n",
    "cleaned = re.sub(r'[^\\w\\s]', '', text)\n",
    "print(cleaned)\n",
    "# Output: Hello This is NLP 2025 AI\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2361a98-a418-4d99-b506-046928b1222c",
   "metadata": {},
   "source": [
    "re.findall()\n",
    "re.findall(pattern, string)\n",
    "\n",
    "\n",
    "pattern → regex expression to search for\n",
    "\n",
    "string → input text\n",
    "\n",
    "Returns a list of all non-overlapping matches in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2688aa77-348a-4fb0-8748-2781117e285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@user']\n",
      "['#NLP']\n"
     ]
    }
   ],
   "source": [
    "# Explanation:\n",
    "# @ → matches the literal @ symbol\n",
    "# \\w+ → one or more word characters (letters, digits, _)\n",
    "\n",
    "# Together → @user\n",
    "# Extract hashtags / mentions (Twitter)\n",
    "mentioned =re.findall(r'@\\w+', \"Hi @user, check #NLP\")  \n",
    "print(mentioned)\n",
    "# Output: ['@user']\n",
    "\n",
    "hashtag = re.findall(r'#\\w+', \"Hi @user, check #NLP\")  \n",
    "print(hashtag)\n",
    "# Output: ['#NLP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c1a4bda-e7c7-4f17-94a5-dd07e74d0260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'is', 'fun', 'in', '2025']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize on words\n",
    "# Explanation of Regex Pattern \\b\\w+\\b\n",
    "\n",
    "# \\b → word boundary\n",
    "\n",
    "# It matches the position between a word character (\\w) and a non-word character (\\W).\n",
    "\n",
    "# Ensures we capture whole words, not substrings inside them.\n",
    "\n",
    "# \\w+ → one or more word characters\n",
    "\n",
    "# \\w = [a-zA-Z0-9_] (letters, digits, underscore).\n",
    "\n",
    "# So \\w+ = full word tokens like NLP, is, 2025.\n",
    "\n",
    "# \\b ... \\b → ensures that the word ends at a boundary (space, punctuation, start/end of string).\n",
    "re.findall(r'\\b\\w+\\b', \"NLP is fun in 2025!\")  \n",
    "# Output: ['NLP', 'is', 'fun', 'in', '2025']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f6f041b-800f-49bc-9143-f6f2632647ae",
   "metadata": {},
   "source": [
    "Regex Pattern: http\\S+|www\\S+\n",
    "\n",
    "http\\S+ → matches any string starting with http followed by non-space characters (\\S+ = one or more non-spaces).\n",
    "→ Matches \"http://abc.com\".\n",
    "\n",
    "| → OR\n",
    "\n",
    "www\\S+ → matches any string starting with www followed by non-spaces.\n",
    "\n",
    "So this removes both http... and www... style URLs.\n",
    "\n",
    "✅ Result: \"Check this link \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60793dd8-5bef-4654-88f2-04304254a43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check this link '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'http\\S+|www\\S+', '', \"Check this link http://abc.com\")\n",
    "# Output: \"Check this link \"\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41bc5ea1-febc-47cb-96da-54885ea60ad5",
   "metadata": {},
   "source": [
    "Regex Pattern: \\S+@\\S+\n",
    "\n",
    "\\S+ → one or more non-space characters (before the @) → matches test.\n",
    "\n",
    "@ → literal @ symbol.\n",
    "\n",
    "\\S+ → one or more non-space characters (after the @) → matches email.com.\n",
    "\n",
    "So test@email.com gets removed.\n",
    "\n",
    "✅ Result: \"Contact us at \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdfbd669-cddc-4092-ac85-bec4b4ccedca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contact us at '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\S+@\\S+', '', \"Contact us at test@email.com\")\n",
    "# Output: \"Contact us at \"\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8497fb0d-b1eb-4ab7-b6aa-ac84553090fb",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "\\S → matches any non-space character.\n",
    "\n",
    "+ → one or more.\n",
    "\n",
    "So \\S+ = a continuous chunk of non-space characters.\n",
    "\n",
    "Useful for emails, URLs, file paths, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c66c2-ce56-40e6-990c-63597b4f5e08",
   "metadata": {},
   "source": [
    "#### Emoji handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8fee504-007e-4d43-9bdc-e4e623f68999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b904b613-9bc0-49ac-b172-1ecbefe15857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76f17215-cdaf-415f-9bbc-95a0c7a324c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🤖', '🔥', '😂']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love NLP 🤖🔥😂\"\n",
    "\n",
    "# Extract all emojis\n",
    "emojis = [ch for ch in text if ch in emoji.EMOJI_DATA]\n",
    "print(emojis)  \n",
    "# ['🤖', '🔥', '😂']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a3eab43-00a4-4d5b-b09f-3299edda2524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love NLP :robot::fire::face_with_tears_of_joy:\n"
     ]
    }
   ],
   "source": [
    "# Convert emojis to text (demojize)\n",
    "text = \"I love NLP 🤖🔥😂\"\n",
    "converted = emoji.demojize(text)\n",
    "print(converted)\n",
    "# I love NLP :robot_face::fire::face_with_tears_of_joy:\n",
    "# Now you can process emojis as words/tokens like :fire: or :face_with_tears_of_joy:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "319ee922-af53-413f-8afd-c4343427c551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love NLP :robot_face:🔥\n"
     ]
    }
   ],
   "source": [
    "# Convert text back to emojis (emojize)\n",
    "converted_back = emoji.emojize(\"I love NLP :robot_face::fire:\")\n",
    "print(converted_back)\n",
    "# I love NLP 🤖🔥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6d04791-92ac-488b-a55b-6f5f89b685bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great job! \n"
     ]
    }
   ],
   "source": [
    "# Remove emojis completely\n",
    "text = \"Great job! 👍👏🎉\"\n",
    "cleaned = emoji.replace_emoji(text, replace=\"\")\n",
    "print(cleaned)\n",
    "# Great job! \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a10d68e1-d0b2-43be-8127-7b3284565eae",
   "metadata": {},
   "source": [
    "The emoji package only works with Unicode emojis (😊, 😢, 😂, 😎, etc.) — the ones you insert from your keyboard or copy-paste.\n",
    "\n",
    "But the text emoticons like:\n",
    "\n",
    ":)\n",
    "\n",
    ":(\n",
    "\n",
    ":D\n",
    "\n",
    ";-)\n",
    "\n",
    ":P\n",
    "\n",
    "…are just plain ASCII characters, not Unicode emoji code points.\n",
    "\n",
    "That’s why:\n",
    "\n",
    "✅ emoji.replace_emoji(\"Hello 😊\", \"\") → removes 😊\n",
    "\n",
    "❌ emoji.replace_emoji(\"Hello :)\", \"\") → leaves :) untouched\n",
    "\n",
    "So we need regex rules (like the emoticon_pattern you saw) to catch those old-school emoticons."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce878e31-efb3-4c3d-8230-2d63b8238f01",
   "metadata": {},
   "source": [
    "👉 In short:\n",
    "\n",
    "emoji library → removes only real Unicode emojis\n",
    "\n",
    "regex for emoticons → removes text-based smileys (:), :P, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba46c19-e07f-4c0b-8ca8-88e34c7acee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove emojis and emoticons from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text without emojis/emoticons.\n",
    "    \"\"\"\n",
    "    # Remove emojis using the emoji library\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "    # Define regex for common emoticons\n",
    "    emoticon_pattern = r\"\"\"\n",
    "        (?:\n",
    "          [<>]?\n",
    "          [:;=8]                     # eyes\n",
    "          [\\-o\\*\\']?                  # optional nose\n",
    "          [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]  # mouth\n",
    "        )\n",
    "        |\n",
    "        (?:\n",
    "          [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]  # mouth\n",
    "          [\\-o\\*\\']?                  # optional nose\n",
    "          [:;=8]                      # eyes\n",
    "          [<>]?\n",
    "        )\n",
    "    \"\"\"\n",
    "    text = re.sub(emoticon_pattern, \"\", text, flags=re.VERBOSE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26348a8f-446b-4b50-8f6e-067d6312b872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
